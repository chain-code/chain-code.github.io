<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Soulmate</title>
    <link>https://chain-code.github.io/</link>
    <description>Recent content in Home on Soulmate</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Tue, 30 Apr 2024 16:16:50 +0800</lastBuildDate><atom:link href="https://chain-code.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>创建型设计模式</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E5%88%9B%E5%BB%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid>
      <description>创建型设计模式 # 单例模式 # 单例模式提供了一种访问其唯一对象的方法，该对象可以直接被访问，无需实例化
双重检查 # var lock = &amp;amp;sync.Mutex{} type singleton struct { } var instance *singleton //获取实例 func GetInstance() *singleton { if instance == nil { lock.Lock() if instance == nil { fmt.Println(&amp;#34;创建单个实例&amp;#34;) instance = new(singleton) } lock.Unlock() } return instance } sync.Once # var once sync.Once //只执行一次 func GetInstance() *singleton { once.Do(func() { instance = new(singleton) fmt.Println(&amp;#34;创建单个实例&amp;#34;) }) return instance } 优点 # 对于内存中只存在一个对象，且需要频繁创建和销毁对象的系统，使用单例模式可以提升系统性能 缺点 # 可扩展性较低 若用于数据库连接池对象，则可能会导致共享连接池对象过多且没有释放的场景，从而出现连接池溢出问题。 如果创建的对象长时间不使用，可能会被操作系统垃圾回收，导致对象丢失 工厂模式 # 介绍 # 工厂方法模式定义了一个用于创建对象的接口，但让子类决定实例化那个类</description>
    </item>
    
    <item>
      <title>系统架构基础</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%9F%BA%E7%A1%80/</guid>
      <description>系统架构基础 # 系统架构 # 简介 # 系统架构描述了设计和构应用程序的模式和技术。是构建应用程序的起点或路线图，但开发者需要根据自己的实际情况，选择对应的编程语言实现。
在决定为新的应用程序适应那种架构或评估当前架构时，软件开发者或架构师应该先确定战略目标，再设计支持该目标的系统架构，不应先选择系统架构，再尝试使应用程序适用于该软件架构。
如何选择 # 选择标准
结合具体产品的功能需求进行选择
每个软件架构都包含一个用于完成常见软件任务的基本结构。开发者需要选择一种能够解决所需问题的架构，而非容易实现的架构
结合开发者的实际情况
不好的架构
不好的架构会使软件开发项目复杂化，增加软件开发工作的工作量，不利于公司节省成本 在选择架构前，需要考虑软件产品顶级组件的整体视图，以及是否符合开发者的实际要求 MVC架构 # 简介 # MVC架构通常用于开发用户界面，将相关的程序逻辑划分为相互关联的3部分，从而将信息的内部表示与向用户呈现信息、接收信息的方式分开。
模型：主要用于管理数据和业务逻辑。模型对应于用户使用的所有数据相关逻辑。模型可以在视图和控制器之间传输数据。 视图：主要用于处理布局和显示相关的业务，以及处理与应用程序有关的UI逻辑。 控制器：主要用于将命令路由到模型和视图。将控制器作为模型和视图之间的接口，用于处理所有业务逻辑和传入的请求，使用模型操作数据并与视图进行交互，从而呈现最终输出。 注意事项 # 包名不一定是模型、视图或控制器 不要将应用程序分解成太多的包 实现 # 创建模型包models及其代码 package modelsimport (&amp;#34;database/sql&amp;#34;&amp;#34;fmt&amp;#34;_ &amp;#34;github.com/go-sql-driver/mysql&amp;#34;)var db *sql.DB//用户模型type User struct {Id intName stringPhone string}//定义一个全局变量var u User//初始化数据库连接func init() {db, _ = sql.Open(&amp;#34;mysql&amp;#34;,&amp;#34;root:a123456@tcp(127.0.0.1:3306)/goDesignPattern&amp;#34;)}//获取用户信息func GetUserInfo(id int) *User {var param intif id &amp;gt; 0 {param = id} else {param = 1}// 非常重要：确保QueryRow之后调用Scan方法，否则持有的数据库链接不会被释放err := db.</description>
    </item>
    
    <item>
      <title>Docker基础</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/docker%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 19 Sep 2022 21:02:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/docker%E5%9F%BA%E7%A1%80/</guid>
      <description>Docker简介 # Docker是一个开源的容器引擎，它基于LCX容器技术，使用Go语言开发。
源代码托管在Github上，并遵从Apache2.0协议。
Docker采用C/S架构，其可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。
Docker就是一种快速解决生产问题的一种技术手段,开发，运行和部署应用程序的开放管理平台。
Docker提供了在一个完全隔离的环境中打包和运行应用程序的能力，这个隔离的环境被称为容器。 由于容器的隔离性和安全性，因此可以在一个主机(宿主机)上同时运行多个相互隔离的容器，互不干预。
Docker主要解决的问题:
保证程序运行环境的一致性; 降低配置开发环境、生产环境的复杂度和成本; 实现程序的快速部署和分发。
架构与结构 # 架构图 # Docker是采用了(c/s)架构模式的应用程序
Client dockerCLI :客户端docker命令行
REST API : 一套介于客户端与服务端的之间进行通信并指示其执行的接口
Server docker daemon:服务端dacker守护进程等待客户端发送命令来执行
Docker的四大核心技术
IMAGE-镜像 CONTAINER-容器 DATA VOLUMES-数据卷 NETWORK-网络 结构图 # Docker客户端(Docker Client) # Docker客户端(Docker Client)是用户与Docker进行交互的最主要方式。当在终端输入docker命令时，对应的就会 在服务端产生对应的作用，并把结果返回给客户端。Docker Client除了连接本地服务端，通过更改或指定 DOCKER_HOST连接远程服务端。
Docker服务端(Docker Server) # Docker Daemon其实就是Docker 的服务端。它负责监听Docker API请求(如Docker Client)并管理Docker对象(Docker Objects)，如镜像、容器、网络、数据卷等
Docker Registries # 俗称Docker仓库，专门用于存储镜像的云服务环境.
Docker Hub就是一个公有的存放镜像的地方，类似Github存储代码文件。同样的也可以类似Github那样搭建私有 的仓库。
Docker 对象(Docker Objects) # 镜像:一个Docker的可执行文件，其中包括运行应用程序所需的所有代码内容、依赖库、环境变量和配置文件等。 容器:镜像被运行起来后的实例。 网络:外部或者容器间如何互相访问的网络方式，如host模式、bridge模式。 数据卷:容器与宿主机之间、容器与容器之间共享存储方式，类似虚拟机与主机之间的共享文件目录。
docker特点 # 三大理念: # 构建:龙珠里的胶囊，将你需要的场景构建好，装在一个小胶囊里</description>
    </item>
    
    <item>
      <title>Go高阶-语言基础</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 07 Sep 2022 15:43:23 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80/</guid>
      <description>前言 # func main(){ name:=&amp;#34;张三&amp;#34; fmt.printf(&amp;#34;%d&amp;#34;,len(name)) } 6 每个汉字3个字符 逃逸分析 # Go语言中，调用new函数得到的内存不一定在堆上，还有可能在栈上。这是因为在Go语言中，堆和栈的区别被“模糊化”了，当然这一切都是Go编译器在后台完成的。
一个变量是在堆上分配，还是在栈上分配，是经过编译器的逃逸分析之后得出的“结论”。
Go语言里就是指编译器的逃逸分析：它是编译器执行静态代码分析后，对内存管理进行的优化和简化。
在编译原理中，分析指针动态范围的方法被称为逃逸分析。通俗来讲，当一个对象的指针被多个方法或线程引用时，则称这个指针发生了逃逸。逃逸分析决定一个变量是分配在堆上还是分配在栈上。
作用 # 逃逸分析把变量合理地分配到它该去的地方，“找准自己的位置”。即使是用new函数申请到的内存，如果编译器发现这块内存在退出函数后就没有使用了，那就分配到栈上，毕竟栈上的内存分配比堆上块很多；反之，即使表面上只是一个普通的变量，但是经过编译器的逃逸分析后发现，在函数之外还有其他的地方在引用，那就分配到堆上。真正做到了按需分配。
如果变量都分配到堆上，堆不像栈可以自动清理。就会引起Go频繁的进行垃圾回收，而垃圾回收会占用比较大的系统开销。
堆和栈相比，堆适合不可预知大小的的内存分配。但是为此付出的代价是分配速度较慢，而且会形成内存碎片；栈内存分配则会非常快。栈分配内存只需要通过PUSH指令，并且会被自动释放；而堆分配内存首先需要去找一个大小合适的内存块，之后要通过垃圾回收才能释放。
通过逃逸分析，可以尽量把哪些不需要分配到堆上的变量直接分配到栈上，堆上的压力变小了，会减轻堆内存分配开销，同时也会减轻垃圾回收的压力，提高程序运行速度。
原则 # Go语言逃逸分析最基本的原则是：如果一个函数返回对一个变量的引用，那么这个变量就会发生逃逸。
Go中的变量只有在编译器可以证明在函数返回后不再被引用的，才分配到栈上，其他情况都分配到堆上。
编译器会根据变量是否被外部引用来决定是否逃逸：
如果变量在函数外部没有引用，则优先放到栈上。 如果变量在函数外部存在引用，则必定放到堆上。 针对第一条，放到堆上的情形：定义了一个很大的数组，需要申请的内存过大，超过了栈的存储能力。
判断 # Go提供了相关的命令，可以查看变量是否发生逃逸。
go build -gcflags &amp;#39;-m -l&amp;#39; main.go 其中-gcflags参数用于启动编译器支持的额外标志。例如，-m用于输出编译器的优化细节（包括使用逃逸分析这种优化），相反可以使用-N来关闭编译器优化；而-l则用于禁用foo函数的内联优化，防止逃逸被编译器通过内联彻底抹除。
GO与C/C++中的堆和栈是同一个概念吗 # 不是
C/C++中提及的“程序堆栈”本质上是操作系统层级的概念，它通过C/C++语言的编译器和所在的系统环境来共同决定。在程序启动时，操纵系统会自动维护一个所启动程序消耗内存的地址空间，并自动将这个空间从逻辑上划分为堆内存空间和栈内存空间。这时，“栈”的概念是指程序运行时自动获得的一小块内存，而后续的函数调用所消耗的栈大小，会在编译期间有编译器决定，用于保存局部变量或者保存函数调用栈。如果在C/C++中声明一个局部变量，则会执行逻辑上的压栈操作，在栈中记录局部变量。而当局部变量离开作用域之后，所谓的自动释放本质上是该位置的内存在下一次函数调用压栈过程中，可以被无条件的覆盖；对于堆而言，每当程序通过系统调用向操作系统申请内存时，会将所需的空间从维护的堆内存地址空间中分配出去，而在归还时则会将归还的内存合并到所维护的地址空间中。
Go程序也是运行在操作系统上的程序，自然同样拥有前面提到的堆和栈的概念。但区别在于传统意义上的“栈”被Go语言的运行时全部消耗了，用于维护运行时各个组件之间的协调，例如调度器、垃圾回收、系统调用等。而对于用户态的Go代码而言，他们所消耗的“堆和栈”，其实只是Go运行时通过管理向操作系统申请的堆内存，构造的逻辑上的“堆和栈”，它们的本质都是从操作系统申请而来的堆内存。
延迟语句 # 延迟语句defer，能把资源的释放语句与申请语句放到距离相近的位置，从而减少资源泄露的发生。
defer是Go语言提供的一种用于注册延迟调用的机制：让函数或语句可以在当前函数执行完毕后（包括通过return正常结束或者Panic导致的异常结束）执行。通常用于一些成对操作的场景：打开连接/关闭连接、加锁/释放锁、打开文件/关闭文件等。
defer会有短暂延迟，对时间要求特别高的程序，可以避免使用它。
defer的执行顺序 # defer语句并不会马上执行，而是会进入一个栈，函数return前，会按先进后出的顺序执行。先进后出的原因是后面定义的函数可能会依赖前面的资源，自然要先执行；否则，如果前面的先执行了，那后面的函数依赖就没有了，因而可能会出错。
在defer函数定义时，对外部变量的引用有两种方式：函数参数、闭包引用。前者在defer定义时就把值传递给defer，并且被cache起来；后者则会在defer函数真正调用时根据整个上下文确定参数当前的值。
func main(){ var whatever [3]struct{} for i:=range whatever{ defer func(){ fmt.Println(i) }() } } 222defer 后面跟的是一个闭包，i是“引用”类型的变量，for循环结束后i的值为2，因此后面打印了3个2.</description>
    </item>
    
    <item>
      <title>操作系统基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sun, 07 Aug 2022 12:20:19 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80/</guid>
      <description>操作系统 # 基础 # 什么是操作系统？ # 操作系统（Operating System，简称OS）是管理计算机软件与硬件资源的程序。 本质上是一个运行在计算机上的软件程序。 操作系统的存在屏蔽了硬件层的复杂性。 操作系统的内核（Kernel）是操作系统的核心部分，它负责系统的内存管理，硬件设备的管理，文件系统的管理以及应用程序的管理。 什么是系统调用？ # 根据进程访问资源的特点，我们可以把进程在系统上的运行分为两个级别：
1、用户态：用户态运行的进程可以直接读取用户程序的数据。
2、系统态：系统态运行的进程或程序几乎可以访问计算机的任何资源，不受限制。
我们运行的程序基本都是运行在用户态，凡是与系统态级别的资源有关的操作(如文件管理、进程控制、内存管理等)，都必须通过系统调用的方式向操作系统提出服务请求，并由操作系统代为完成。
这些系统调用按功能大致可分为如下几类：
设备管理。完成设备的请求或释放，以及设备启动等功能。 文件管理。完成文件的读、写、创建及删除等功能。 进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。 进程通信。完成进程之间的消息传递或信号传递等功能。 内存管理。完成内存的分配、回收以及获取作业占用内存区大小及地址等功能。 简单说下你对并发和并行的理解？ # 并发
在一个时间段中多个程序都启动运行在同一个处理机中
并行
假设目前A，B两个进程，两个进程分别由不同的 CPU 管理执行，两个进程不抢占 CPU 资源且可以同时运行，这叫做并行。
同步、异步、阻塞、非阻塞的概念 # 同步：当一个同步调用发出后，调用者要一直等待返回结果。通知后，才能进行后续的执行。
异步：当一个异步过程调用发出后，调用者不能立刻返回结果。实际处理这个调用的部件在完成后，通过状态，通知和回调来通知调用者。
阻塞：是指调用结果返回前，当前线程会被挂起，即阻塞。
非阻塞：是指调用结果没返回，也不会阻塞当前线程。
形象比喻：
小Q去钓鱼，抛完线后就傻傻的看着有没有动静，有则拉杆(同步阻塞) 小Q去钓鱼，拿鱼网捞一下，有没有鱼立即知道，不用等，直接就捞(同步非阻塞) 小Q去钓鱼，这个鱼缸比较牛皮，扔了后自己就打王者荣耀去了，因为鱼上钩了这个鱼缸带的报警器会通知我。这样实现异步(异步非阻塞） 异常的类型 # 故障 终止 自陷 三者的核心区别 # 类型 触发原因 保存的指令地址 是否可恢复 典型场景 故障 可修复的错误 故障指令地址 是（修复后重试） 缺页、权限不足 终止 不可恢复的错误 无（直接终止） 否 硬件故障、内存损坏 自陷 程序主动请求 下一条指令地址 是（继续执行） 系统调用、调试断点 总结 # 故障：像“临时堵车”，修复后可继续原路行驶。 终止：像“桥梁坍塌”，必须终止行程。 自陷：像“主动进服务区”，完成后继续旅程。 缓存 # 为了缓解数据库的压力，往往在数据库前面增加一个缓存：</description>
    </item>
    
    <item>
      <title>fabric相关机制与原理</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-02-25-fabric%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 25 Feb 2022 09:22:44 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-02-25-fabric%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8E%9F%E7%90%86/</guid>
      <description>Hyperledger fabric认知 # fabric由来 # ​ 2015年12月由Linux基金会主导并牵头，IBM、Intel、Cisco等制造和科技行业的巨头共同宣布了Hyperledger fabric联合项目成立。
​	hyperledger fabric利用容器技术来托管链码，其中包括系统的应用程序逻辑。
​	Hyperledger Fabric 是分布式账本解决方案的平台，采用模块化架构，提供高安全性、弹性、灵活性和可扩展性。它被设计为支持以可插拔方式实现不同组件，并适应复杂的经济生态系统。
hyperledger fabric与其他公有区块链系统最大的不同主要体现在以下两个方面：（1）私有fabric提供建立通道的功能，允许参与交易新建一个单独的账本。（2）许可与开放无须许可的网络系统允许未知身份的参与者加入网络不同（需要通过工作量证明协议来保证交易有效并维护网络的安全），Hyperledger fabric通过MSP来登记所有成员。 cURL是什么？有什么作用？
cURL是一个可以在终端命令行下使用URL语法执行的开源文件传输工具。它支持基于HTTP/Socket的代理；cURL还支持使用SSL证书，支持HTTP POST、HTTP PUT，支持FTP上传，以及基于HTTP表单的上传；支持cookie，可以使用用户名+密码的方式实现认证等。
Hyperledger fabric架构 # 交易流程 # 背书 # 一个示例背书策略可能这样定义：参与区块链网络的四个组织中有三个必须在交易被认为有效之前签署该交易。所有的交易，无论是有效的还是无效的，都会被添加到分布式账本中，但只有有效交易会更新世界状态。
如果一项背书策略指定了必须有不止一个组织来签署交易，那么只有当足够数量的组织都执行了智能合约，才能够生成有效交易。
背书策略是 Hyperledger Fabric 与以太坊（Ethereum）或比特币（Bitcoin）等其他区块链的区别所在。在这些区块链系统中，网络上的任何节点都可以生成有效的交易。而 Hyperledger Fabric 更真实地模拟了现实世界；交易必须由 Fabric 网络中受信任的组织验证。例如，一个政府组织必须签署一个有效的 issueIdentity 交易，或者一辆车的 买家 和 卖家 都必须签署一个 车辆 转移交易。
有效交易 # 当智能合约执行时，它会在区块链网络中组织所拥有的节点上运行。智能合约提取一组名为交易提案的输入参数，并将其与程序逻辑结合起来使用以读写账本。对世界状态的更改被捕获为交易提案响应（或简称交易响应），该响应包含一个读写集，其中既含有已读取的状态，也含有还未书写的新状态（如果交易有效的话）。注意，在执行智能合约时世界状态没有更新！
所有的交易都有一个识别符、一个提案和一个被一群组织签名的响应。所有交易，无论是否有效，都会被记录在区块链上，但仅有效交易会更新世界状态。
一项交易被分发给网络中的所有节点，各节点通过两个阶段对其进行验证。首先，根据背书策略检查交易，确保该交易已被足够的组织签署。其次，继续检查交易，以确保当该交易在受到背书节点签名时它的交易读集与世界状态的当前值匹配，并且中间过程中没有被更新。如果一个交易通过了这两个测试，它就被标记为有效。所有交易，不管是有效的还是无效的，都会被添加到区块链历史中，但是仅有效的交易才会更新世界状态。
共享账本 # Hyperledger Fabric 有一个账本子系统，包括两个组件： 世界状态 和 交易日志 。每个参与者都拥有他们所属的每个 Hyperledger Fabric 网络的账本副本。</description>
    </item>
    
    <item>
      <title>golang力扣刷题（一）</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-10-14-golang%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E4%B8%80/</link>
      <pubDate>Thu, 14 Oct 2021 21:03:50 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-10-14-golang%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E4%B8%80/</guid>
      <description>力扣刷题（一） # 力扣刷题 全部题目模块（1～100）
简单 # 山峰数组 # 符合下列属性的数组 arr 称为 山峰数组（山脉数组） ：arr.length &amp;gt;= 3存在 i（0 &amp;lt; i &amp;lt; arr.length - 1）使得： arr[0] &amp;lt; arr[1] &amp;lt; &amp;hellip; arr[i-1] &amp;lt; arr[i] arr[i] &amp;gt; arr[i+1] &amp;gt; &amp;hellip; &amp;gt; arr[arr.length - 1] 给定由整数组成的山峰数组 arr ，返回任何满足 arr[0] &amp;lt; arr[1] &amp;lt; &amp;hellip; arr[i - 1] &amp;lt; arr[i] &amp;gt; arr[i + 1] &amp;gt; &amp;hellip; &amp;gt; arr[arr.length - 1] 的下标 i ，即山峰顶部。
示例 1：
输入：arr = [0,1,0]输出：1 示例 2：</description>
    </item>
    
    <item>
      <title>go语言基础（一）</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-04-07-go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%B8%80/</link>
      <pubDate>Wed, 07 Apr 2021 16:58:12 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-04-07-go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%B8%80/</guid>
      <description>第一章 概述 # go语言特征 # 简单
并发模型
go语言从根部将一切都并发化，运行时用Goroutine运行所有的一切，包括main.main入口函数。Goroutine是go的显著特征。它用类协程的方式处理并发单元，又在运行时层面做了更深度的优化处理。搭配channel,实现CSP模型。
csp模型
Actor 模型中 Actor 之间就是不能共享内存的，彼此之间通信只能依靠消息传递的方式。Golang 实现的 CSP 模型和 Actor 模型看上去非常相似，虽然 Golang 中协程之间，也能够以共享内存的方式通信，但是并不推荐；而推荐的以通信的方式共享内存，实际上指的就是协程之间以消息传递方式来通信。
Channel模型中，worker之间不直接彼此联系，而是通过不同channel进行消息发布和侦听。消息的发送者和接收者之间通过Channel松耦合，发送者不知道自己消息被哪个接收者消费了，接收者也不知道是哪个发送者发送的消息。
Go语言的CSP模型是由协程Goroutine与通道Channel实现：
Go协程goroutine: 是一种轻量线程，它不是操作系统的线程，而是将一个操作系统线程分段使用，通过调度器实现协作式调度。是一种绿色线程，微线程，它与Coroutine协程也有区别，能够在发现堵塞后启动新的微线程。 通道channel: 类似Unix的Pipe，用于协程之间通讯和同步。协程之间虽然解耦，但是它们和Channel有着耦合。 内存分配
刨去因配合垃圾回收器而修改的内容，内存分配器完整的保留了tcmalloc的原始架构。除偶尔因性能问题而被迫采用对象池和自主内存管理外，我们基本无须参与内存管理操作。
垃圾回收
​ go垃圾回收不咋地
静态链接 只须编译一个可执行文件，无须附加任何东西就能部署。将运行时、依赖库直接打包到可执行文件内部，简化了部署和发布操作，无须事先安装运行环境和下载诸多第三方库。
标准库 工具链 设计初衷 # 少即是多（less is more）：如果一个特性并不对解决任何问题有显著价值，那么go就不提供它；如果需要一个特性，那么只有一种方法去实现 面向接口编程：非侵入式接口，反对继承、反对虚函数和虚函数重载（多态）、删除构造和析构函数 正交+组合的语言特性：语言的特性之间相互独立，不相互影响。比如类型和方法是互相独立的，类型之间也是相互独立的，没有子类，包也没有子包。不同特性用组合的方式来松耦合 并发在语言层面支持：并发更好利用多核，有更强的表现力来模拟真实世界 在设计上，Go秉承了C的简单粗暴。
为什么没有继承 # Go没有子类型的概念，只能把类型嵌入到另一个类型中，所以没有类型系统。Go的作者认为类型系统被过度使用了，应该在这个方向上退一步。
使用伸缩性良好的组合，而不是继承 数据和方法不再绑定在一起，数据的集合用struct，方法的集合用interface，保持正交 类似子类父类的系统造成非常脆弱的代码。类型的层次必须在早期进行设计，通常会是程序设计的第一步，但是一旦写出程序后，早期的决策就很难进行改变了。所以，类型层次结构会促成早期的过度设计，因为程序员要尽力对软件可能需要的各种可能的用法进行预测，不断地为了避免挂一漏万，不断的增加类型和抽象的层次。这种做法有点颠倒了，系统各个部分之间交互的方式本应该随着系统的发展而做出相应的改变，而不应该在一开始就固定下来。
作者附了一个例子，是一些以接口为参数并且其返回结果也是一个接口的函数：
// 入参是接口的函数，而不是成员方法func ReadAll(r io.Reader) ([]byte, error)// 封装器 - 出入参都是接口func LoggingReader(r io.Reader) io.Reader //读到的内容录入日志func LimitingReader(r io.</description>
    </item>
    
    <item>
      <title>fabric网络中的报错（一）</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/fabric%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%8A%A5%E9%94%99%E4%B8%80/</link>
      <pubDate>Tue, 26 Jan 2021 11:17:56 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/fabric%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%8A%A5%E9%94%99%E4%B8%80/</guid>
      <description>重要声明 ，早期写的博客，后面发现里面的某些解决办法是错误的，看个开心就好 ，我也懒得改。 # 报错一： # Error: Could not assemble transaction, err Proposal response was not successful, error code 500, msg error starting container: error starting container: Post http://unix.sock/containers/create?name=dev-peer0.org2.example.com-mycc-1.0: dial unix /host/var/run/docker.sock: connect: no such file or directory
问题原因 # 此问题是由适用于macOS的Docker Desktop的较新版本引起的。
要解决此问题，请在Docker Desktop首选项中，取消选中该框Use gRPC FUSE for file sharing， 以使用旧版osxfs文件共享，然后单击**Apply****＆**Restart
报错二： # 问题原因： # 环境配置问题，进入go.mod文件 重新配置
github.com/Shopify/sarama v1.27.2 // indirectgithub.com/astaxie/beego v1.12.1github.com/fsouza/go-dockerclient v1.7.0 // indirectgithub.com/grpc-ecosystem/go-grpc-middleware v1.2.2 // indirectgithub.</description>
    </item>
    
    <item>
      <title>argparse</title>
      <link>https://chain-code.github.io/docs/python/package/argparse/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/argparse/</guid>
      <description>argparse # 在命令行程序中，经常需要获取命令行参数。Python内置的sys.argv保存了完整的参数列表，我们可以从中解析出需要的参数：
# copy.py import sys print(sys.argv) source = sys.argv[1] target = sys.argv[2] # TODO... 运行上述copy.py，并传入参数，打印如下：
[&amp;#39;copy.py&amp;#39;, &amp;#39;source.txt&amp;#39;, &amp;#39;copy.txt&amp;#39;] 这种方式能应付简单的参数，但参数稍微复杂点，比如可以使用-d复制目录，使用--filename *.py过滤文件名等，解析起来就非常麻烦。
为了简化参数解析，我们可以使用内置的argparse库，定义好各个参数类型后，它能直接返回有效的参数。
假设我们想编写一个备份MySQL数据库的命令行程序，需要输入的参数如下：
host参数：表示MySQL主机名或IP，不输入则默认为localhost； port参数：表示MySQL的端口号，int类型，不输入则默认为3306； user参数：表示登录MySQL的用户名，必须输入； password参数：表示登录MySQL的口令，必须输入； gz参数：表示是否压缩备份文件，不输入则默认为False； outfile参数：表示备份文件保存在哪，必须输入。 其中，outfile是位置参数，而其他则是类似--user root这样的“关键字”参数。
用argparse来解析参数，一个完整的示例如下：
# backup.py import argparse def main(): # 定义一个ArgumentParser实例: parser = argparse.ArgumentParser( prog=&amp;#39;backup&amp;#39;, # 程序名 description=&amp;#39;Backup MySQL database.&amp;#39;, # 描述 epilog=&amp;#39;Copyright(r), 2023&amp;#39; # 说明信息 ) # 定义位置参数: parser.add_argument(&amp;#39;outfile&amp;#39;) # 定义关键字参数: parser.add_argument(&amp;#39;--host&amp;#39;, default=&amp;#39;localhost&amp;#39;) # 此参数必须为int类型: parser.add_argument(&amp;#39;--port&amp;#39;, default=&amp;#39;3306&amp;#39;, type=int) # 允许用户输入简写的-u: parser.</description>
    </item>
    
    <item>
      <title>C&#43;&#43;部署PaddleOCR</title>
      <link>https://chain-code.github.io/docs/c/c&#43;&#43;%E9%83%A8%E7%BD%B2paddleocr/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/c/c&#43;&#43;%E9%83%A8%E7%BD%B2paddleocr/</guid>
      <description> # </description>
    </item>
    
    <item>
      <title>CGo</title>
      <link>https://chain-code.github.io/docs/c/cgo/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/c/cgo/</guid>
      <description>CGO入门 # Golang 自带的 CGO 可以支持与 C 语言接口的互通。
Go 与 C 的桥梁：cgo 入门，剖析与实践 - 知乎 (zhihu.com)
启用CGO特性 # 在 golang 代码中加入 import “C” 语句就可以启动 CGO 特性。这样在进行 go build 命令时，就会在编译和连接阶段启动 gcc 编译器。
package mainimport &amp;#34;C&amp;#34; // import &amp;#34;C&amp;#34;更像是一个关键字，CGO工具在预处理时会删掉这一行func main() {} 使用 -x 选项可以查看 go 程序编译过程中执行的所有指令。可以看到 golang 编译器已经为 test1.go 创建了 CGO 编译选项
[root@VM-centos ~/cgo_test/golink2]# go build -x test1.goWORK=/tmp/go-build330287398mkdir -p $WORK/b001/cd /root/cgo_test/golink2CGO_LDFLAGS=&amp;#39;&amp;#34;-g&amp;#34; &amp;#34;-O2&amp;#34;&amp;#39; /usr/lib/golang/pkg/tool/linux_amd64/cgo -objdir $WORK/b001/ -importpath command-line-arguments -- -I $WORK/b001/ -g -O2 .</description>
    </item>
    
    <item>
      <title>CGO遇到的问题解决</title>
      <link>https://chain-code.github.io/docs/c/cgo%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/c/cgo%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</guid>
      <description>CGO调用海康威视SDK # 问题一：宏定义问题 # 在C语言中，extern &amp;quot;C&amp;quot; 是用于指定C++编译器按照C语言的方式进行函数名的命名规则和链接的修饰符。然而，根据您提供的错误信息，您正在使用的是C语言的编译器（gcc），而不是C++编译器。
cgo: gcc errors for preamble:In file included from .\hikvision.go:6:0:error: expected identifier or &amp;#39;(&amp;#39; before string constant#define NET_DVR_API extern &amp;#34;C&amp;#34; __declspec(dllimport)note: in definition of macro &amp;#39;NET_DVR_API&amp;#39;#define NET_DVR_API extern &amp;#34;C&amp;#34; __declspec(dllimport)^~~ 添加这个：
#ifndef __cplusplus#define NET_DVR_API#else#define NET_DVR_API extern &amp;#34;C&amp;#34;#endif 原文件
#ifndef _HC_NET_SDK_H_ #define _HC_NET_SDK_H_ #ifndef _WINDOWS_ #if (defined(_WIN32) || defined(_WIN64)) #include &amp;lt;winsock2.h&amp;gt; #include &amp;lt;windows.h&amp;gt; #endif #endif #if defined(_WIN64) #define OS_WINDOWS64 1 #endif #if defined(__LP64__) #define OS_POSIX64 1 #endif #ifndef __PLAYRECT_defined #define __PLAYRECT_defined typedef struct __PLAYRECT { int x; int y; int uWidth; int uHeight; }PLAYRECT; #endif #if (defined(_WIN32)) //windows //#define NET_DVR_API extern &amp;#34;C&amp;#34; __declspec(dllimport) 防止宏被重复定义 这里注释掉 typedef unsigned __int64 UINT64; typedef signed __int64 INT64; #elif defined(__linux__) || defined(__APPLE__) //linux #define BOOL int typedef unsigned int DWORD; typedef unsigned short WORD; typedef unsigned short USHORT; typedef short SHORT; typedef int LONG; typedef unsigned char BYTE; typedef unsigned int UINT; typedef void* LPVOID; typedef void* HANDLE; typedef unsigned int* LPDWORD; typedef unsigned long long UINT64; typedef signed long long INT64; #ifndef TRUE #define TRUE 1 #endif #ifndef FALSE #define FALSE 0 #endif #ifndef NULL #define NULL 0 #endif #define __stdcall #define CALLBACK #define NET_DVR_API extern &amp;#34;C&amp;#34; typedef unsigned int COLORKEY; typedef unsigned int COLORREF; #ifndef __HWND_defined #define __HWND_defined #if defined(__APPLE__) || defined(ANDROID) typedef void* HWND; #elif defined(__linux__) typedef unsigned int HWND; #else typedef void* HWND; #endif #endif #ifndef __HDC_defined #define __HDC_defined #if defined(__linux__) typedef struct __DC { void* surface; //SDL Surface HWND hWnd; //HDC window handle }DC; typedef DC* HDC; #else typedef void* HDC; #endif #endif typedef struct tagInitInfo { int uWidth; int uHeight; }INITINFO; #endif #ifndef __cplusplus #define NET_DVR_API #else #define NET_DVR_API extern &amp;#34;C&amp;#34; #endif 问题二：C语言不支持在函数参数列表中使用默认参数 # cgo: gcc errors for preamble:In file included from .</description>
    </item>
    
    <item>
      <title>Channel</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/channel/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/channel/</guid>
      <description>channel # 遍历一个未关闭的channel会造成死循环
channel的使用场景 # 把channel用在数据流动的地方：
消息传递、消息过滤 信号广播 事件订阅与广播 请求、响应转发 任务分发 结果汇总 并发控制 同步与异步 … channel的基本操作和注意事项 # channel存在3种状态：
nil，未初始化的状态，只进行了声明，或者手动赋值为nil active，正常的channel，可读或者可写 closed，已关闭，千万不要误认为关闭channel后，channel的值是nil channel可进行3种操作：
读 写 关闭 把这3种操作和3种channel状态可以组合出9种情况：
操作 nil的channel 正常channel 已关闭channel &amp;lt;- ch 阻塞 成功或阻塞 读到零值 ch &amp;lt;- 阻塞 成功或阻塞 panic close(ch) panic 成功 panic 对于nil通道的情况，也并非完全遵循上表，有1个特殊场景：当nil的通道在select的某个case中时，这个case会阻塞，但不会造成死锁。
如何判断通道为空 # if len(channel) == 0 {// 通道为空} select {case &amp;lt;-channel:// 通道不为空，可以接收元素default:// 通道为空} 如何判断通道已关闭 # v, ok := &amp;lt;-ch 通道各种花里胡哨用法</description>
    </item>
    
    <item>
      <title>collections</title>
      <link>https://chain-code.github.io/docs/python/package/collections/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/collections/</guid>
      <description>collections # collections是Python内建的一个集合模块，提供了许多有用的集合类。
namedtuple # 我们知道tuple可以表示不变集合，例如，一个点的二维坐标就可以表示成：
&amp;gt;&amp;gt;&amp;gt; p = (1, 2) 但是，看到(1, 2)，很难看出这个tuple是用来表示一个坐标的。
定义一个class又小题大做了，这时，namedtuple就派上了用场：
&amp;gt;&amp;gt;&amp;gt; from collections import namedtuple &amp;gt;&amp;gt;&amp;gt; Point = namedtuple(&amp;#39;Point&amp;#39;, [&amp;#39;x&amp;#39;, &amp;#39;y&amp;#39;]) &amp;gt;&amp;gt;&amp;gt; p = Point(1, 2) &amp;gt;&amp;gt;&amp;gt; p.x 1 &amp;gt;&amp;gt;&amp;gt; p.y 2 namedtuple是一个函数，它用来创建一个自定义的tuple对象，并且规定了tuple元素的个数，并可以用属性而不是索引来引用tuple的某个元素。
这样一来，我们用namedtuple可以很方便地定义一种数据类型，它具备tuple的不变性，又可以根据属性来引用，使用十分方便。
可以验证创建的Point对象是tuple的一种子类：
&amp;gt;&amp;gt;&amp;gt; isinstance(p, Point) True &amp;gt;&amp;gt;&amp;gt; isinstance(p, tuple) True 类似的，如果要用坐标和半径表示一个圆，也可以用namedtuple定义：
# namedtuple(&amp;#39;名称&amp;#39;, [属性list]): Circle = namedtuple(&amp;#39;Circle&amp;#39;, [&amp;#39;x&amp;#39;, &amp;#39;y&amp;#39;, &amp;#39;r&amp;#39;]) deque # 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。
deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：
&amp;gt;&amp;gt;&amp;gt; from collections import deque &amp;gt;&amp;gt;&amp;gt; q = deque([&amp;#39;a&amp;#39;, &amp;#39;b&amp;#39;, &amp;#39;c&amp;#39;]) &amp;gt;&amp;gt;&amp;gt; q.</description>
    </item>
    
    <item>
      <title>crontab使用</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/crontab%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/crontab%E4%BD%BF%E7%94%A8/</guid>
      <description>简介 # Linux crontab 是 Linux 系统中用于设置周期性被执行的指令的命令。
当安装完成操作系统之后，默认便会启动此任务调度命令。
crond 命令每分钟会定期检查是否有要执行的工作，如果有要执行的工作便会自动执行该工作。
**注意：**新创建的 cron 任务，不会马上执行，至少要过 2 分钟后才可以，当然你可以重启 cron 来马上执行。
Linux 任务调度的工作主要分为以下两类：
**1、系统执行的工作：**系统周期性所要执行的工作，如备份系统数据、清理缓存 **2、个人执行的工作：**某个用户定期要做的工作，例如每隔 10 分钟检查邮件服务器是否有新信，这些工作可由每个用户自行设置 语法 # crontab [ -u user ] file crontab [ -u user ] { -l | -r | -e } 说明：
crontab 是用来让使用者在固定时间或固定间隔执行程序之用，换句话说，也就是类似使用者的时程表。
-u user 是指设定指定 user 的时程表，这个前提是你必须要有其权限(比如说是 root)才能够指定他人的时程表。如果不使用 -u user 的话，就是表示设定自己的时程表。
参数说明：
-e : 执行文字编辑器来设定时程表，内定的文字编辑器是 Vi/Vim，如果你想用别的文字编辑器，则请先设定 VISUAL 环境变数来指定使用那个文字编辑器(比如说 setenv VISUAL joe) -r : 删除目前的时程表 -l : 列出目前的时程表 查看当前用户的 crontab 文件：</description>
    </item>
    
    <item>
      <title>cursor续杯</title>
      <link>https://chain-code.github.io/docs/ai/basic/cursor%E7%BB%AD%E6%9D%AF/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/cursor%E7%BB%AD%E6%9D%AF/</guid>
      <description>https://github.com/yeongpin/cursor-free-vip?tab=readme-ov-file
Windows：PowerShell
irm https://raw.githubusercontent.com/yeongpin/cursor-free-vip/main/scripts/install.ps1 | iex 先重置机器码
再用2925邮箱注册账号
不断更改账号</description>
    </item>
    
    <item>
      <title>datetime</title>
      <link>https://chain-code.github.io/docs/python/package/detetime/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/detetime/</guid>
      <description>datetime # datetime是Python处理日期和时间的标准库。
获取当前日期和时间 # 我们先看如何获取当前日期和时间：
&amp;gt;&amp;gt;&amp;gt; from datetime import datetime &amp;gt;&amp;gt;&amp;gt; now = datetime.now() # 获取当前datetime &amp;gt;&amp;gt;&amp;gt; print(now) 2015-05-18 16:28:07.198690 &amp;gt;&amp;gt;&amp;gt; print(type(now)) &amp;lt;class &amp;#39;datetime.datetime&amp;#39;&amp;gt; 注意到datetime是模块，datetime模块还包含一个datetime类，通过from datetime import datetime导入的才是datetime这个类。
如果仅导入import datetime，则必须引用全名datetime.datetime。
datetime.now()返回当前日期和时间，其类型是datetime。
获取指定日期和时间 # 要指定某个日期和时间，我们直接用参数构造一个datetime：
&amp;gt;&amp;gt;&amp;gt; from datetime import datetime &amp;gt;&amp;gt;&amp;gt; dt = datetime(2015, 4, 19, 12, 20) # 用指定日期时间创建datetime &amp;gt;&amp;gt;&amp;gt; print(dt) 2015-04-19 12:20:00 datetime转换为timestamp # 在计算机中，时间实际上是用数字表示的。我们把1970年1月1日 00:00:00 UTC+00:00时区的时刻称为epoch time，记为0（1970年以前的时间timestamp为负数），当前时间就是相对于epoch time的秒数，称为timestamp。
你可以认为：
timestamp = 0 = 1970-1-1 00:00:00 UTC+0:00 对应的北京时间是：</description>
    </item>
    
    <item>
      <title>Decord</title>
      <link>https://chain-code.github.io/docs/python/package/decord/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/decord/</guid>
      <description>分关键帧记录 # import numpy as np from typing import Union, BinaryIO import decord def _read_video_key_frames2( video_file: Union[str, BinaryIO], start_time: int, end_time: int ) -&amp;gt; Generator[tuple[np.ndarray, int, int], None, None]: &amp;#34;&amp;#34;&amp;#34; 获取视频中的关键帧 :param video_file: 视频文件路径/流 :param start_time: 视频开始时间（毫秒） :param end_time: 视频结束时间（毫秒） :return: 生成器，产出（帧numpy数组，累计计数，时间戳） &amp;#34;&amp;#34;&amp;#34; try: # 初始化视频读取器（CPU 解码） video = decord.VideoReader(video_file) except Exception as e: raise RuntimeError(f&amp;#34;Failed to open video with Decord: {str(e)}&amp;#34;) frame_count = 0 total_frames = len(video) print(f&amp;#34;Total frames: {total_frames}&amp;#34;) fps = video.</description>
    </item>
    
    <item>
      <title>Docker问题</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/docker%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/docker%E9%97%AE%E9%A2%98/</guid>
      <description>为什么使用docker # Docker 是一个高效的轻量级容器化解决方案，**可以让应用程序快速部署，安全运行，并且使其更容易跨环境迁移和管理。**Docker 可以为应用程序提供一个可复用的容器，从而可以在同一操作系统中和不同操作系统中部署应用，实现跨平台应用开发。它可以简化应用程序的部署，提高性能，确保应用程序的完整性和可靠性，以及降低运行和管理环境的成本。
优势：
节约资源：通过分割物理机的硬件资源，将其抽象出来，在需要的时候为不同的容器提供资源，这样就能减小磁盘和内存的开销； 轻量级：Docker容器本身只有几百兆的大小，而且不需要安装宿主操作系统，能够极大地缩小应用程序启动时间； 灵活性：Docker支持跨平台的部署，可以更灵活的部署应用程序和微服务； 弹性：Docker可以更加简单的增加节点，并能够实现服务的弹性扩容和缩容； 安全性：根据容器隔离，可以避免恶意攻击，提高应用程序的安全性； 可重复性：docker容器可以在任何地方重复利用，从而大大减少了管理时间和成本； 可移植性：Docker容器可以在开发环境和生产环境之间更加容易的移植。 有了docker为啥还要k8s? # Docker是一个容器编排工具，可以帮助我们管理应用程序。但是，如果想要管理大量的容器，就需要更为强大的编排工具，而 Kubernetes（K8s）就是这样一个工具。
K8s 由多个容器组成，并且能够自动检测、部署和扩缩容，它提供了资源管理、服务发现、多租户支持等功能，从而管理和调度大量容器，让整个编排更加高效、可靠、稳定。因此，K8s不仅仅是让Docker容器能够更好的运行，而且它可以用于管理任何容器化的部署。
容器与虚拟机的区别是什么？ # 虚拟机是基于硬体的多个客户操作系统，由虚拟机监视器实现。 容器是应用程序级构造，并模拟共享单个内核的多个虚拟环境。
虚拟机包含操作系统的完整副本，包括内核及所有驱动和应用，因此占用更多的磁盘空间和内存。
容器在操作系统层面进行虚拟化，共享宿主机的操作系统内核。容器由容器引擎（如 Docker）管理，所有容器可以运行在单一的操作系统实例上。
Docker File是什么？它通常包含哪些指令？ # Dockerfile 是一个文本文件，用于自动化地构建 Docker 镜像。Dockerfile 定义了从基础镜像开始，按步骤配置环境和部署应用所需要的所有命令。
# 使用 Java 11 官方镜像作为基础镜像 FROM openjdk:11-jdk # 设置维护者信息 LABEL maintainer=&amp;#34;hollis@hollis.com&amp;#34; # 环境变量，可以设置默认的环境变量用于应用配置 ENV SPRING_PROFILES_ACTIVE=prod \ JAVA_OPTS=&amp;#34;-Xmx512m -Xms256m&amp;#34; # 设置容器内的工作目录 WORKDIR /app # 将 jar 包复制到工作目录 COPY target/my-spring-boot-app.jar my-app.jar # 指定对外暴露的端口号 EXPOSE 8080 # 容器健康检查，定期检查应用是否响应 HEALTHCHECK --interval=1m --timeout=3s \ CMD curl -f http://localhost:8080/actuator/health || exit 1 # 使用 ENTRYPOINT 和 CMD 以提供默认执行命令，同时允许覆盖参数 ENTRYPOINT [&amp;#34;java&amp;#34;, &amp;#34;-jar&amp;#34;, &amp;#34;my-app.</description>
    </item>
    
    <item>
      <title>Elastic Search面试题</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/elasticsearch%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/elasticsearch%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description>为什么使用Elastic Search？和传统关系型数据库有什么不同？ # 数据类型 # ElasticSearch是基于文档的搜索引擎，它使用json文档来存储数据。在Elastic search中，相关数据通常存储在一个文档中，而不是分数在多个表中。
mysql是一个关系型数据库管理系统，它使用表、行和列的结构来组织数据。数据通过外键关系分散在多个表中。
查询语言 # ElasticSearch使用Query DSL，是一种非常灵活的查询语言，基于json，支持全文搜索、复合查询、过滤以及聚合等。
Mysql使用SQL，是一种强类型和非常成熟的语言，专门用于查询和管理关系型数据库。
全文搜索 # ElasticSerach的**核心功能是全文搜索。**它对数据进行索引时会自动建立全文搜索引擎，使其在搜索大量文本数据时表现优异。
Mysql虽然也提供了全文搜索功能，但其主要设计目标是处理结构化数据的存储和查询，对全文搜索的支持不如Elastic Search.
事务支持 # Elasticsearch不支持传统事务。虽然它确保了单个文档操作的原子性，但不适用于跨多个文档的复杂事务。
主要场景和优势 # Elasticsearch是一个开源的分布式搜索和分析引擎，主要适用于以下场景：
搜索引擎：用于快速检索文档、商品、新闻等 日志分析：通过分析日志数据，帮助企业了解其业务性能情况。 数据分析：帮助数据科学家和数据分析师进行数据分析，以获取有价值信息。 商业智能：帮助企业制定数据驱动决策，已实现商业上的成功 实时监控：帮助企业实时检测系统性能、监控数据变化，以保证系统正常运行。 安全性：帮助企业保证数据安全性，保证数据不被非法窃取 应用程序开发：帮助开发人员开发基于搜索的应用程序，以增加用户体验。 优势：
高性能：具有高性能搜索和分析能力，其中涵盖了多种查询语言和数据结构 可扩展性：是分布式的，可以通过增加节点数量扩展搜索和分析能力 灵活性：支持多种数据类型，支持多种语言，支持动态映射，允许快速地调整模型以适应不同需求 实时分析：支持实时分析，可以对数据进行实时查询，这对快速检索数据非常有用 可靠性：具有可靠性和高可用性，支持数据备份和恢复。 Elasticsearch为什么这么快？ # Elasticsearch是一个高性能、分布式搜索引擎，它之所以快，主要有以下几个原因：
分布式存储：Elasticsearch使用分布式存储技术，将数据存储在多个节点上，从而减少单个节点的压力，提高整体性能。 索引分片：Elasticsearch把每个索引划分成多个分片，这样可以让查询操作并行化，从而提高查询速度。 全文索引：Elasticsearch使用了高效的全文索引技术，把文档转化成可搜索的结构化数据，使得搜索操作快速高效。 倒排索引：Elasticsearch支持倒排索引这种数据结构，**倒排索引将文档中的每个词与该词出现在哪些文档中进行映射，并存储这些信息。**当搜索请求发生时，ES可以快速查找包含所有搜索词的文档，从而返回结果。 索引优化：Elasticsearch通过索引优化技术，可以使查询速度更快。例如，它支持索引覆盖、索引下推等优化技术，使得查询速度更快。 预存储结果：Elasticsearch在插入数据时，对数据进行预处理，把结果预存储到索引中，从而在查询时不需要再重新计算，提高查询速度。 高效的查询引擎：Elasticsearch使用了高效的查询引擎，支持各种类型的查询，并对复杂查询提供了优化策略，从而提高查询速度。 异步请求处理：ES使用了异步请求处理机制，能够在请求到达时立即返回，避免长时间的等待，提高用户体验。 内存存储：ES使用了内存存储技术，能够在读写数据时大大减少磁盘访问次数，提高数据存储和查询效率。 倒排索引是什么？ # 在 ElasticSearch 中，倒排索引是一种常用的索引结构，用于快速搜索文档中的某个词汇。
倒排索引的结构与传统的索引结构相反，传统的索引结构是由文档构成的，每个文档包含了若干个词汇，然后根据这些词汇建立索引。而倒排索引是由词汇构成的，每个词汇对应了若干个文档，然后根据这些文档建立索引。
对于一个包含多个词汇的文档，倒排索引会将每个词汇作为一个关键字（Term），然后记录下该词汇所在的文档编号（Document ID）及该词汇在文档中的位置（Term Position）。这样，当用户输入一个关键字时，就可以快速地查找到包含该关键字的文档编号，然后通过文档编号再查找到对应的文档内容。
**倒排索引的优点在于它可以快速定位包含关键字的文档，而且可以支持复杂的搜索操作，如词组搜索、通配符搜索等。**同时，由于倒排索引是由词汇构成的，因此在进行数据分析和统计时也非常有用。在 ElasticSearch 中，倒排索引是一种非常重要的索引结构，它被广泛应用于搜索引擎、日志分析、推荐系统等领域。
倒排索引建立过程 # ES中的倒排索引建立过程主要有2个步骤，分别是分词、建立倒排索引
比如我们现在有三份文档内容，分别是
id content 1 深入理解Java核心技术—Hollis 2 深入理解Java虚拟机—周志明 3 Java编程思想—布鲁斯·埃克尔 分词</description>
    </item>
    
    <item>
      <title>Flag</title>
      <link>https://chain-code.github.io/docs/golang/package/flag/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/flag/</guid>
      <description># flag # 概述 # flag包提供了一系列解析命令行参数的功能接口，官方教程的地址为：https://golang.org/pkg/flag/#pkg-overview
命令行语法 # 命令行语法主要有以下几种形式
-flag //只支持bool类型-flag=x-flag x //只支持非bool类型 以上语法对于一个或两个‘－’号，效果是一样的，但是要注意对于第三种情况，只支持非bool类型，原因是碰到如下情况时
cmd -x * *为0，false有可能表示一个文件名或文件，也有可能表示x标签的值为0或false，会产生二义性，因此规定第三种只支持非bool类型。对于整形flag，合法的值可以为1234, 0664,0x1234或负数等。对于布尔型flag，可以为1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False等
命令行参数解析 # flag.Parse() 解析函数将会在碰到第一个非flag命令行参数时停止，非flag命令行参数是指不满足命令行语法的参数，如命令行参数为cmd --flag=true abc则第一个非flag命令行参数为“abc”
调用Parse解析后，就可以直接使用flag本身(指针类型)或者绑定的变量了(值类型) fmt.Println(&amp;#34;ip has value &amp;#34;, *ip) fmt.Println(&amp;#34;flagvar has value &amp;#34;, flagvar) 12 还可通过flag.Args(), flag.Arg(i)来获取非flag命令行参数
如果需要每个函数的详细demo，可参见Gopkg:flag
命令行参数解析方法 # 使用flag主要包括以下几步
定义flag参数，有三种方式
通过flag.String(), Bool(), Int() 等flag.Xxx()方法，该种方式返回一个相应的指针 import &amp;#34;flag&amp;#34; var ip = flag.Int(&amp;#34;flagname&amp;#34;, 1234, &amp;#34;help message for flagname&amp;#34;) 通过flag.</description>
    </item>
    
    <item>
      <title>Gin参数绑定</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/gin%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/gin%E5%8F%82%E6%95%B0%E7%BB%91%E5%AE%9A/</guid>
      <description>奇怪问题 # type BugReportParam struct { Desc string `json:&amp;#34;desc&amp;#34;`	Stack string `json:&amp;#34;stack&amp;#34;` Function string `json:&amp;#34;function&amp;#34;` Contact string `json:&amp;#34;contact&amp;#34;` Email string `json:&amp;#34;email&amp;#34;` ExtFiles []string `json:&amp;#34;extFiles&amp;#34;` } // BugReport godoc // //	@Description	日志反馈 //	@Tags	bug_Report //	@Accept	json //	@Produce	json //	@Param	body	body	param.BugReportParam	true	&amp;#34;日志反馈参数&amp;#34; //	@Success	200	{object}	response.Response //	@Failure	500	{object}	response.Response //	@Router	/bugreport [post] func (r BugReport) BugReport(c *gin.Context) { var par param.</description>
    </item>
    
    <item>
      <title>Go 中使用 sync.Pool 时可能遇到的陷阱</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go-%E4%B8%AD%E4%BD%BF%E7%94%A8-sync.pool-%E6%97%B6%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%99%B7%E9%98%B1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go-%E4%B8%AD%E4%BD%BF%E7%94%A8-sync.pool-%E6%97%B6%E5%8F%AF%E8%83%BD%E9%81%87%E5%88%B0%E7%9A%84%E9%99%B7%E9%98%B1/</guid>
      <description>虽然 sync.Pool 使用起来很简单，但如果使用不当，还是有可能出现问题的，例如，
对象重置不彻底：若未正确重置对象状态（例如忘记清空某个字段），可能导致脏数据污染，引发逻辑错误。 长时间占用对象：若某个 Goroutine 持有对象时间过长（例如执行阻塞操作），可能导致其他 Goroutine 频繁创建新对象，削弱池的优势。 高并发下池对象不足问题：若并发量远高于池中对象数量，New 函数会被频繁调用，可能抵消复用对象的收益。 返回池对象引用：应该限制池对象仅在局部作用域内使用，不应暴露给外部，否则会导致 data race 问题。 由于前两个问题相对简单直接，本文仅讨论后两个问题。
关于“高并发下池对象不足问题” # 下面通过真实场景案例，分析一下 “并发量远高于池中对象数量，导致 New 函数频繁调用，抵消对象复用收益” 的问题。
案例 1：高并发 HTTP 服务器的 JSON 序列化缓冲区 # 场景描述 # 某电商平台的商品详情接口需要处理 每秒 10 万次（QPS） 的请求。每个请求需要将商品数据序列化为 JSON 返回给客户端。为优化性能，开发团队使用 sync.Pool 缓存 *bytes.Buffer 对象，避免重复分配内存。
代码实现
效果
New 调用频率下降 70%，GC 频率恢复至正常水平。 请求延迟从 15ms 降低到 5ms。 *案例 2：实时日志处理系统的临时对象池* # 场景描述
某日志分析服务需实时解析海量日志条目（每秒 50 万条），每条日志需解析为结构体并暂存到内存队列。开发团队使用 sync.Pool 缓存日志解析的临时结构体对象。
代码实现
问题现象
内存占用波动剧烈：服务内存使用量周期性骤增，与 GC 周期高度相关。 解析性能下降：日志处理吞吐量从 50 万条/秒下降到 30 万条/秒。 根因分析</description>
    </item>
    
    <item>
      <title>golangci-lint</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golangci-lint/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golangci-lint/</guid>
      <description>golangci-lint # golangci-lint按照官方的说法是用于go语言的代码静态检查工具集（因为包含它多种 Go 代码检查工具）。
官网 https://golangci-lint.run/
特性： 1.快速：并行非执行 linters，可以复用 Go构建cache和caches分析结果 2.配置文件基于yaml语法进行配置 3.可以与常见开发工具集成，例如：VS Code、Sublime、Goland、Emacs、Vim、Atom、Github Actions 4.包含了很多 linters，不需要安装 5.执行结果输出带有美观，不仅带有颜色，还有源码行号和标识 6.尽可能的减少误报，可以通过设置忽略某些模式
golangci-lint 的安装 # # macosbrew install golangci-lintbrew upgrade golangci-lint# linux and windows# binary will be $(go env GOPATH)/bin/golangci-lintcurl -sSfL https://raw.githubusercontent.com/golangci/golangci-lint/master/install.sh | sh -s -- -b $(go env GOPATH)/bin v1.46.2golangci-lint --version 源码安装
# Go 1.16+go install github.com/golangci/golangci-lint/cmd/golangci-lint@v1.46.2# Go version &amp;lt; 1.16go get -u github.</description>
    </item>
    
    <item>
      <title>Golang控流</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golang%E6%8E%A7%E6%B5%81/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golang%E6%8E%A7%E6%B5%81/</guid>
      <description>概述 # 流控(Rate limiting)是构建可扩展弹性系统的重要技术之一，目的是通过限制指定时间内允许通过的请求数量来控制流量。在 Go 中实施流控可以确保最佳的资源利用率，并保护应用不被过多的流量或滥用行为所冲垮。
流控包括定义一套规则，确定客户端在给定时间窗口内可以发出多少请求，从而确保系统能够处理负载，防止滥用或拒绝服务攻击。两种常见的流控方法是：
拒绝服务攻击（Denial of Service, DoS）是一种恶意行为，旨在剥夺合法用户访问网络服务或资源的能力。在拒绝服务攻击中，攻击者通过采取各种手段使目标系统或网络资源过载或不可用，从而阻止合法用户访问。
拒绝服务攻击的目标可以是各种网络服务，例如网站、服务器、路由器、域名系统（DNS）等。攻击者可能利用系统或网络的弱点，通过发送大量请求、占用资源、耗尽带宽或利用其他漏洞来导致服务不可用。
固定窗口控流：在这种方法中，在一个固定时间窗口内执行控流。例如，如果流控设置为每分钟 100 个请求，则系统在任何给定的 60 秒窗口内最多允许 100 个请求，超过此限制的请求将被拒绝或延迟到下一个时间窗口。 令牌桶控流：令牌桶控流基于令牌从桶中消失的概念。令牌桶最初装满固定数量的令牌，每个令牌代表一个请求。当客户端要发出请求时，必须从桶中获取一个令牌。如果令牌桶是空的，客户端必须等待，直到有令牌可用。 Go 提供了一个名为 golang.org/x/time/rate 的内置软件包，实现了流控功能。接下来我们看看如何使用固定窗口和令牌桶两种方法来实现流控。
固定窗口控流 # package main import ( &amp;#34;fmt&amp;#34; &amp;#34;golang.org/x/time/rate&amp;#34; &amp;#34;time&amp;#34; ) func main() { limiter := rate.NewLimiter(rate.Limit(100), 1) // Allow 100 requests per second for i := 0; i &amp;lt; 200; i++ { if !limiter.Allow() { fmt.Println(&amp;#34;Rate limit exceeded. Request rejected.&amp;#34;) continue } // Process the request fmt.</description>
    </item>
    
    <item>
      <title>golang限流实践</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golang%E9%99%90%E6%B5%81%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/golang%E9%99%90%E6%B5%81%E5%AE%9E%E8%B7%B5/</guid>
      <description>前言 # 在编写服务的过程中，我们会遇到需要对服务接入请求进行限流的情况。通过对不同用户、访问内容的限流，我们可以减轻单个服务收到较大流量时对其他系统内服务的影响，防止系统因为单个来源的请求而无法及时对其他来源的请求响应，同时也可以减轻后续服务处理的压力。
当前系统限流方式 # 当前的服务中主要使用了令牌桶对用户和 token api 接入的请求进行了区分限流。令牌桶是一个比较均衡的限流方式，在能保证均匀恢复限流阈值的同时，还允许按照设定的容量值进行动态增加容量和速率。
起始时令牌桶中填满了可用的令牌，当有新的请求进入后，会从令牌桶取走令牌，同时令牌桶按照指定的速率恢复令牌。当令牌桶的令牌被取完后，新的请求因无法获取到令牌而被拒绝，直至令牌恢复到最小单个请求所需的令牌数量后，新的请求才可通过。也就是说请求到来的速度如果始终大于令牌产生的速度时，在消耗完桶中已存有的令牌后，超出部分的请求会被拒绝；桶内存有的令牌允许短时间内接收等于令牌数量的请求，即允许按照配置的数量（容量）处理突发的请求。
系统中设置了两个令牌桶限流器组，分别是用户令牌桶组和 api 令牌桶组。以 api 令牌桶组为例，配置令牌桶添加令牌的速率为 20 / s，桶的容量为 30（即在桶中的令牌未被取走前最大允许同时有 30 个请求进入）。桶容量一般设置为速率的 0.6~1.5 倍之间。
var (userLimiters = NewLimiters(&amp;#34;user&amp;#34;, 10, 15) // 用户限流tokenLimiters = NewLimiters(&amp;#34;token&amp;#34;, 20, 30) // api 限流)func NewLimiters(_type string, r float64, size int) *Limiters {return &amp;amp;Limiters{_type: _type,r: rate.Limit(r),b: size,l: sync.RWMutex{},m: make(map[int64]*rate.Limiter),}}type Limiters struct {_type string // 限流类型r rate.</description>
    </item>
    
    <item>
      <title>Go内存对齐</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/</guid>
      <description>内存对齐就是让数据在内存中的地址按照某种规则排列，通常是某个特定数字（比如 4 或 8）的倍数。为什么要这样做？因为处理器读取数据时效率最高的方式是一次性读取整个“块”，而不是零敲碎打。
举个例子，假设你有一个 int32 类型的变量，占 4 个字节。如果它的内存地址是 4 的倍数，处理器可以一步到位读取整个变量。但如果地址偏了，比如是 5，处理器就得分成两次读取，还要额外拼接数据，效率自然下降。
在 Golang 中，结构体字段会根据它们的类型和顺序自动对齐。合理的字段设计不仅能减少内存浪费，还能显著提升性能，尤其是在大数据处理或高并发场景下。
Golang 中的内存对齐 # Golang 的编译器会自动为结构体字段进行内存对齐，但这并不意味着我们可以完全撒手不管。字段的顺序直接影响内存布局和程序性能。让我们通过一个例子来看看。
未优化的结构体 # type User struct { Age int32 // 4 字节 IsAdmin bool // 1 字节 Score int64 // 8 字节 } 在这个结构体中，Age 是 4 字节，IsAdmin 是 1 字节，Score 是 8 字节。由于 Score 需要在 8 字节对齐的地址上，IsAdmin 后面可能会填充 3 个字节的“空隙”（padding）。最终，这个结构体的大小可能高达 24 字节（4 + 1 + 3 + 8 + 额外填充）。
优化后的结构体 # 现在，我们调整一下顺序：</description>
    </item>
    
    <item>
      <title>go泛型介绍</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E6%B3%9B%E5%9E%8B%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E6%B3%9B%E5%9E%8B%E4%BB%8B%E7%BB%8D/</guid>
      <description>什么是泛型 # 泛型是一种编程特性，允许你编写更通用的代码。 泛型可以让你编写一个函数或类型，而不是针对特定的数据类型。 这样，你可以使用相同的函数或类型处理不同的数据类型，而无需为每种数据类型编写重复的代码，在python和其他语言中很早就被支持了，但是在go中直到1.18版本之后才被支持。
为什么需要泛型 # 假如我们需要计算两数之和
func Add(a int, b int) int {return a + b} 此时，如果我们需要去计算其他类型的，比如浮点或者字符串的和，就需要新建方法去实现
func AddFloat32(a float32, b float32) float32 {return a + b}func AddString(a string, b string) string {return a + b} 我们也可以使用反射去解决问题，但是使用反射在运行期间获取变量类型会降低代码的执行效率并且失去编译的类型检查，同时大量的反射代码也会让程序变得复杂。如果将传入的确定的类型转换成一个类型集合，这样就只需要定义一个方法就能实现上述需求
// 假设 T 是类型形参，在定义函数时它的类型是不确定的，类似占位符func Add[T string|float64](a T, b T) T { return a + b} 泛型语法 # 借助上面的例子，我们对于go泛型编程有了最基本的认识，对于泛型go还有很多的新的概念
类型形参、类型实参 # 现在go语言中的函数和类型支持类型参数。类型参数列表看起来像普通的参数列表，只不过它使用方括号（[]）而不是圆括号（()）。</description>
    </item>
    
    <item>
      <title>Grpc</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/grpc/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/grpc/</guid>
      <description>GRPC # https://github.com/shimingyah/pool
基于GRPC的多路复用、超时重连特性，我们很容易实现GRPC连接池。
介绍 # gRPC 是一个高性能、开源和通用的 RPC 框架，面向移动和 HTTP/2 设计。
gRPC基于 HTTP/2标准设计，带来诸如双向流、流控、头部压缩、单 TCP连接上的多复用请求等特。这些特性使得 其在移动设备上表现更好，更省电和节省空间占用。
在 gRPC里客户端应用可以像调用本地对象一样直接调用另一台不同的机器上服务端应用的方法，使得您能够更容 易地创建分布式应用和服务。与许多 RPC系统类似， gRPC也是基于以下理念：
定义一个服务，指定其能够被远程调用的方法（包含参数和返回类型）。
在服务端实现这个接口，并运行一个 gRPC服务器来处理客户端调用。
在客户端拥有一个存根能够像服务端一样的方法。 gRPC客户端和服务端可以在多种环境中运行和交互 -从 google 内部的服务器到你自己的笔记本，并且可以用任何 gRPC支持的语言 来编写。
所以，你可以很容易地用 Java创建一个 gRPC服务端，用 Go、 Python、Ruby来创建客户端。此外， Google最新 API将有 gRPC版本的接口，使你很容易地将 Google的功能集成到你的应用里。
gRPC 内置了以下 encryption 机制：
SSL / TLS：通过证书进行数据加密； ALTS：Google开发的一种双向身份验证和传输加密系统。 只有运行在 Google Cloud Platform 才可用，一般不用考虑。 gRPC 中的连接类型一共有以下3种：
insecure connection：不使用TLS加密 server-side TLS：仅服务端TLS加密 mutual TLS：客户端、服务端都使用TLS加密 gRPC 与 RESTful API比较 # 特性 gRPC RESTful API 规范 必须.</description>
    </item>
    
    <item>
      <title>grpc拦截器retry</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/grpc%E6%8B%A6%E6%88%AA%E5%99%A8retry/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/grpc%E6%8B%A6%E6%88%AA%E5%99%A8retry/</guid>
      <description>为什么要retry # retry很好理解，就是重试，为什么需要重试？首先，grpc常用于移动端和跨机房调用的场景，这两种场景的共同点就是，客户端和服务器之间需要经过一段无法保证质量的网络链路，这时候，为了保证请求到达的成功率，重试就很有必要。另一方面，当某一时刻qps突然很高的时候，服务器可能出现短暂的服务不可用，这时候，设置一个带有随机退避时间的重试，就可以解决问题。
何谓拦截器 # 重试要怎么实现呢？既然已经用了grpc这种成熟的框架，那自然就不用我们自己再去实现。grpc实现重试的方法，是使用Interceptor，翻译过来就是拦截器，这里第一次看可能会很疑惑，重试和拦截器有什么关系？？为了理解拦截器的概念，我们可以画一个简单的图，抽象一下grpc的调用过程。
首先，没有拦截器的时候，一次rpc调用是这样的：
首先，我们会调用protoc生成的接口，将请求发给对端，收到对端的回复后，grpc接口会将结果返回，这样就完成了一次调用。
而有了拦截器，上面的过程就变成了这样：
可以看到，拦截器会在grpc接口收到回复之前先收到回复，也就是拦截了回复，拦截回复之后，拦截器就可以对回复进行一些处理，比如身份验证，消息内容校验，当然，还可以进行重试， 比如回复中的code为5xx时，拦截器先不返回，而是重新发送一次请求，直到code为200了再返回给grpc接口，这样对于应用程序而言，就在毫不知情的情况下，提高了调用的成功率。
retry拦截器的具体实现 # retry拦截器的作用很好理解，但它是怎么工作的呢？这一节我们就来深入源码，看看retry拦截器的具体实现。
grpc本身是支持拦截器的，但是并没有实现各种拦截器，也就是说，grpc允许你进行拦截，但是拦截之后怎么处理，要你自己实现，不过呢，像重试、身份验证这样比较常用的拦截器，已经有非常成熟的库了，所以我们直接拿来用就好了，接下来，我们就先来看看官方的retry拦截器是怎么工作的。
retry的逻辑 # 我们先不管调用是怎么走到拦截器这里的，先只看retry拦截器的逻辑，首先这个拦截器是这个包里实现的：
&amp;#34;github.com/grpc-ecosystem/go-grpc-middleware/retry&amp;#34; **拦截器是个什么？其实就是一个函数，这个函数会被最外层的grpc接口调用，而函数内部又会调用调用grpc的底层接口，实现真正的rpc，然后在返回外层之前，在函数内部对rpc的返回进行处理。**接下来我们就看看重试拦截器的函数内部逻辑。
关于重试的逻辑是下面这段代码：
func(parentCtx context.Context, method string, req, reply interface{}, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error { grpcOpts, retryOpts := filterCallOptions(opts) callOpts := reuseOrNewWithCallOptions(intOpts, retryOpts) // short circuit for simplicity, and avoiding allocations. if callOpts.max == 0 { return invoker(parentCtx, method, req, reply, cc, grpcOpts...) } var lastErr error for attempt := uint(0); attempt &amp;lt; callOpts.</description>
    </item>
    
    <item>
      <title>libewf库编译</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/libewf%E5%BA%93%E7%BC%96%E8%AF%91/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/libewf%E5%BA%93%E7%BC%96%E8%AF%91/</guid>
      <description>地址：libyal/libewf: Libewf is a library to access the Expert Witness Compression Format (EWF) (github.com)
1、下载最新稳定版本
libewf-experimental-&amp;lt;version&amp;gt;.tar.gz 2、解压
3、在麒麟系统上安装软件包
sudo apt install git autoconf automake autopoint libtool pkg-config flex bison 4、进入libewf文件夹编译
./configure --enable-shared=no --enable-static=yes --enable-wide-character-type=yes --enable-shared=no：表示禁用共享库的生成，即只生成静态库。
--enable-static=yes：表示启用静态库的生成。
--enable-wide-character-type=yes：表示启用宽字符类型（wide character type）支持
若不启用 会报 in function _cgo_8405f37c7b66_Cfunc_libewf_handle_open_wide&#39;: undefined reference to libewf_handle_open_wide 的错误
5、make
make -j8 6、生成的文件在../libewf/.libs文件夹下</description>
    </item>
    
    <item>
      <title>Linux问题总结</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/linux%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/linux%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93/</guid>
      <description>你如何查看日志，做分析的？ # grep 命令 一个强大的文本搜索工具
如：
grep &amp;#34;hello&amp;#34; file.txt # 在 file.txt 中查找 &amp;#34;hello&amp;#34; grep &amp;#34;hello&amp;#34; *.txt 常用选项
选项 说明 -i 忽略大小写 -v 反向匹配（显示不包含模式的行） -n 显示匹配行的行号 -c 统计匹配的行数 -r 或 -R 递归搜索目录 -w 匹配整个单词 tail -f 是 Linux/Unix 系统中用于实时查看文件末尾内容的命令，常用于监控日志文件的更新。
tail -f [选项] [文件] tail -f /var/log/syslog # 实时查看 syslog 日志 tail -f /var/log/nginx/access.log # 监控 Nginx 访问日志 常用选项
选项 说明 -f 实时跟踪文件变化（默认显示最后 10 行） -n [NUM] 指定显示的行数（如 -n 20 显示最后 20 行） less 是一个功能强大的文件查看工具</description>
    </item>
    
    <item>
      <title>localhost与127.0.0.1</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/localhost%E4%B8%8E127.0.0.1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/localhost%E4%B8%8E127.0.0.1/</guid>
      <description> 一、基本概念 # 首先，我们需要明确localhost和127.0.0.1各自的定义。
localhost：在计算机网络中，localhost是一个主机名（hostname），指的是当前你正在使用的设备。它是一个常用于访问本机上运行的网络服务的域名。 127.0.0.1：而127.0.0.1则是一个IP地址，属于IPv4协议下的一个特殊地址。它被称为环回地址（loopback address），用于网络软件 测试 以及访问本机服务。 二、技术细节与差异 # 解析过程的不同 # 虽然localhost和127.0.0.1都指向本机，但它们的工作方式存在差异。
当你使用localhost时，系统会通过DNS（域名系统）解析来将其转换为相应的IP地址。一般情况下，这个过程很快，因为大多数操作系统都会在本地的hosts文件中对localhost进行映射，使其指向127.0.0.1或类似的环回地址。相反，使用127.0.0.1时，由于它本身就是一个IP地址，因此无需通过DNS解析，数据包直接在本机内部路由。
性能差异 # 虽然这两者之间的性能差异微乎其微，但在某些高性能要求的环境中，避免即使是最小的延迟也是至关重要的。
使用localhost可能会引入微小的延迟，因为需要经过DNS解析的过程。127.0.0.1则可以省略这一步骤，稍微提升效率。
IPv6环境 # 在IPv6环境下，localhost的解析和使用还具有更多的考量。
localhost在IPv6中通常解析为::1，这是IPv6下的环回地址。直接使用127.0.0.1无法利用IPv6的优势，因此在IPv6优先的网络环境中，推荐使用localhost。
三、应用场景举例 # 开发环境 # 在软件和网站开发过程中，开发 者经常需要在本地机器上运行和测试代码。使用localhost或127.0.0.1可以方便地访问本地开发服务器，无需通过外部网络。
# 通过localhost访问本地开发服务器curl http://localhost:8080# 或者使用IP地址curl http://127.0.0.1:8080 网络软件测试 # 开发网络应用或服务时，测试环回功能非常重要。这可以确保软件在将数据发送到网络之前能正确处理数据。127.0.0.1在这种情况下被广泛使用。
四、最佳实践建议 # 在大多数常规应用场景中，使用localhost和127.0.0.1不会造成明显的差别。但是，从性能和兼容性的角度考虑，理解二者的差异是有益的。 对于侧重于性能的应用，直接使用IP地址（127.0.0.1或::1）可以略微减少DNS解析的开销。 当开发依赖于IPv6环境的应用时，优先使用localhost以确保正确解析环回地址。 </description>
    </item>
    
    <item>
      <title>MCP服务</title>
      <link>https://chain-code.github.io/docs/ai/basic/mcp%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/mcp%E6%9C%8D%E5%8A%A1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Mongodb安装</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/mongodb%E5%AE%89%E8%A3%85/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/mongodb%E5%AE%89%E8%A3%85/</guid>
      <description> Mac版 # 官网下载
https://www.mongodb.com/try/download/community
解压
目录 重新命名为mongodb，并把挪到：/usr/local目录下
添加环境变量
vi ~/.bash_profile export PATH=$PATH:/usr/local/mongoDB/bin source ~/.bash_profile mongod -version#返回下面信息生效db version v7.0.4Build Info: {&amp;#34;version&amp;#34;: &amp;#34;7.0.4&amp;#34;,&amp;#34;gitVersion&amp;#34;: &amp;#34;38f3e37057a43d2e9f41a39142681a76062d582e&amp;#34;,&amp;#34;modules&amp;#34;: [],&amp;#34;allocator&amp;#34;: &amp;#34;system&amp;#34;,&amp;#34;environment&amp;#34;: {&amp;#34;distarch&amp;#34;: &amp;#34;x86_64&amp;#34;,&amp;#34;target_arch&amp;#34;: &amp;#34;x86_64&amp;#34;}} 在目录：/usr/local/mongoDB 创建两个文件夹: data 和 log
chmod 777 /usr/local/mongodbmkdir /usr/local/mongodb/datamkdir /usr/local/mongodb/log 启动MongoDB
mongod --dbpath /usr/local/mongodb/data --logpath /usr/local/mongodb/log/mongod.log --logappend </description>
    </item>
    
    <item>
      <title>MySql相关问题2</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%982/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%982/</guid>
      <description>什么是OnlineDDL # DDL,即Data Defination（定义、注释） Language，是用于定义数据库结构的操作。DDL操作用于创建、修改和删除数据库中的表、索引、视图、约束等数据库对象，而不涉及实际数据的操作。
以下是一些常见的DDL操作：
CREATE
ALTER
DROP
用于永久删除数据库对象
TRUNCATE
用于快速清空表中的所有数据，但保留表结构
与DDL相对的是DML，及Data Manipulation（操作） Language，用于操作数据。即包括我们常用的INSERT、DELETE和UPDATE等。
在MySQL 5.6之前，所有的ALTER操作其实是会阻塞DML操作的，如：添加/删除字段、添加/删除索引等，都是会锁表的。
但是在MySQL 5.6中引入了Online DDL，OnLineDDL是MySQL5.6提出的加速DDL方案，尽最大可能保证DDL期间不阻塞DML动作。但是需要注意，这里说的尽最大可能意味着不是所有DDL语句都会使用OnlineDDL加锁。
Online DDL的优点就是可以减少阻塞，是MySQL的一种内置优化手段，但是需要注意的是，DDL在刚开始和快结束的时候，都需要获取MDL锁，而在获取锁的时候如果有事务未提交，那么DDL就会因为加锁失败而进入阻塞状态，也会造成性能影响。
还有就是，如果Online DDL操作失败，其回滚操作可能成本较高。以及长时间运行的Online DDL操作可能导致主从同步滞后。
但是需要注意的是，即使有了Online DDL，也不意味着就可以随意在业务高峰期进行DDL变更了
在SQL后增加ALGORITHM=INPLACE, LOCK=NONE;或者 ALGORITHM=INSTANT、ALGORITHM=COPY 要根据不同的业务场景，选择不同的算法
ALGORITHM算法选择 算法 原理 适用场景 缺点 COPY 创建临时表复制数据，原表被替换 所有ALTER操作（最通用） 锁表时间长，空间占用翻倍 INPLACE 直接在原表上修改（不复制数据） 添加索引、修改列类型（有限支持） 可能仍有短暂锁 INSTANT 仅修改元数据（最快） MySQL 8.0+ 的列添加/删除等操作 支持的操作有限 LOCK锁定策略 锁定级别 行为 适用场景 NONE 允许并发读写 高并发业务时段 SHARED 允许读但阻塞写 需要数据一致性但可接受读 EXCLUSIVE 完全锁表（默认） 维护时段或低峰期 具体算法内容待补充
为什么不推荐使用外键 # MySQL 外键最大的作用就是有助于维护数据的一致性和完整性。
但是，其实在很多大型互联网公司中，很少用外键的，甚至阿里巴巴Java开发手册中明确规定了：</description>
    </item>
    
    <item>
      <title>Ocr识别</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/ocr%E8%AF%86%E5%88%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/ocr%E8%AF%86%E5%88%AB/</guid>
      <description>使用项目：
https://github.com/shibing624/imgocr
https://github.com/WenmuZhou/PytorchOCR</description>
    </item>
    
    <item>
      <title>Opencv</title>
      <link>https://chain-code.github.io/docs/python/package/opencv/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/opencv/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Opencv Cuda编译</title>
      <link>https://chain-code.github.io/docs/ai/basic/opencv_cuda%E7%BC%96%E8%AF%91/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/opencv_cuda%E7%BC%96%E8%AF%91/</guid>
      <description>相关参考资料 # 安装python
原生python官网下载地址，选择Windows版本。
cmake安装
CMake官方下载地址
其他版本下载地址：https://cmake.org/files/
opencv下载
Opencv官方下载地址，下载OpenCV – 4.8.0 Sources，下载解压opencv-4.8.0.zip
opencv_contrib
opencv_contrib官方下载地址，选择opencv对应的contrib版本，例如opencv4.8.0对应就是opencv_contrib-4.8.0.zip。下载后直接解压。
NUIDIA-cuDNN
NVIDIA cuDNN
visual studio 2019 16.11.43
https://learn.microsoft.com/en-us/visualstudio/releases/2019/history
编译相关文章 # https://blog.csdn.net/iracer/article/details/125360183
https://www.cnblogs.com/guojin-blogs/p/17939955
https://zhuanlan.zhihu.com/p/354838274
https://blog.csdn.net/fixed_zhang/article/details/110930716
https://blog.csdn.net/yangyu0515/article/details/129643486
https://blog.csdn.net/yangyu0515/article/details/133794355
https://www.rwr.ink/index.php/2023/11/07/opencv-with-cuda%E7%BC%96%E8%AF%91%E5%AE%9E%E6%88%98/
OpenCV CUDA 安装 # https://github.com/chrismeunier/OpenCV-CUDA-installation/blob/main/README.md#check-install-and-troubleshooting
Conda+PyTorch+OpenCV-contrib-cuda环境下，import cv2 出现dll找不到的问题 # https://blog.csdn.net/zMGAM/article/details/138158027
解决opencv编译中出现的#error: This file was generated by an older version of protoc which is (编C1189译源文件）问题 # https://blog.csdn.net/m0_58326153/article/details/142381832
https://github.com/opencv/opencv/issues/17389
https://blog.csdn.net/weixin_38934440/article/details/107093908
protoc下载地址
https://github.com/protocolbuffers/protobuf/tags?after=v3.29.3
实操准备 # opencv 4.8.0
opencv_contrib
cmake 3.27.7
visual Studio 2019</description>
    </item>
    
    <item>
      <title>panic</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/panic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/panic/</guid>
      <description>panic # 1、panic 中可以传任何值，不仅仅可以传 string
func main(){ defer func(){ if r := recover();r != nil{ fmt.Println(r) } }() panic([]int{12312}) } [12312] 在Go语言中，宕机(panic)是程序遇到无法继续执行的严重错误时触发的机制。正确处理panic可以防止程序意外崩溃，提高系统稳定性。
panic与recover基础 # panic机制 # panic是Go语言中处理不可恢复错误的机制，类似于其他语言的异常。当函数执行panic时：
当前函数停止执行 开始执行延迟函数(defer) 逐层向上返回，直到被recover捕获或程序崩溃 funcriskyFunction(){panic(&amp;#34;something went wrong!&amp;#34;)} recover机制 # recover是用于捕获panic的内置函数，必须在defer函数中调用才有效：
funcsafeFunction(){deferfunc(){if r :=recover(); r !=nil{fmt.Println(&amp;#34;Recovered from panic:&amp;#34;, r)}}()riskyFunction()} 宕机恢复最佳实践 # 基本恢复模式 # func ProtectedRun() {defer func() {if err := recover(); err != nil {log.</description>
    </item>
    
    <item>
      <title>pprof</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/pprof/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/pprof/</guid>
      <description>[pprof用法简介](Go 语言性能调试与分析工具：pprof 用法简介 | wxsm&amp;rsquo;s pace)
一、什么是pprof # 1、简介 # pprof是GoLang程序性能分析工具，可以用于可视化和分析性能数据的工具，prof是profile（画像）的缩写，本文将对下面包进行运用：
net/http/pprof：对 runtime/pprof 的二次封装，一般用于web server，它一直运行。这个包对提供的http服务进行数据采集分析。 上面的 pprof 开启后，每隔一段时间就会采集当前程序的堆栈信息，获取函数的 cpu、内存等使用情况。通过对采样的数据进行分析，形成一个数据分析报告。
pprof 以profile.proto的格式保存数据，然后根据这个数据可以生成可视化的分析报告，支持文本形式和图形形式报告。profile.proto里具体的数据格式是 protocol buffers。
2、支持的功能 # profile：CPU 占用率 heap：当前时刻的内存使用情况 allocs：所有时刻的内存使用情况，包括正在使用的及已经回收的 goroutine：目前的goroutine数量及运行情况 mutex：锁争用情况 block：协程阻塞情况 二、net/http/pprof使用介绍 # 1、样例 # 准备炸弹代码: git clone https://github.com/wolfogre/go-pprof-practice.git ,运行代码开始分析问题
2、分析代码 # 处理cpu问题 # # 采集10秒CPU数据排查问题: go tool pprof &amp;#34;http://localhost:6060/debug/pprof/profile?seconds=10&amp;#34; 输入top命令，查看CPU占用较高的调用，如下图:
flat：函数上运行耗时
flat%：CPU运行耗时总比例
sum%：累积使用CPU总比例
cum：函数加上它之上的调用运行总耗时
cum%：CPU运行耗时总比例
最后一列为函数名称，可以通过这五列得出一个应用程序的运行情况
可以看到主要是tiger.Eat占用较高,使用 list Eat可以查看详情，如图
注释问题代码解决问题
处理内存占用过高 # # 采集内存数据排查问题 go tool pprof http://localhost:6060/debug/pprof/heap 输入top命令，查看内存占用较高的调用，如下图:</description>
    </item>
    
    <item>
      <title>ProtoBuf</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/protobuf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/protobuf/</guid>
      <description>ProtoBuf # protobuf是google旗下的一款平台无关，语言无关，可扩展的序列化结构数据格式。所以很适合用做数据存储和作 为不同应用，不同语言之间相互通信的数据交换格式，只要实现相同的协议格式即同一 proto文件被编译成不同的 语言版本，加入到各自的工程中去。这样不同语言就可以解析其他语言通过 protobuf序列化的数据。
Google Protocol Buffer(简称 Protobuf)是一种轻便高效的结构化数据存储格式，平台无关、语言无关、可扩展， 可用于通讯协议和数据存储等领域。
数据交互的格式比较 # 数据交互xml、json、protobuf格式比较
1、json: 一般的web项目中，最流行的主要还是json。因为浏览器对于json数据支持非常好，有很多内建的函数支 持。
2、xml: 在webservice中应用最为广泛，但是相比于json，它的数据更加冗余，因为需要成对的闭合标签。json使 用了键值对的方式，不仅压缩了一定的数据空间，同时也具有可读性。
3、protobuf:是后起之秀，是谷歌开源的一种数据格式，适合高性能，对响应速度有要求的数据传输场景。因为 profobuf是二进制数据格式，需要编码和解码。数据本身不具有可读性。因此只能反序列化之后得到真正可读的数 据。
相对于其它protobuf更具有优势
1：序列化后体积相比Json和XML很小，适合网络传输
2：支持跨平台多语言
3：消息格式升级和兼容性还不错
4：序列化反序列化速度很快，快于Json的处理速速
protoBuf的优点 # Protobuf 有如 XML，不过它更小、更快、也更简单。你可以定义自己的数据结构，然后使用代码生成器生成的代 码来读写这个数据结构。你甚至可以在无需重新部署程序的情况下更新数据结构。只需使用 Protobuf 对数据结构 进行一次描述，即可利用各种不同语言或从各种不同数据流中对你的结构化数据轻松读写。
它有一个非常棒的特性，即“向后”兼容性好，人们不必破坏已部署的、依靠“老”数据格式的程序就可以对数据结构 进行升级。
Protobuf 语义更清晰，无需类似 XML 解析器的东西（因为 Protobuf 编译器会将 .proto 文件编译生成对应的数据 访问类以对 Protobuf 数据进行序列化、反序列化操作）。使用 Protobuf 无需学习复杂的文档对象模型， Protobuf 的编程模式比较友好，简单易学，同时它拥有良好的文档和示例，对于喜欢简单事物的人们而言， Protobuf 比其他的技术更加有吸引力。
ProtoBuf 的不足 # Protobuf 与 XML 相比也有不足之处。它功能简单，无法用来表示复杂的概念。
XML 已经成为多种行业标准的编写工具，Protobuf 只是 Google 公司内部使用的工具，在通用性上还差很多。 由 于文本并不适合用来描述数据结构，所以 Protobuf 也不适合用来对基于文本的标记文档（如 HTML）建模。另 外，由于 XML 具有某种程度上的自解释性，它可以被人直接读取编辑，在这一点上 Protobuf 不行，它以二进制的 方式存储，除非你有 .</description>
    </item>
    
    <item>
      <title>python内存泄漏排查方法</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E6%8E%92%E6%9F%A5%E6%96%B9%E6%B3%95/</guid>
      <description>https://docs.python.org/zh-cn/3.13/library/tracemalloc.html
https://blog.csdn.net/kuailebeihun_/article/details/140575926
https://zhuanlan.zhihu.com/p/436577356</description>
    </item>
    
    <item>
      <title>python基础</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%9F%BA%E7%A1%80/</guid>
      <description>安装Python # 安装Python 3 # 目前，Python有两个版本，一个是2.x版，一个是3.x版，这两个版本是不兼容的。由于3.x版越来越普及，我们的教程将以最新的Python 3.x版本为基础。请确保你的电脑上安装的Python版本是最新的3.x，这样，你才能无痛学习这个教程。
在Windows上安装Python # 首先，根据你的Windows版本（64位还是32位）从Python的官方网站下载Python 3对应的安装程序，然后，运行下载的exe安装包：
特别要注意勾上Add Python 3.x to PATH，然后点“Install Now”即可完成安装。
在Mac上安装Python # 如果你正在使用Mac，那么系统自带的Python版本是2.7。要安装最新的Python 3，有两个方法：
方法一：从Python官网下载Python 3 macOS版的安装程序，下载后双击运行并安装；
方法二：如果安装了Homebrew，直接通过命令brew install python3安装即可。
在Linux上安装Python # 如果你正在使用Linux，那我可以假定你有Linux系统管理经验，自行安装Python 3应该没有问题，否则，请换回Windows系统。
对于大量的目前仍在使用Windows的同学，如果短期内没有打算换Mac，就可以继续阅读以下内容。
运行Python # 安装成功后，打开命令提示符窗口，敲入python后，会出现两种情况：
情况一：
┌────────────────────────────────────────────────────────┐│Command Prompt - □ x │├────────────────────────────────────────────────────────┤│Microsoft Windows [Version 10.0.0] ││(c) 2015 Microsoft Corporation. All rights reserved. ││ ││C:\&amp;gt; python ││Python 3.12 ... ││[MSC v... 64 bit (AMD64)] on win32 ││Type &amp;#34;help&amp;#34;, &amp;#34;copyright&amp;#34;, &amp;#34;credits&amp;#34; or &amp;#34;license&amp;#34;.</description>
    </item>
    
    <item>
      <title>python安装</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%AE%89%E8%A3%85/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/python%E5%AE%89%E8%A3%85/</guid>
      <description>Python安装 # 安装地址：python
查看python版本 # python --verison 查看python路径 # where python 列出所有已安装的 Python 版本 # py -0 指定运行某个版本 # py -3.12 指定版本环境 # 右键点击“此电脑” &amp;gt; 选择“属性” &amp;gt; 高级系统设置 &amp;gt; 环境变量
调整版本上下顺序
C:\Users\&amp;lt;用户名&amp;gt;\AppData\Local\Programs\Python\Python38\C:\Users\&amp;lt;用户名&amp;gt;\AppData\Local\Programs\Python\Python312\ virtualenv介绍 # Virtualenv 是一个工具，它能够帮我们创建一个独立（隔离）的 Python 环境。想象你有一个应用程序，依赖于版本为2的第三方模块，但另一个程序依赖的版本是3，请问你如何使用和开发这些应用程序？
如果你把一切都安装到了 /usr/lib/python2.7/site-packages（或者其它平台的标准位置），那很容易出现某个模块被升级而你却不知道的情况。
在另一种情况下，想象你有一个已经开发完成的程序，但是你不想更新它所依赖的第三方模块版本；但你已经开始另一个程序，需要这些第三方模块的版本。
使用 virtualenv！针对每个程序创建独立（隔离）的 Python 环境，而不是在全局安装所依赖的模块。
要安装它，只需要在命令行中输入以下命令：
$ pip install virtualenv 最重要的命令是：
$ virtualenv myproject$ source myproject/bin/activate 执行第一个命令在 myproject 文件夹创建一个隔离的 virtualenv 环境，第二个命令激活这个隔离的环境（virtualenv）。
在创建 virtualenv 时，你必须做出决定：这个 virtualenv 是使用系统全局的模块呢，还是只使用这个virtualenv内的模块？ 默认情况下，virtualenv 不会使用系统全局模块。</description>
    </item>
    
    <item>
      <title>pytorch食谱</title>
      <link>https://chain-code.github.io/docs/ai/basic/pytorch%E9%A3%9F%E8%B0%B1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/pytorch%E9%A3%9F%E8%B0%B1/</guid>
      <description>PyTorch食谱</description>
    </item>
    
    <item>
      <title>Qwen2.5-vl源码部署</title>
      <link>https://chain-code.github.io/docs/ai/generative-ai/qwen2.5-vl%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/generative-ai/qwen2.5-vl%E6%BA%90%E7%A0%81%E9%83%A8%E7%BD%B2/</guid>
      <description>Qwen2.5-vl源码部署 # 官方
本地运行 Qwen2-VL
使用vLLM部署Qwen2.5-VL-7B-Instruct模型的详细指南
阿里最新开源模型Qwen2.5-VL本地部署教程：视觉理解超越GPT-4o！
cudn加速库 # cudnn-windows-x86_64-8.9.7.29_cuda12-archive 仓库地址 # https://github.com/QwenLM/Qwen2.5-VL?tab=readme-ov-file
模型地址 # https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ
环境对比 # (base) PS C:\Users\admin\python\qwen\Qwen2.5-VL&amp;gt; pip listPackage Version--------------------------------- ------------------aiobotocore 2.12.3aiohappyeyeballs 2.4.0aiohttp 3.10.5aioitertools 0.7.1aiosignal 1.2.0alabaster 0.7.16altair 5.0.1anaconda-anon-usage 0.4.4anaconda-catalogs 0.2.0anaconda-client 1.12.3anaconda-cloud-auth 0.5.1anaconda-navigator 2.6.3anaconda-project 0.11.1annotated-types 0.6.0anyio 4.2.0appdirs 1.4.4archspec 0.2.3argon2-cffi 21.3.0argon2-cffi-bindings 21.2.0arrow 1.2.3astroid 2.14.2astropy 6.</description>
    </item>
    
    <item>
      <title>Rabbit Mq面试题</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/rabbitmq%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/rabbitmq%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Redis面试题总结2</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/redis%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%932/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/redis%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%932/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reid数据集</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/reid%E6%95%B0%E6%8D%AE%E9%9B%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/reid%E6%95%B0%E6%8D%AE%E9%9B%86/</guid>
      <description>单模态 # Market-1501：Person Re-Identification Meets Image Search：
链接：https://pan.baidu.com/s/1ntIi2Op
2015年，论文 Person Re-Identification Meets Image Search 提出了 Market 1501 数据集，现在 Market 1501 数据集已经成为行人重识别领域最常用的数据集之一。
Market 1501 的行人图片采集自清华大学校园的 6 个摄像头，一共标注了 1501 个行人。其中，751 个行人标注用于训练集，750 个行人标注用于测试集，训练集和测试集中没有重复的行人 ID，也就是说出现在训练集中的 751 个行人均未出现在测试集中。
训练集：751 个行人，12936 张图片 测试集：750 个行人，19732 张图片 query 集：750 个行人，3368 张图片 query 集的行人图片都是手动标注的图片，从 6 个摄像头中为测试集中的每个行人选取一张图片，构成 query 集。测试集中的每个行人至多有 6 张图片，query 集共有 3368 张图片。
网络模型训练时，会用到训练集；测试模型好坏时，会用到测试集和 query 集。此时测试集也被称作 gallery 集。因此实际用到的子集为，训练集、gallery 集 和 query 集。
MARS: A Video Benchmark for Large-Scale Person Re-identification（基于视频）</description>
    </item>
    
    <item>
      <title>Reid行人重识别</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/reid%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/reid%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB/</guid>
      <description>原理 # 在监控拍不到人脸的情况下，ReID可以代替人脸识别来在视频序列中找到我要找到目标对象。那么他的应用就很广了，可以做安防，可以做个人定位，在商场上可以配合推荐系统，搭建出个性化的推荐服务等等
我们利用训练后的网络计算特征从所有搜索到的图像中提取特征，并计算搜索图与地库之间的特征距离。然后根据计算出的距离对它们进行排序。排名越高，相似性越高，上图中，绿色边框的是正确检索的结果，红色边框的是错误检索的结果。
代码仓库：https://github.com/JDAI-CV/fast-reid
提供了针对ReID任务的完整的工具箱，包括训练、评估、微调和模型部署，另外实现了在多个任务中的最先进的模型。
实操 # 模型初始化 # 创建ONNX推理会话，使用CUDA执行提供者
class ReidModel: def __init__(self, model_name, model_dir): &amp;#34;&amp;#34;&amp;#34; 初始化 ReidModel 实例。 Args: model_name (str): Reid 模型的特定名称 model_dir (str): 包含 Reid ONNX 模型文件的目录路径。 &amp;#34;&amp;#34;&amp;#34; self._key = b&amp;#39;fm20Za6uii..........AvPdMhxXs=&amp;#39; img_onnx_model_path = os.path.join(model_dir, &amp;#34;weights.onnx&amp;#34;) decrypted_model = decrypt_model(img_onnx_model_path, self._key) //模型解密 img_sess_options = onnxruntime.SessionOptions() img_run_options = onnxruntime.RunOptions() img_run_options.log_severity_level = 2 self.input_height = 384 self.input_width = 128 self.session = onnxruntime.InferenceSession(decrypted_model, sess_options=img_sess_options, providers=[&amp;#34;CUDAExecutionProvider&amp;#34;])# NVIDIA GPU (CUDA) self.input_name = self.</description>
    </item>
    
    <item>
      <title>Restful API</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/restfulapi/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/restfulapi/</guid>
      <description>一、协议 # API与用户的通信协议，总是使用HTTPs协议。
二、域名 # 应该尽量将API部署在专用域名之下。
https://api.example.com 如果确定API很简单，不会有进一步扩展，可以考虑放在主域名下。
https://example.org/api/ 三、版本（Versioning） # 应该将API的版本号放入URL。
https://api.example.com/v1/ 另一种做法是，将版本号放在HTTP头信息中，但不如放入URL方便和直观。Github采用这种做法。
四、路径（Endpoint） # 路径又称&amp;quot;终点&amp;quot;（endpoint），表示API的具体网址。
在RESTful架构中，每个网址代表一种资源（resource），所以网址中不能有动词，只能有名词，而且所用的名词往往与数据库的表格名对应。一般来说，数据库中的表都是同种记录的&amp;quot;集合&amp;quot;（collection），所以API中的名词也应该使用复数。
举例来说，有一个API提供动物园（zoo）的信息，还包括各种动物和雇员的信息，则它的路径应该设计成下面这样。
https://api.example.com/v1/zoos https://api.example.com/v1/animals https://api.example.com/v1/employees 五、HTTP动词 # 对于资源的具体操作类型，由HTTP动词表示。
常用的HTTP动词有下面五个（括号里是对应的SQL命令）。
GET（SELECT）：从服务器取出资源（一项或多项）。 POST（CREATE）：在服务器新建一个资源。 PUT（UPDATE）：在服务器更新资源（客户端提供改变后的完整资源）。 PATCH（UPDATE）：在服务器更新资源（客户端提供改变的属性）。 DELETE（DELETE）：从服务器删除资源。 还有两个不常用的HTTP动词。
HEAD：获取资源的元数据。 OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。 下面是一些例子。
GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息） PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物 六、过滤信息（Filtering） # 如果记录数量很多，服务器不可能都将它们返回给用户。API应该提供参数，过滤返回结果。
下面是一些常见的参数。
?limit=10：指定返回记录的数量 ?offset=10：指定返回记录的开始位置。 ?page=2&amp;amp;per_page=100：指定第几页，以及每页的记录数。 ?sortby=name&amp;amp;order=asc：指定返回结果按照哪个属性排序，以及排序顺序。 ?animal_type_id=1：指定筛选条件 参数的设计允许存在冗余，即允许API路径和URL参数偶尔有重复。比如，GET /zoo/ID/animals 与 GET /animals?zoo_id=ID 的含义是相同的。
七、状态码（Status Codes） # 服务器向用户返回的状态码和提示信息，常见的有以下一些（方括号中是该状态码对应的HTTP动词）。</description>
    </item>
    
    <item>
      <title>resty</title>
      <link>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/resty/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/resty/</guid>
      <description>介绍： # RESTful API 已成为现代 Web 开发的基石，可实现客户端与服务器之间的无缝通信。在本文中，我们将探索使用 Resty（一种流行的 HTTP 客户端库）在 Go 中执行常见操作（如 GET、POST、UPDATE 和 DELETE 请求）的强大功能和简便性。我们还将学习如何在请求中传递标头，从而使我们能够自定义和增强 API 交互。
网址：https://github.com/go-resty/resty
安装 Resty： # 首先，我们需要在 Go 环境中安装 Resty。我们可以使用以下命令来安装 Resty 包：
go get -u github.com/go-resty/resty/v2 GET # 发出 GET 请求： # 让我们首先研究如何使用 Resty v2 执行 GET 请求。以下代码片段演示了一个简单的 GET 请求并将响应绑定到结构体中：
package mainimport (&amp;#34;fmt&amp;#34;&amp;#34;log&amp;#34;&amp;#34;github.com/go-resty/resty/v2&amp;#34;)type DevUser struct {ID int `json:&amp;#34;id&amp;#34;`Name string `json:&amp;#34;name&amp;#34;`Email string `json:&amp;#34;email&amp;#34;`}func main() {var users []DevUserresponse, err := resty.</description>
    </item>
    
    <item>
      <title>schtask使用</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/schtasks%E4%BD%BF%E7%94%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/schtasks%E4%BD%BF%E7%94%A8/</guid>
      <description>简介 # SCHTASKS 允许管理员创建、删除、查询、更改、运行和中止本地或远程系统上的计划任务。
语法 # SCHTASKS /parameter [arguments]描述:允许管理员创建、删除、查询、更改、运行和中止本地或远程系统上的计划任务。参数列表:/Create 创建新计划任务。/Delete 删除计划任务。/Query 显示所有计划任务。/Change 更改计划任务属性。/Run 按需运行计划任务。/End 中止当前正在运行的计划任务。/ShowSid 显示与计划的任务名称相应的安全标识符。/? 显示此帮助消息。Examples:SCHTASKSSCHTASKS /?SCHTASKS /Run /?SCHTASKS /End /?SCHTASKS /Create /?SCHTASKS /Delete /?SCHTASKS /Query /?SCHTASKS /Change /?SCHTASKS /ShowSid /? 格式 # /SC schedule 指定计划频率：MINUTE、 HOURLY、DAILY、WEEKLY、MONTHLY, ONCE, ONSTART, ONLOGON, ONIDLE, ONEVENT./MO MINUTE: 1 到 1439 分钟。 HOURLY: 1 - 23 小时。 DAILY: 1 到 365 天。 WEEKLY: 1 到 52 周。 MONTHLY: 1 到 12，或 FIRST, SECOND, THIRD, FOURTH, LAST, LASTDAY。/ST starttime 指定运行任务的开始时间：时间格式为 HH:mm (24 小时时间)，例如 14:30 表示 2:30 PM。如果未指定 /ST，则默认值为当前时间。/ET endtime 指定运行任务的结束时间：时间格式为 HH:mm (24 小时时间)，例如 14:50 表示 2:50 PM。/TN taskname 指定唯一识别这个计划任务的名称。/TR taskrun 指定在这个计划时间运行的程序的路径和文件名。例如: C:\windows\system32\calc.</description>
    </item>
    
    <item>
      <title>Swagger</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/swagger/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/swagger/</guid>
      <description>Swag将Go的注释转换为Swagger2.0文档。我们为流行的 Go Web Framework 创建了各种插件，这样可以与现有Go项目快速集成（使用Swagger UI）。
官方文档
快速开始 # 将注释添加到API源代码中，请参阅声明性注释格式。 使用如下命令下载swag： go install github.com/swaggo/swag/cmd/swag@latest 从源码开始构建的话，需要有Go环境（1.19及以上版本）。
或者从github的release页面下载预编译好的二进制文件。
在包含main.go文件的项目根目录运行swag init。这将会解析注释并生成需要的文件（docs文件夹和docs/docs.go）。 swag init 确保导入了生成的docs/docs.go文件，这样特定的配置文件才会被初始化。如果通用API注释没有写在main.go中，可以使用-g标识符来告知swag。
swag init -g http/api.go (可选) 使用fmt格式化 SWAG 注释。(请先升级到最新版本) swag fmt swag cli # swag init -h NAME: swag init - Create docs.go USAGE: swag init [command options] [arguments...] OPTIONS: --generalInfo value, -g value API通用信息所在的go源文件路径，如果是相对路径则基于API解析目录 (默认: &amp;#34;main.go&amp;#34;) --dir value, -d value API解析目录 (默认: &amp;#34;./&amp;#34;) --exclude value 解析扫描时排除的目录，多个目录可用逗号分隔（默认：空） --propertyStrategy value, -p value 结构体字段命名规则，三种：snakecase,camelcase,pascalcase (默认: &amp;#34;camelcase&amp;#34;) --output value, -o value 文件(swagger.</description>
    </item>
    
    <item>
      <title>tracker目标追踪</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/tracker%E7%9B%AE%E6%A0%87%E8%BF%BD%E8%B8%AA/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/tracker%E7%9B%AE%E6%A0%87%E8%BF%BD%E8%B8%AA/</guid>
      <description> Ultralytics包下两个配置文件的区别 # bytetrack.yaml # 作用：配置 ByteTrack 跟踪器，纯几何/分数驱动，强调简洁与速度。 关键字段 tracker_type : 设为 bytetrack 。 track_high_thresh/track_low_thresh/new_track_thresh : 与上同，决定高/低分样本如何参与匹配。 track_buffer : 控制丢失后保活时长。 match_thresh : 候选几何匹配阈值。 fuse_score : 是否融合检测分数进行排序/匹配。 优点 极简高效，实时性强；对高召回检测器表现优异。 部署依赖少，维护成本低。 参数调优相对直接。 缺点 在遮挡、相似目标密集、移动相机场景更易发生 ID 切换。 不包含 GMC 与 ReID，复杂场景下稳定性不如 BoT-SORT。 对检测质量和稳定帧率更依赖。 botsort.yaml # 作用：配置 BoT-SORT 跟踪器，强调几何匹配+相机运动补偿（GMC），可选外观特征（ReID）。 关键字段 tracker_type : 设为 botsort ，启用 BoT-SORT 管线。 track_high_thresh/track_low_thresh/new_track_thresh : 控制进入/维持/新建轨迹的检测置信度阈值。 track_buffer : 轨迹丢失后保活的帧数，影响断联重连能力。 match_thresh : 匹配候选的几何相似度阈值（如 IoU/距离），越高越严格。 fuse_score : 是否在匹配时融合检测分数，提升鲁棒性。 gmc_method : 相机运动补偿方法（如 sparseOptFlow ），减少移动车机/抖动造成的错配。 with_reid : 是否启用外观特征辅助（若集成了 ReID 模型）。 优点 在移动相机、拥挤/遮挡场景显著提升 ID 一致性。 GMC 对剧烈运动或背景变化更稳，错配更少。 可与外观特征融合，跨遮挡/长时空隔更可靠。 缺点 速度较慢、算力占用更高（GMC、可选 ReID）。 参数更敏感，需按场景细调。 如启用 ReID，还需额外模型与预处理集成。 </description>
    </item>
    
    <item>
      <title>Trae使用心得分享</title>
      <link>https://chain-code.github.io/docs/ai/basic/trae%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/trae%E4%BD%BF%E7%94%A8%E5%BF%83%E5%BE%97%E5%88%86%E4%BA%AB/</guid>
      <description>Trae介绍 # Trae 是字节跳动推出的一款融合了 AI 辅助编程、智能代码建议、生成代码文件 以及 灵活适配不同场景 的 IDE。它不仅能够帮助开发者更快地编写代码，还可以根据具体提示语生成代码并进行维护，从而优化开发流程，实现高效协作。
选择 Trae ，主要因为它是国产软件，有中文界面和文档，并且完全免费，完全免费，完全免费。缺点是bug较多，反应速度较慢，好的一点是字节几乎一天一更新，相信将来会变得更好。
Trae下载官网
主要功能 # 上下文 # 当你需要参考某个特定函数、接口的代码，或者想要了解某个文件、文件夹的整体内容，又或者想对整个工作空间有一个全局的认识时，就可以使用该技巧向 AI 助手获取相关信息。
可以选择Code、File、Folder、Workspace、Doc、Web不同功能，能够更好地满足复杂开发需求。
在选择代码上下文时，我常用的使用快捷键Ctrl+U添加,也可以将终端中的内容作为上下文
模型 # Trae 预置了一系列业内表现比较出色的模型，你可以直接切换不同的模型进行使用。此外，Trae 还支持通过 API 密钥（API Key）接入自定义模型，从而满足个性化的需求。
个人使用过程中，感觉Gemini-2.5-Pro-Preview模型、和Claude-3.7-Sonnet模型对于代码的理解相对于其他模型较好。
AI修复 # Trae会自动探测代码中存在的明显问题，并指明具体位置
根据指示，当鼠标位于上方会出现AI修复提示
点击修复按钮，Trae会给出修改建议如下：
代码补全 # 在光标所在位置，敲击回车键换行，AI 助手会阅读并理解当前代码，然后自动补全后续代码。
按下 Tab 键，接受所有自动补全的代码。
多模态图片 # 当你遇到一些用文字难以描述清楚的问题时，就可以使用该技巧通过添加图片的方式更准确高效地表达需求。
规则 # 可以通过制定规则来规范 AI 在 Trae IDE 内的行为。
比如：我告诉AI我的操作系统为Windows，跟我保持中文对话。
智能体+MCP # 智能体 # 智能体是你面向不同开发场景的编程助手。除内置的智能体 Builder 外，你还可以创建自定义智能体，通过灵活配置提示词和工具集，使其更高效地帮你完成复杂任务。
Trae 提供以下内置智能体：
Builder：Builder 可以帮助你从 0 到 1 开发一个完整的项目。根据你的需求，Builder 会调用不同的工具，包括分析代码文件的工具、编辑代码文件的工具、运行命令的工具等等，从而更加精确且有效地处理你的需求。 Builder with MCP：在 Builder 的基础上，你配置的所有 MCP Server 都会默认添加至 Builder with MCP，且不可编辑。 MCP # AI 模型通过连接外部应用，来扩展功能。每个外部应用的接口，都不一样，如果要接入10个应用，就要写10种接入代码，非常麻烦。而且，要是换一个模型，可能所有接入代码都要重写。</description>
    </item>
    
    <item>
      <title>URL</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/url/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/url/</guid>
      <description> Url # </description>
    </item>
    
    <item>
      <title>uvicorn</title>
      <link>https://chain-code.github.io/docs/python/package/uvicorn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/uvicorn/</guid>
      <description>uvicorn # Uvicorn 是由 Starlette 框架的作者编写的 ASGI 服务器，旨在提供高性能的异步请求处理能力。它使用 asyncio 库实现异步 I/O 操作，支持 HTTP 和 WebSocket 协议，可与各种 ASGI 应用程序框架（如 FastAPI、Django、Starlette 等）配合使用。
示例 # # main.pyfrom fastapi import FastAPIapp = FastAPI()@app.get(&amp;#34;/&amp;#34;)async def read_root():return {&amp;#34;message&amp;#34;: &amp;#34;Hello, World!&amp;#34;} from fastapi import FastAPIimport uvicornapp = FastAPI()@app.get(&amp;#34;/&amp;#34;)async def read_root():return {&amp;#34;message&amp;#34;: &amp;#34;Hello, World!&amp;#34;}if __name__ == &amp;#34;__main__&amp;#34;:host = &amp;#34;0.0.0.0&amp;#34;port = 8000uvicorn.run(app, host=host, port=port, log_level=40) 配置选项 # Uvicorn 提供了丰富的配置选项，以满足不同需求。可以通过命令行参数或配置文件来配置 Uvicorn 的行为。</description>
    </item>
    
    <item>
      <title>venv虚拟环境</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/venv%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/venv%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83/</guid>
      <description>venv &amp;mdash; 虚拟环境的创建 # venv 模块支持创建轻量的“虚拟环境”，每个虚拟环境将拥有它们自己独立的安装在其 site 目录中的 Python 软件包集合。 虚拟环境是在现有的 Python 安装版基础之上创建的，这被称为虚拟环境的“基础”Python，并且还可选择与基础环境中的软件包隔离开来，这样只有在虚拟环境中显式安装的软件包才是可用的。
当在虚拟环境中使用时，常见安装工具如 pip 将把 Python 软件包安装到虚拟环境而无需显式地指明这一点。
虚拟环境是（主要的特性）：
用来包含支持一个项目（库或应用程序）所需的特定 Python 解释器、软件库和二进制文件。 它们在默认情况下与其他虚拟环境中的软件以及操作系统中安装的 Python 解释器和库保持隔离。 包含在一个目录中，根据惯例被命名为项目目录下的venv 或 .venv，或是有许多虚拟环境的容器目录下，如 ~/.virtualenvs。 不可签入 Git 等源代码控制系统。 被视为是可丢弃性的 —— 应当能够简单地删除并从头开始重建。 你不应在虚拟环境中放置任何项目代码。 不被视为是可移动或可复制的 —— 你只能在目标位置重建相同的环境。 创建虚拟环境 # 通过执行 venv 指令来创建一个 虚拟环境:
python -m venv /path/to/new/virtual/environment 运行此命令将创建目标目录（父目录若不存在也将创建），并放置一个 pyvenv.cfg 文件在其中，文件中有一个 home 键，它的值指向运行此命令的 Python 安装（目标目录的常用名称是 .venv）。它还会创建一个 bin 子目录（在 Windows 上是 Scripts），其中包含 Python 二进制文件的副本或符号链接（视创建环境时使用的平台或参数而定）。它还会创建一个（初始为空的） lib/pythonX.Y/site-packages 子目录（在 Windows 上是 Lib\site-packages）。如果指定了一个现有的目录，这个目录就将被重新使用。</description>
    </item>
    
    <item>
      <title>Vim编程常用快捷键</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/vim%E7%BC%96%E7%A8%8B%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/vim%E7%BC%96%E7%A8%8B%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE/</guid>
      <description> 常用快捷键 # 插入 # 在光标后插入：a在光标前插入：i在光标下方新开一行插入：o (小写) 光标上下左右移动 # 左移：h 退格 //退格可以左移动到上一行右移：l 空格 //空格可以右移到下一行 推荐空格上移：k下移：j 上下移动行 # 下移一行到第一个非空白字符串：+ enter //推荐enter上移一行到第一个非空白字符串：- 快速上下移动 # 移动到文档末尾：G移动到文档开头：gg向下移动一段：} //代码中的空行 也算一段的隔离标识向上移动一段：{向下移动一部分：[] //代码中就是一个函数一个函数的移动 比较实用向上移动一部分：][ 单词左右移动 # 向左移动一个单词：w向右移动一个单词：b移动到当前行开头：0 （零）移动到当前行末尾：$移动到当前单词末尾：e 选择文本 # 进入逐字可视模式：v退出可视模式：Esc 删除 # 删除该行： dd删除该单词：dw 复制粘贴 # 复制：y剪切：d 和删除类似粘贴：p复制当前行：yy剪切当前行：dd 撤销 # 撤销最后操作：u </description>
    </item>
    
    <item>
      <title>Xinference基础</title>
      <link>https://chain-code.github.io/docs/ai/generative-ai/xinference%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/generative-ai/xinference%E5%9F%BA%E7%A1%80/</guid>
      <description>介绍 # Xinference 是一个开源的 AI 模型推理平台。你可以把它想象成一个用来部署和管理各种大型 AI 模型（特别是大语言模型 LLMs）的工具或框架。它的目标是让开发者和研究人员能够轻松地在自己的硬件（无论是个人电脑、服务器还是云实例）上运行和使用这些强大的模型。
Xinference 的主要作用和特点：
简化模型部署 (Simplified Deployment): 运行大型 AI 模型通常需要复杂的环境配置和依赖管理。Xinference 极大地简化了这个过程，让你可以用简单的命令或通过 Web UI 来下载、设置和启动模型。 统一的 API (Unified API): 它为不同类型的模型（如聊天模型、嵌入模型、重排序模型、图像模型、音频模型等）提供了一套统一的、简洁的 API 接口。这意味着你可以用类似的方式与各种不同的模型进行交互，降低了学习和使用的成本。 广泛的模型支持 (Broad Model Support): Xinference 支持非常多的开源模型，涵盖了： 大语言模型 (LLMs): 如 Llama, ChatGLM, Qwen, Baichuan, Mixtral, Yi 等。 嵌入模型 (Embedding Models): 用于将文本转换为向量表示。 重排序模型 (Rerank Models): 用于优化搜索结果排序。 多模态模型 (Multimodal Models): 如处理图像和文本的模型。 图像模型和音频模型 等。 灵活的部署选项 (Flexible Deployment Options): 本地运行: 可以在你的个人笔记本电脑或工作站上运行。 分布式集群: 可以将模型部署在多台机器组成的集群上，以获得更强的计算能力或服务更大的负载。 硬件兼容性 (Hardware Compatibility): 支持在多种硬件上运行，包括： CPU NVIDIA GPU AMD GPU Apple Silicon (M系列芯片) 类 OpenAI API 兼容性 (OpenAI-Compatible API): 对于很多流行的模型（特别是 LLMs），Xinference 提供了与 OpenAI API 兼容的接口。这意味着如果你之前使用过 OpenAI 的 API，可以很容易地将应用切换到使用通过 Xinference 部署的本地模型，只需修改 API 的基地址 (base URL) 和 API 密钥即可。 成本效益和数据隐私 (Cost-Effectiveness &amp;amp; Data Privacy): 通过在本地或私有云上部署模型，你可以更好地控制成本（相比于完全依赖商业 API），并且可以确保数据不离开你的控制范围，增强了数据隐私和安全性。 Web UI 管理界面: 提供了一个用户友好的 Web 界面，方便用户查看可用的模型、管理正在运行的模型实例以及进行简单的交互测试。 安装 # 安装 — Xinference</description>
    </item>
    
    <item>
      <title>yolo-world</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/yolo-world/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/yolo-world/</guid>
      <description>YOLO-World：实时开放词汇的目标检测 # 本文档旨在介绍 YOLO-World，一个前沿的实时开放词汇目标检测模型。我们将探讨其核心概念、设计架构、关键用途，并将其与传统的 YOLO 模型进行对比，阐述其创新之处和优势。
1. 引言：什么是 YOLO-World？ # YOLO-World 是一个革命性的目标检测系统，它将强大的 YOLO 实时检测框架与开放词汇（Open-Vocabulary） 能力相结合。这意味着 YOLO-World 不再局限于预先训练时定义好的固定类别集，而是能够根据用户在推理时提供的 任意文本描述（词汇、短语） 来实时检测图像或视频中的相应目标。
其核心突破在于，它利用大规模视觉-语言预训练的知识，让模型能够理解文本提示（prompts）与图像中视觉内容之间的关联，从而实现对“世界万物”的检测潜力，而无需针对每一个新类别都进行重新训练。
2. 用途与价值主张 # 传统的目标检测器（如标准 YOLO）在训练完成后，只能识别训练数据中包含的特定类别（例如 PASCAL VOC 的 20 类或 COCO 的 80 类）。如果需要检测新的物体类别，就必须收集大量标注数据并重新训练模型，成本高昂且灵活性差。
YOLO-World 的出现旨在解决这一痛点，其主要用途和价值在于：
极高的灵活性与适应性： 用户可以随时定义新的、甚至是训练时从未见过的物体类别进行检测，只需提供相应的文本描述即可。例如，你可以让它检测“戴着红色帽子的狗”、“损坏的包裹”或“特定的工具名称”。 零样本（Zero-Shot）检测： 无需为新类别准备任何标注样本即可进行检测。 降低数据标注和重训练成本： 大幅减少了为扩展检测能力而进行的数据收集、标注和模型训练工作。 3. 架构设计：YOLO-World 如何工作？ # YOLO-World 的设计巧妙地融合了 YOLO 的高效检测架构和视觉-语言模型（VLMs）的语义理解能力。其核心组件通常包括：
YOLO 检测器骨干（Backbone）： 采用高效的 YOLO 系列骨干网络（如 YOLOv8 的 Darknet 变体）来提取图像的视觉特征。这些特征包含了丰富的空间和语义信息。 文本编码器（Text Encoder）： 使用强大的预训练文本编码器（如 CLIP 的文本编码器或其他 Transformer 模型）将用户输入的文本提示（类别名称、描述等）转换为高维度的文本嵌入向量（text embeddings）。这些向量捕获了文本的语义含义。 视觉-语言融合网络（Vision-Language Fusion Network）： 这是 YOLO-World 的关键创新。它不再是在模型输出端简单地比较视觉和文本特征，而是在检测网络的颈部（Neck）（例如 PANet 或其变种）中引入了文本嵌入信息。 RepVL-PAN (Region-Prompt Vision-Language Path Aggregation Network): YOLO-World 论文中提出的一种代表性结构。它允许文本嵌入在不同层级与视觉特征进行深度融合和交互。这使得模型能够将全局的文本语义信息有效地传递到局部的像素级特征，从而指导检测头关注与文本描述匹配的图像区域。 检测头（Detection Head）： 类似于标准 YOLO，检测头根据融合了文本信息的视觉特征来预测边界框（Bounding Boxes）和目标存在置信度（Objectness Score）。关键区别在于，类别的判断不再是输出固定类别的分数，而是通过计算检测到的物体视觉特征与输入文本提示的嵌入向量之间的相似度来确定该物体是否匹配某个文本描述。 预训练策略： YOLO-World 的强大能力来源于大规模的预训练。它在包含图像、对应边界框以及文本描述的大型数据集（如目标检测、视觉定位、图文对数据集）上进行训练，学习将视觉区域与自由形式的文本描述关联起来的能力。</description>
    </item>
    
    <item>
      <title>yolo底层原理</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/yolo%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/yolo%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86/</guid>
      <description>目标检测-YOLO的基本原理详解
YOLO原理与实现
深入理解YOLO原理
Yolov8原理详细解析</description>
    </item>
    
    <item>
      <title>yolo数据集</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/yolo%E6%95%B0%E6%8D%AE%E9%9B%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/yolo%E6%95%B0%E6%8D%AE%E9%9B%86/</guid>
      <description>https://data.baai.ac.cn/data</description>
    </item>
    
    <item>
      <title>业务代码</title>
      <link>https://chain-code.github.io/docs/c/%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/c/%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81/</guid>
      <description> 监听windows是否处于休眠唤醒状态 # #include &amp;lt;windows.h&amp;gt; #include &amp;lt;iostream&amp;gt; #include &amp;lt;powrprof.h&amp;gt; #pragma comment(lib, &amp;#34;Powrprof.lib&amp;#34;) using namespace std; ULONG CALLBACK DeviceCallback(PVOID Context, ULONG Type, PVOID Setting) { if (Type == PBT_APMSUSPEND) { cout &amp;lt;&amp;lt; &amp;#34;close&amp;#34; &amp;lt;&amp;lt; endl; } if (Type == PBT_APMRESUMESUSPEND) { cout &amp;lt;&amp;lt; &amp;#34;open&amp;#34; &amp;lt;&amp;lt; endl; } return ERROR_SUCCESS; } int main() { HPOWERNOTIFY g_power_notify_handle = NULL; DEVICE_NOTIFY_SUBSCRIBE_PARAMETERS params; params.Callback = DeviceCallback; params.Context = 0; PowerRegisterSuspendResumeNotification(DEVICE_NOTIFY_CALLBACK, &amp;amp;params, &amp;amp;g_power_notify_handle); MSG msg; while (GetMessage(&amp;amp;msg, NULL, 0, 0)) { TranslateMessage(&amp;amp;msg); DispatchMessage(&amp;amp;msg); } PowerUnregisterSuspendResumeNotification(g_power_notify_handle); return 0; } </description>
    </item>
    
    <item>
      <title>代码整洁之道</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E6%95%B4%E6%B4%81%E4%B9%8B%E9%81%93/</guid>
      <description>整洁代码 # 简单代码，根据重要顺序应该是：
能通过所有测试 没有重复代码 体现系统中的全部设计理念 包括尽量少的实体，比如方法、函数等 光写好代码是不够的，必须时时刻刻保持整洁。
有意义的命名 # 1、命名要名副其实，直接体现要做的事情。
openFile--&amp;gt;openVideo 2、选择一个好名字要花时间，但省下来的时间比花掉的多。一旦发现有更好的名称，就换掉旧的。
3、避免使用会引起误解的名称，尤其是遇到一些缩写，要防止其存在歧义。
//反面例子 type Std struct { Name string Age int Cls string } std 可能被误解为 Standard，而实际上它应该表示 Student（学生）。
4、不要去做无意义的区分，例如：taskData和taskInfo，名称虽然不同但意义没有差别。
variable一词不应当出现在变量名中，table一词不应该出现在表名中。例如：NameString不可取，名称后面建议不要加类型。
//反面例子 var recordMap map[int64]*RecordDbManager 总之在读者能区分的情况下、不产生歧义的情况下，越简单的命名就是好命名。
5、使用可以读出来的命名。
generationTimestamp要比genymdhms要好的多，即使前一个比较长一点，但是无所谓，表达准确意思。 6、使用可以搜索的名字
长名称胜于短名称，搜得到的名称比自编代码写就的名称要好。
//反面例子 for i：=0；i&amp;lt;30;i++{ s+=t[i]*4/5 } //4 和 5代表什么意思要用常量说明，否则维护人员很难注意 const WorkDays int =5 const realDays int =4 7、命名不要有无意义的前缀，人们只会看到名称中有意义的部分。
//反面例子 var m_student string 8、避免思维映射。例如：循环计数通常使用i，j，k，千万别用l 。专业的程序员编写其他人能理解的代码。
//反面例子 for l：=0；l&amp;lt;30;l++{ s+=t[l]*4/5 } 9、结构体命名不应该用动词，方法命名应该用动词，可加上get, is ,set 前缀。</description>
    </item>
    
    <item>
      <title>代码注释</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%BB%A3%E7%A0%81%E6%B3%A8%E9%87%8A/</guid>
      <description>/***　*　瓦瓦　十　*　十齱龠己　亅瓦車己　*　乙龍龠毋日丶　丶乙己毋毋丶　*　十龠馬鬼車瓦　己十瓦毋毋　*　鬼馬龠馬龠十　己己毋車毋瓦　*　毋龠龠龍龠鬼乙丶丶乙車乙毋鬼車己　*　乙龠龍龍鬼龍瓦　十瓦毋乙瓦龠瓦亅　*　馬齱龍馬鬼十丶日己己己毋車乙丶　*　己齱馬鬼車十十毋日乙己己乙乙　*　車馬齱齱日乙毋瓦己乙瓦日亅　*　亅車齺龖瓦乙車龖龍乙乙十　*　日龠龠十亅車龍毋十十　*　日毋己亅　己己十亅亅　*　丶己十十乙　丶丶丶丶丶　*　亅己十龍龖瓦　丶　丶　乙十　*　亅己十龠龖毋　丶丶　丶己鬼鬼瓦亅　*　十日十十日亅丶亅丶　丶十日毋鬼馬馬車乙　*　十日乙十亅亅亅丶　十乙己毋鬼鬼鬼龍齺馬乙　*　丶瓦己乙十十亅丶亅乙乙乙己毋鬼鬼鬼龍齱齺齺鬼十　*　乙乙十十十亅乙瓦瓦己日瓦毋鬼鬼龠齱齱龍龍齱齱毋丶　*　亅十十十十乙瓦車毋瓦瓦日車馬龠龍龍龍龍龍龠龠龠馬亅*　十十十十己毋車瓦瓦瓦瓦鬼馬龠龍龠龠龍龠龠龠馬龠車*　亅十十日毋瓦日日瓦鬼鬼鬼龠龠馬馬龠龍龍龠馬馬車*　亅亅亅乙瓦瓦毋車車車馬龍龠鬼鬼馬龠龍龍龠馬馬鬼*　丶丶乙亅亅乙車鬼鬼鬼毋車龍龍龠鬼馬馬龠龍齱齱龍馬鬼*　亅己十十己十日鬼鬼車瓦毋龠龍龠馬馬龠龠龠齱齺齺齱龠鬼*　亅乙乙乙十車馬車毋馬齱齱龍龠龠龠馬龠龍齱龍龠龠鬼瓦*　丶毋龠鬼車瓦車馬龠龍龠龠龍齱齱龠馬馬鬼毋日*　十乙己日十　丶己鬼龍齱齺齱龍馬馬馬車毋己*　丶十己乙亅丶　亅瓦馬龠龍龠龠馬毋瓦乙*　丶十十乙亅十　亅己瓦車馬龠鬼車瓦乙*　丶十乙十十丶　丶丶亅十瓦鬼車瓦己*　丶亅亅丶　亅日瓦日*　丶*/ /*** * .</description>
    </item>
    
    <item>
      <title>借助AI编程工具从零构建数据可视化平台复盘</title>
      <link>https://chain-code.github.io/docs/ai/basic/%E5%80%9F%E5%8A%A9ai%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0%E5%A4%8D%E7%9B%98-/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/%E5%80%9F%E5%8A%A9ai%E7%BC%96%E7%A8%8B%E5%B7%A5%E5%85%B7%E4%BB%8E%E9%9B%B6%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B9%B3%E5%8F%B0%E5%A4%8D%E7%9B%98-/</guid>
      <description>上次看到晖哥使用Trae从零搭建了一个雷电云的埋点数据分析平台，感觉很有意思。正好迭代最后几天进入覆盖测试阶段，手上任务都做完了，也想搭建一个视频取证的数据统计分析平台；一来多学习学习前端知识，二来埋点数据可视化页面有助于团队基于数据做出更精准的决策和迭代优化。
项目说明 # 项目采用 Vite + Vue 3 + TypeScript 技术栈构建，并搭配了 Element Plus UI 组件库。通过 vue-router 管理页面导航，并使用 Axios 与公司部署的 Elastic APM 数据库进行交互。属于纯前端项目，没有任何后端代码。
项目使用Trae工具从零开始搭建，除了一些参数翻译字典手敲之外，全程由AI生成；项目所有功能都是基于视频取证产品埋点数据结构体设计，后续会根据产品需求在空闲之余继续添加功能；我是一个后端程序员，前端只是可以稍微看得懂代码。
效果展示 # 完整地址：http://172.16.60.141:3000 三楼同事可自行查看
概览 # 概览功能支持任务状态分布、版本分布、任务成功率等等。
数据展示 # 数据展示界面可删选所有记录、任务等，支持搜索等功能，详细界面还翻译了任务参数
任务使用排行榜 # 任务使用排行榜，分记录和任务两个维度，按使用次数排行。
点击具体任务，可查看此任务相关参数统计图，及每个任务的详细信息
像高级识别任务物体描述是加密上传到elastic平台的，这里还做了解密处理。
IP地址排行榜 # IP地址排行榜对不同IP的数据进行了分类统计排序，可直观看到我们的用户分布。
心得体会 # 最热门编程语言 # 这张图内大多数情形我们或多或少都经历过，虽说是网上的段子，但它真实记录了我们驾驭AI过程中走过的路。
转变角色 # 随着AI技术的发展，特别是像Trae这样的编程助手，正在如何重塑我们的开发工作流。我们基本上不再编码，是否可以逐渐向“架构师”的方向转变，从执行者到定义者再到决策者。技术知识仍然重要，但更重要的是理解业务需求、系统设计和用户体验等更广泛的技能。
整体规划 # 开发项目之前，先想清楚要做什么？它的框架是什么样的？想清楚之后，将它写入一个文档，可以让AI进行润色。我们在后面开发的过程中可以根据实际情况不断的修改这个文档，每当AI开始不受控制胡说八道的时候，先让它阅读一下这个文档，再跟上你的功能需求描述。
不仅仅是框架，当遇到一些重复且复杂的任务，也可以写成文档，让AI根据文档的流程去处理，避免AI自行发挥。
一次只办一件事 # 一次只办一件事情，不要让它同时做多个任务，贪多必失
你必须看得懂代码 # 如果你不知道它生成的代码是什么意思，或许刚开始没有什么问题；但随着你问的问题越来越多，功能越来越复杂，你埋下的“坑”就会像雪球一样越滚越大。一个微小的改动都可能引发连锁反应，AI还自行解决不了。由于你从未真正理解底层逻辑，排查和解决问题的困难很大，只能不断地将报错复制给AI，继续生成更多不可控的代码，形成恶性循环。
不要随意接受任何代码 # 当我们发现功能没有问题的时候，很容易接受所有代码。但这样会有隐患，你不知道它在什么时候改了一行会影响到其他功能的代码，与其后面排查半天，不如每次仔细审查每一行代码。
先问自己在问什么？ # 这是我在某个文章中看到的一句话，很受触动。
比如你在开发一个项目前，你问别人，你用什么开发工具？
很正常的提问，却是一个非常差的问题，因为它需要多步的沟通来确认真正的问题，跟晖哥之前说的“报联商”有相似的地方。比如，你指的是那类开发工具？（IDE、文本编辑器、版本控制、API测试？），或者针对什么技术栈？（前端、Java、Go、Python？），由或者你的使用场景是什么？
正确的提问方式是什么？ 以“开发工具”这个问题为例，先想清楚我要问什么。
你为什么想问这个？ 是你正在用的工具遇到了问题（例如：“VS Code在打开大型项目时卡顿，有何优化方案？”），还是你要开始学习一门新语言（例如：“刚开始学Go，有哪些轻量级的IDE推荐？”）？如果是前者，这完全是另一个“解决问题”而非“寻求推荐”的问题。问题解决了，你根本不用换工具。 如果你想找一个新工具，最基本的，你要定义什么是“合适”，罗列完整自己的需求。哪些是必须项，哪些是加分项，哪些可有可无。必须项： 免费、对Python支持好、调试方便。加分项： 界面美观、插件市场丰富、Git集成流畅。可有可无： 内置数据库管理工具。于是你的问题变成了：“我需要一个免费的、对Python和Django框架支持良好的IDE，调试功能要强大，有什么推荐？” 。 更进一步，你应该先搜索一下，了解主流的选择（比如PyCharm， VS Code， Vim等），看看相关的测评，试试它们的官方基础功能。把这些工具和你的需求进行匹配。现在，你的问题是：“我想学Python Web开发，需要免费且调试功能强的IDE。了解到VS Code很轻量且免费，PyCharm专业版收费但社区版免费。我的需求是免费 、Django支持好、 调试方便。请问社区版PyCharm和配置了Python插件的VS Code相比，在调试和Django支持上差距大吗？或者有更优解？” 对AI下达指令也是一样的道理，先问问自己在问什么？</description>
    </item>
    
    <item>
      <title>分布式面试题</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%88%86%E5%B8%83%E5%BC%8F%E9%9D%A2%E8%AF%95%E9%A2%98/</guid>
      <description> 什么是分布式系统？和集群的区别？ # 分布式就是把一个集中式系统拆分成多个系统，每一个系统单独对外提供部分功能，整个分布式系统整体对外提供一整套服务。对于访问分布式系统的用户来说，感知上就像访问一台计算机一样。
分布式（distributed）是指在多台不同的服务器中部署不同的服务模块，通过远程调用协同工作，对外提供服务。
集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。
什么是分布式系统的一致性？ # 所谓一致性，是指数据在多个副本之间是否能够保持一致的特性。
大的分类上面，主要有三种，分别是强一致性、弱一致性和最终一致性：
强一致性模型（Strong Consistency）： 在强一致性模型下，系统保证每个读操作都将返回最近的写操作的结果，即任何时间点，客户端都将看到相同的数据视图。这包括线性一致性（Linearizability）、顺序一致性（Sequential Consistency）和严格可串行性（Strict Serializability）等子模型。强一致性模型通常牺牲了可用性来实现数据一致性。 弱一致性模型（Weak Consistency）： 弱一致性模型放宽了一致性保证，它允许在不同节点之间的数据访问之间存在一定程度的不一致性，以换取更高的性能和可用性。这包括因果一致性（Causal Consistency）、会话一致性（Session Consistency）和单调一致性（Monotonic Consistency）等子模型。弱一致性模型通常更注重可用性，允许一定程度的数据不一致性。 最终一致性模型（Eventual Consistency）： 最终一致性模型是一种最大程度放宽了一致性要求的模型。它允许在系统发生分区或网络故障后，经过一段时间，系统将最终达到一致状态。这个模型在某些情况下提供了很高的可用性，但在一段时间内可能会出现数据不一致的情况。 线性一致性和顺序一致性 # 线性一致性是一种最强的一致性模型，它强调在分布式系统中的任何时间点，读操作都应该返回最近的写操作的结果。
顺序一致性也是一种强一致性模型，但相对于线性一致性而言，它放宽了一些限制。在顺序一致性模型中，系统维护一个全局的操作顺序，以确保每个客户端看到的操作顺序都是一致的。
与线性一致性不同，顺序一致性不强调实时性，只要操作的顺序是一致的，就可以接受一些延迟。
什么是一致性哈希？ # 一致性哈希是一种用于分布式系统中数据分片和负载均衡的算法。它的目标是在节点的动态增加或删除时，尽可能的减少数据迁移和重新分布的成本。
实现一致性哈希算法首先需要构造一个哈希环，然后把他划分为固定数量的虚拟节点，如2^32。那么他的节点编号就是 0-2^32-1：
接下来， 我们把128张表作为节点映射到这些虚拟节点上，每个节点在哈希空间上都有一个对应的虚拟节点：
hash(table_0000)%2^32、hash(table_0001)%2^32、hash(table_0002)%2^32 &amp;hellip;. hash(table_0127)%2^32
在把这些表做好hash映射之后，我们就需要存储数据了，现在我们要把一些需要分表的数据也根据同样的算法进行hash，并且也将其映射哈希环上。
hash(buyer_id)%2^32：hash(12321)%2^32、hash(34432)%2^32、hash(54543)%2^32 &amp;hellip;. hash(767676)%2^32
这样，这个hash环上的虚拟节点就包含两部分数据的映射了，一部分是存储数据的分表的映射，一部分是真实要存储的数据的映射。
那么， 我们最终还是要把这些数据存储到数据库分表中，那么做好哈希之后，这些数据又要保存在哪个数据库表节点中呢？
其实很简单，只需要按照数据的位置，沿着顺时针方向查找，找到的第一个分表节点就是数据应该存放的节点：
因为要存储的数据，以及存储这些数据的数据库分表，hash后的值都是固定的，所以在数据库数量不变的情况下，下次想要查询数据的时候，只需要按照同样的算法计算一次就能找到对应的分表了。
以上，就是一致性hash算法的原理，那么，再回到我们开头的问题，如果我要增加一个分表怎么办呢？
我们首先要将新增加的表通过一致性hash算法映射到哈希环的虚拟节点中：
这样，会有一部分数据，因为节点数量发生变化，那么他顺时针遇到的第一个分表可能就变了。
相比于普通hash算法，在增加服务器之后，影响的范围非常小，只影响到一部分数据，其他的数据是不需要调整的。
优点：
数据均衡：在增加或删除节点时，一致性哈希算法只会影响到少量的数据迁移，保持了数据的均衡性。
高扩展性：当节点数发生变化时，对于已经存在的数据，只有部分数据需要重新分布，不会影响到整体的数据结构。
缺点：
hash倾斜：在节点数较少的情况下，由于哈希空间是有限的，节点的分布可能不够均匀，导致数据倾斜。
节点的频繁变更：如果频繁添加或删除节点，可能导致大量的数据迁移，造成系统压力。
哈希倾斜 # 其实，hash倾斜带来的主要问题就是如果数据过于集中的话，就会使得节点数量发生变化时，数据的迁移成本过高。
那么想要解决这个问题，比较好的办法就是增加服务器节点，这样节点就会尽可能的分散了。
但是如果没有那么多服务器，我们也可以引入一些虚拟节点，把一个服务器节点，拆分成多个虚拟节点，然后数据在映射的时候先映射到虚拟节点，然后虚拟节点在找到对应的物理节点进行存储和读取就行了。
常见的分布式事务有哪些？ # 分布式事务的目的是保证分布式系统中的多个参与方的数据能够保证一致性。
一致性分为强一致性，和最终一致性。
如果想要强一致性，就一定要引入协调者，通过协调者来协调所有参与者进行提交或者回滚。
这类方案包含基于XA规范的二阶段即三阶段提交、以及支持2节点提交。
什么是XA规范？ # 有了二阶段提交为什么还需要3阶段提交？ # </description>
    </item>
    
    <item>
      <title>分库分表</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/</guid>
      <description>什么是分库？分表？ # **分库主要解决的是并发量大的问题。**因为数据库的连接数是有限的，虽然可以调整，但是也不是无限调整的。
比较典型的分库的场景就是我们在做微服务拆分的时候，就会按照业务边界，把各个业务的数据从一个单一的数据库中拆分开，分别把订单、物流、商品、会员等数据，分别放到单独的数据库中。
**分表主要解决的是数据量大的问题。**假如你的单表数据量非常大，因为并发不高，数据量连接可能还够，但是存储和查询的性能遇到了瓶颈了，你做了很多优化之后还是无法提升效率的时候，就需要考虑做分表了。
一般我们认为，单表行数超过 500 万行或者单表容量超过 2GB之后，才需要考虑做分库分表了，小于这个数据量，遇到性能问题先建议大家通过其他优化来解决。
分区和分表的区别？ # 数据库中数据量过多，表太大的时候，不仅可以做分库分表，还可以做分区，分区和分表类似，都是按一定规则将一张大表进行分解。
主要是分区和分表后数据的数据存储方式有变化。
在Innodb中（8.0之前），表存储主要依赖两个文件，分别是.frm文件和.ibd文件。.frm文件用于存储表结构定义信息，而.ibd文件则用于存储表数据。
users_p1.ibd users_p2.ibd users_p3.ibd users_p4.ibd users.frm 在做了分区后，表面是还是只有一张表，只不过数据保存在不同的位置上了（同一个.frm文件），在做数据读取的时候操作的表名还是users表，数据库会自己去组织各个分区的数据。
users_1.ibd users_1.frm users_2.ibd users_2.frm users_3.ibd users_3.frm users_4.ibd 而在做了分表之后，不管是表面上，还是实际上，都已经是不同的表了（多个.frm文件），数据库操作的时候，需要去指定具体的表名。
一般来说，数据量变大时，我们应该先考虑分区，分区搞不定再考虑分表。
MySQL 数据库支持的分区类型为水平分区。
常见的表分区实践中，可以按照以下一些原则进行分区：
按照系统负载，将数据分到不同的区域中； 按照应用程序查询模式，将数据库分为不同的分区； 按照月份或者年份分区； 通过实践哈希法可以将记录放置到不同的分区中； 基于范围查询，使用分段来将记录放置到不同的分区中，以便提高查询效率。 分表字段如何选择？ # 根据业务慎重选择
通常，如果有特殊的诉求，比如按照月度汇总、地区汇总等以外，我们通常建议大家按照买家Id进行分表。因为这样可以避免一个关键的问题那就是——数据倾斜（热点数据）。
买家还是卖家 # 首先，我们先说为什么不按照卖家分表？
因为我们知道，电商网站上面是有很多买家和卖家的，但是，一个大的卖家可能会产生很多订单，比如像苏宁易购、当当等这种店铺，他每天在天猫产生的订单量就非常的大。如果按照卖家Id分表的话，那同一个卖家的很多订单都会分到同一张表。
那就会使得有一些表的数据量非常的大，但是有些表的数据量又很小，这就是发生了数据倾斜。这个卖家的数据就变成了热点数据，随着时间的增长，就会使得这个卖家的所有操作都变得异常缓慢。
但是，买家ID做分表字段就不会出现这类问题，因为不太容易出现一个买家能把数据买倾斜了。
但是需要注意的是，我们说按照买家Id做分表，保证的是同一个买家的所有订单都在同一张表 ，并不是要给每个买家都单独分配一张表。
我们在做分表路由的时候，是可以设定一定的规则的，比如我们想要分1024张表，那么我们可以用买家ID或者买家ID的hashcode对1024取模，结果是0000-1023，那么就存储到对应的编号的分表中就行了。
卖家查询怎么办？ # 首先，业务问题我们要建立在业务背景下讨论。电商网站订单查询有几种场景？
1、买家查自己的订单
2、卖家查自己的订单
3、平台的小二查用户的订单。
首先，我们用买家ID做了分表，那么买家来查询的时候，是一定可以把买家ID带过来的，我们直接去对应的表里面查询就行了。
那如果是卖家查呢？卖家查询的话，同样可以带卖家id过来，那么，我们可以有一个基于binlog、flink等准实时的同步一张卖家维度的分表，这张表只用来查询，来解决卖家查询的问题。
​ 数据同步
本质上就是用空间换时间的做法。
不知道大家看到这里会不会有这样的疑问：同步一张卖家表，这不又带来了大卖家的热点问题了吗？
首先，我们说同步一张卖家维度的表来，但是其实所有的写操作还是要写到买家表的，只不过需要准实时同步的方案同步到卖家表中。也就是说，我们的这个卖家表理论上是没有业务的写操作，只有读操作的。
所以，这个卖家库只需要有高性能的读就行了，那这样的话就可以有很多选择了，比如可以部署到一些配置不用那么高的机器、或者其实可以干脆就不用MYSQL，而是采用HBASE、PolarDB、Lindorm等数据库就可以了。这些数据库都是可以海量数据，并提供高性能查询的。
还有呢就是，大卖家一般都是可以识别的，提前针对大卖家，把他的订单，再按照一定的规则拆分到多张表中。因为只有读，没有写操作，所以拆分多张表也不用考虑事务的问题。
订单查询怎么办？ # 上面说的都是有买卖家ID的情况，那没有买卖家ID呢？用订单号直接查怎么办呢？
这种问题的解决方案是，在生成订单号的时候，我们一般会把分表结果编码到订单号中去，因为订单生成的时候是一定可以知道买家ID的，那么我们就把买家ID的路由结果比如1023，作为一段固定的值放到订单号中就行了。这就是所谓的&amp;quot;基因法&amp;quot;</description>
    </item>
    
    <item>
      <title>包管理</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%8C%85%E7%AE%A1%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%8C%85%E7%AE%A1%E7%90%86/</guid>
      <description>Golang的包管理有四种方式，分别是：go path、go vendor、go module、go work，其中是go path、go vendor已经算是“上古时代”的产物，本文主要详细介绍一下后两种管理方式。
1. Go Module # 从Go1.13版本开始，go module将是Go语言默认的依赖管理工具，平常我们用的最多的命令应该就是：go mod init myModule， go mod tidy，这些相信大家已经很熟练了，但是到实际要引用某个包下的函数时，往往写import xxx/xxx语句时会不知道该怎么开头，以下是一个示例，包含了工作中的大部分情况：
testModule/ |-- softwareExample| |-- example.go| |-- go.mod| |-- go.sum| |-- httpclient| | |-- httpclient.go| | |-- task| | | `-- taskapi.go`-- utils.go|-- go.mod|-- go.sum 以上文件结构展示了一个Module中包含一个子Module的情况，另外softwareExample模块中还有多个package的情况。以上各个文件中的内容如下：
testModule/go.mod
module test go 1.20 testModule/utils.go
package testUtil func Util_ADD(a, b int){ fmt.Println(a+b) } testModule/softwareExample/go.mod</description>
    </item>
    
    <item>
      <title>单元测试</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95/</guid>
      <description>单元测试 # https://learnku.com/articles/52896
https://www.topgoer.com/%E5%87%BD%E6%95%B0/%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.html
介绍 # 单元测试可以检查我们的代码能否按照预期进行，代码逻辑是否有问题，以此可以提升代码质量。 简单来说单元测试就是针对某一个函数方法进行测试，我们要先测试正确的传值与获取正确的预期结果，然后再添加更多测试用例，得出多种预期结果。尽可能达到该方法逻辑没有问题，或者问题都能被我们预知到。这就是单元测试的好处。
Go 语言的单元测试默认采用官方自带的测试框架，通过引入 testing 包以及 执行 go test 命令来实现单元测试功能。
在源代码包目录内，所有以 _test.go 为后缀名的源文件会被 go test 认定为单元测试的文件，这些单元测试的文件不会包含在 go build 的源代码构建中，而是单独通过 go test 来编译并执行。
规范 # Go 单元测试的基本规范如下：
每个测试函数都必须导入 testing 包。测试函数的命名类似func TestName(t *testing.T)，入参必须是 *testing.T 测试函数的函数名必须以大写的 Test 开头，后面紧跟的函数名，要么是大写开关，要么就是下划线，比如 func TestName(t *testing.T) 或者 func Test_name(t *testing.T) 都是 ok 的， 但是 func Testname(t *testing.T)不会被检测到 通常情况下，需要将测试文件和源代码放在同一个包内。一般测试文件的命名，都是 {source_filename}_test.go，比如我们的源代码文件是allen.go ，那么就会在 allen.go 的相同目录下，再建立一个 allen_test.go 的单元测试文件去测试 allen.go 文件里的相关方法。 当运行 go test 命令时，go test 会遍历所有的 *_test.</description>
    </item>
    
    <item>
      <title>同时使用github和gitlab</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8github%E5%92%8Cgitlab/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E5%90%8C%E6%97%B6%E4%BD%BF%E7%94%A8github%E5%92%8Cgitlab/</guid>
      <description>同一台电脑同时使用gitHub和gitLab # 工作中我们有时可能会在同一台电脑上使用多个git账号，例如：公司的gitLab账号，个人的gitHub账号。怎样才能在使用gitlab与github时，切换成对应的账号，并且免密？
gitlab配置ssh Key # GitLab使用SSH协议与Git进行安全通信。当您使用SSH密钥对GitLab远程服务器进行身份验证时，您不需要每次都提供您的用户名和密码。SSH使用两个密钥，公钥和私钥。公钥可以分发。私钥应该受到保护。上传您的公钥是不可能泄露机密数据的。
配置GitLab的SSH Key，打开GitBash或者是cmd或者是shell
1、配置name
git config --global user.name &amp;#34;Kem.Gong&amp;#34; 2、配置email
git config --global user.email kemgong@163.com 3、生成SSH key，输入命令
ssh-keygen -t rsa 一直按回车既可，不要输入东西
4、输入
cat ~/.ssh/id_rsa.pub 5、将输出的内容复制，然后打开GitLab，单击settings-&amp;gt;SSH Keys,把复制的内容粘贴到到Key中，点击Add key按钮完成添加
配置github # 1、生成ssh密钥并配置
ssh-keygen -t rsa -C &amp;#34;github邮箱地址&amp;#34; -f ~/.ssh/github_rsa 2、将github公钥即github_rsa.pub中的内容配置到自己的github上
3、打开github_rsa.pub，复制有所内容，填入后点击“Add SSH key”按钮。接着可能会跳转页面需要输入你的GitHub密码，输入确定即可。
配置git，访问不同host时使用不同的密钥 # 进入密钥生成的位置（C:/Users/用户名/.ssh/），手动创建一个config文件（注意这个config文件要无后缀）。
在新建的config文件里面配置如下内容：
# 自己的github账号配置 Host github.com port 22 User git HostName github.com PreferredAuthentications publickey IdentityFile C:\Users\xiaoqq\.ssh\github_rsa # 公司的gitlab账号配置(HostName为公司的gitlab地址) Host gitlab.xxx.cn port 22 User git HostName gitlab.</description>
    </item>
    
    <item>
      <title>图像增强介绍</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E4%BB%8B%E7%BB%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E4%BB%8B%E7%BB%8D/</guid>
      <description>图像增强技术介绍 # 什么是图像增强？ # 图像增强 (Image Enhancement) 是指通过一系列图像处理技术，有选择性地突出图像中感兴趣的特征、抑制不必要的特征，或改善图像的视觉效果，使其更适合于人类观察或计算机分析。其目的不是试图恢复图像的原始信息（这更偏向于图像复原），而是改善图像的质量，使其在特定应用场景下更有用。
常用的图像增强原理/类别：
空间域增强 (Spatial Domain Enhancement): 直接对图像的像素值进行操作。 点操作 (Point Operations): 对单个像素进行处理，不考虑其邻域像素。常见的有： 灰度变换: 如对比度拉伸、亮度调整、伽马校正、直方图均衡化等。通过修改像素的灰度级来改善图像的对比度和动态范围。 伪彩色处理: 将灰度图像的不同灰度级映射为不同的颜色，以突出细节。 邻域操作 (Neighborhood Operations): 基于像素及其邻域像素的值进行处理。常见的有： 图像平滑 (Smoothing): 使用均值滤波、中值滤波、高斯滤波等去除噪声，模糊图像。 图像锐化 (Sharpening): 使用拉普拉斯算子、梯度算子（Sobel, Prewitt）等增强图像的边缘和细节，使图像更清晰。 频率域增强 (Frequency Domain Enhancement): 将图像变换到频率域（如傅里叶变换），对频率分量进行修改，然后再反变换回空间域。 低通滤波 (Low-pass Filtering): 衰减高频分量，保留低频分量，效果类似于空间域的平滑，可以去除噪声。 高通滤波 (High-pass Filtering): 衰减低频分量，保留高频分量，效果类似于空间域的锐化，可以增强边缘。 带通/带阻滤波: 保留或去除特定频率范围的分量。 同态滤波 (Homomorphic Filtering): 一种在频率域中同时压缩亮度范围和增强对比度的技术，常用于改善光照不均的图像。 基于深度学习的增强 (Deep Learning-based Enhancement): 利用深度神经网络（尤其是卷积神经网络 CNN、生成对抗网络 GAN、Transformer 等）学习从低质量图像到高质量图像的复杂映射关系。这是当前研究的热点和主流方向，在超分辨率、去噪、去模糊、去雨去雾、低光照增强等方面取得了突破性进展。 SwinIR 架构、超分原理及技术 # SwinIR 架构设计: SwinIR 将 Swin Transformer 成功应用于图像复原任务。其核心架构主要包括三个部分：</description>
    </item>
    
    <item>
      <title>图像超分原理</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%8E%9F%E7%90%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/%E5%9B%BE%E5%83%8F%E8%B6%85%E5%88%86%E5%8E%9F%E7%90%86/</guid>
      <description>三分钟读懂《超分辨率技术》
AIGC算法：GAN图像超分原理与实现
图像超分辨率技术-简介
一文掌握图像超分辨率重建（算法原理、Pytorch实现）——含完整代码和数据</description>
    </item>
    
    <item>
      <title>在CGO中集成和调用DLL文件</title>
      <link>https://chain-code.github.io/docs/c/%E5%9C%A8cgo%E4%B8%AD%E9%9B%86%E6%88%90%E5%92%8C%E8%B0%83%E7%94%A8dll%E6%96%87%E4%BB%B6/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/c/%E5%9C%A8cgo%E4%B8%AD%E9%9B%86%E6%88%90%E5%92%8C%E8%B0%83%E7%94%A8dll%E6%96%87%E4%BB%B6/</guid>
      <description>前言 # CGO是Go语言中的一种机制，用于调用C代码或集成C库，它可以让我们通过Go程序直接使用C语言写的库文件（如.dll文件）。这种功能在需要利用已有的C库时非常有用，尤其是在Windows环境下的DLL文件调用。
本文将围绕如何在CGO中集成和调用DLL文件，介绍相关步骤和注意事项。
本文不会涉及到C动态库的内部实现，重点是使用CGO调用动态库的方法介绍
不同类型C库的分发方式 # 要使用C库，首先需要对不同类型库的相关文件有一个基本认识，c库通常有以下两种类型
静态库 头文件（*.h） 库文件（.lib/.a） 动态库 头文件（*.h） 库文件（.lib/.a）（这里的lib库是导入库类型，包含函数所在的DLL文件和文件中函数位置的信息，并没有具体的实现） 动态库（.dll/.so） 区分静态库和动态库 # 因为正常链接过程都是使用.lib或者.a的文件，在没有明确告知库类型的情况下，可能会不知道自己使用的是静态库还是动态库，因此需要一个方法来区分，以MSVC环境的为例
使用lib命令行工具
lib /list *库文件* 如果输出都是.obj目标文件，那么就是静态库，相反如果是.dll动态库，那么这个.lib文件就是动态库的导入库
lib /list avp.lib
CGO调用DLL动态库函数的方法 # 要使用CGO，需要确保Go编译器开启了cgo，可以用go env来检查，确保环境变量CGO_ENABLED=1（现在默认都是开启的，后面编译阶段如果出现问题可以排查一下是否是这个原因导致的）；以及需要配置好MingW的环境，这步不再赘述
编译时链接动态库 # 编译时链接动态库是指在编译时通过#cgo LDFLAGS指定动态库，生成的可执行文件在运行时会自动依赖这些库。Go程序启动时会自动加载这些动态库，无需手动管理动态库的加载和函数的绑定。
和静态库的链接方法一样，在LDFLAGS中指定库目录以及链接的库即可
/* #cgo LDFLAGS: -L${SRCDIR}/../lib -lavp */ import &amp;#34;C&amp;#34; 后续使用就引入头文件，直接调用库函数即可
/* #include &amp;lt;stdlib.h&amp;gt; #include &amp;#34;c_avp.h&amp;#34; */ import &amp;#34;C&amp;#34; func (meTool *MediaTool) cGetVideoInfo(cInputVideoPath *C.char, needDecodeData C.bool) (result string) { // c_get_video_properties接口 在c_avp.h中声明 result = C.GoString(C.c_get_video_properties(cInputImagePath, needDecodeData)) return } 这种方案的优缺点：</description>
    </item>
    
    <item>
      <title>场景题</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%9C%BA%E6%99%AF%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E5%9C%BA%E6%99%AF%E9%A2%98/</guid>
      <description>如何设计一个订单号生成服务？ # 唯一性：订单号必须保持唯一，uuid，Snowflake等。 数据量：在设计订单号的时候，需要充分的考虑到后续数据量变大的情况下该如何兼容。所以需要提前预留出足够的位数。 可读性：订单号应该易于理解和记忆，可以根据业务需求自定义订单号的格式和组成方式，例如使用时间戳、随机数、用户ID等信息来构造订单号。 基因法：订单系统到最后都可能会考虑分库分表，所以在最初设计订单号的时候，需要考虑将和分表有关的字段编码到订单号中，如买家ID等。 可扩展性：订单号生成服务需要支持高并发、分布式部署和横向扩展等特性，可以采用分布式ID生成器、Redis等技术来实现。 高性能：订单号生成服务需要具有高性能和低延迟的特点，可以使用内存缓存、异步处理等技术来优化性能。 高可用：订单号生成服务需要保证高可用性，可以使用多节点部署、负载均衡、健康检查等技术来提高系统的可靠性和稳定性。 通过2位数字表示业务类型，如交易订单、支付单、结算单等都是不同的业务类型，可以有不同的编号。中间的18-20位用一个唯一的ID来表示，可以用雪花算法，也可以用Leaf，总之就是他需要保证唯一性。最后4位，基于基因法，将分表后的结果获取到，把他也编码到订单号中。
订单到期关闭如何实现？ # 定时任务，最简单，最方便
redis过期监听不推荐，因为Redis官网上明确的说过，Redis并不保证Key在过期的时候就能被立即删除，更不保证这个消息能被立即发出。所以，消息延迟是必然存在的，随着数据量越大延迟越长，延迟个几分钟都是常事儿。
如何设计一个购物车功能？ # 未登陆购物车
对于未登录的用户，其实他的购物车的信息没必要存储在后端，只需要在客户端做临时缓存就行了。客户端存储可以选择Cookie 和 LocalStorage等技术。
在存储时，只需要设计一个JSON格式就可以了，因为用户没登录，所以也就不需要标识数据属于谁，那么只需要如下存储即可：
{ &amp;#34;cart&amp;#34;: [ { &amp;#34;SKUID&amp;#34;: 10086, &amp;#34;timestamp&amp;#34;: 1666513990, &amp;#34;count&amp;#34;: 2 }, { &amp;#34;SKUID&amp;#34;: 10010, &amp;#34;timestamp&amp;#34;: 1666513990, &amp;#34;count&amp;#34;: 10 } ] } 已登陆
如果是使用数据库，那么就直接建表存储就行了，表中主要需要包含user_id、sku_id、count、time_stamp等几个业务字段就可以了。这样每一个加过购物车的用户都有一条记录。
如果使用Redis来保存的话，其实也简单，只需要在上面的未登录用户的购物车的基础上增加一个user_id作为key就行了：
{ &amp;#34;KEY&amp;#34;: 12343123, &amp;#34;VALUE&amp;#34;: [ { &amp;#34;SKUID&amp;#34;: 10086, &amp;#34;timestamp&amp;#34;: 1666513990, &amp;#34;count&amp;#34;: 2 }, { &amp;#34;SKUID&amp;#34;: 10010, &amp;#34;timestamp&amp;#34;: 1666513990, &amp;#34;count&amp;#34;: 10 } ] } 如果你的业务量突然提升100倍QPS，怎么做？ # 正常情况</description>
    </item>
    
    <item>
      <title>基础知识</title>
      <link>https://chain-code.github.io/docs/ai/basic/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</guid>
      <description>NumPy 数组 (numpy.ndarray) # 是什么？
NumPy (Numerical Python) 是 Python 语言的一个核心库，专门用于进行科学计算，特别是处理大型多维数组和矩阵。 NumPy 最核心的对象是 ndarray（N-dimensional array），它是一个同质（所有元素类型相同，如全是整数或全是浮点数）的多维数组。你可以把它想象成一个灵活的、强大的网格结构，可以是一维（向量）、二维（矩阵）、三维（立方体）甚至更高维度。 为什么重要？
性能： NumPy 底层是用 C 语言实现的，其数组操作（如数学运算、索引、切片）比 Python 内置的列表（list）快得多，尤其是处理大量数据时。这是因为它利用了向量化操作，避免了 Python 级别的循环。 内存效率： NumPy 数组在内存中是连续存储的（通常情况下），这使得访问和操作更加高效。 功能丰富： 提供了大量的数学函数（线性代gebra、傅里叶变换、随机数生成等）来操作这些数组。 生态基础： NumPy 是许多其他 Python 科学计算库（如 SciPy、Pandas、Scikit-learn、Matplotlib）的基础。很多库的输入输出都接受或返回 NumPy 数组。 关键特性：
维度 (Dimensions/Axes)： 数组的“方向”数量，称为 ndim。 形状 (Shape)： 一个描述数组在每个维度上大小的元组，称为 shape。例如，一个 3x4 的矩阵，shape 是 (3, 4)。 数据类型 (Data Type/dtype)： 数组中元素的数据类型，如 int32, float64, uint8 (常用于图像)。 常见用途：
任何需要高效数值计算的场景。 数据分析中的数据存储和预处理。 图像表示： 图像可以被看作是二维（灰度图）或三维（彩色图）的像素网格，NumPy 数组是表示它们的自然方式。例如，一个 640x480 的彩色图像可以用一个 shape 为 (480, 640, 3) 的 NumPy 数组表示（高度、宽度、颜色通道）。 示例：</description>
    </item>
    
    <item>
      <title>奇怪的io.copy</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/io.copy/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/io.copy/</guid>
      <description>介绍 # 遇到一个从挂载目录copy文件慢的问题，从挂载目录直接鼠标拖动文件到其他地方很快，但使用函数进行copy却很慢。
这四种copy方式有什么区别？
io.Copy # io.Copy 是标准库中用于文件复制的简便方式，它会根据内部默认的缓冲区大小（通常较小，可能是 32KB ）进行读取和写入。由于 io.Copy 的缓冲区较小，它在大文件复制时可能需要更多的 I/O 操作次数，导致了较大的性能开销。
func CopyFile1(src string, dst string) (int64, error) { srcFile, err := os.Open(src) if err != nil { return -1, err } defer srcFile.Close() dstFile, err := os.OpenFile(dst, os.O_RDWR|os.O_CREATE|os.O_TRUNC, os.ModePerm) if err != nil { return -1, err } defer dstFile.Close() return io.Copy(dstFile, srcFile) } 点开io.Copy，发现里面调用copyBuffer，与下面要说的io.CopyBuffer函数入口一致。
func Copy(dst Writer, src Reader) (written int64, err error) { return copyBuffer(dst, src, nil) } io.</description>
    </item>
    
    <item>
      <title>常用业务代码</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%B8%B8%E7%94%A8%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E5%B8%B8%E7%94%A8%E4%B8%9A%E5%8A%A1%E4%BB%A3%E7%A0%81/</guid>
      <description>go语言在.csv文件中加入超链接 # func main() { // 打开文件以写入 CSV 数据 file, err := os.Create(&amp;#34;output.csv&amp;#34;) if err != nil { panic(err) } defer file.Close() // 创建 CSV writer writer := csv.NewWriter(file) defer writer.Flush() // 写入 CSV 头部 header := []string{&amp;#34;File Name&amp;#34;, &amp;#34;Hyperlink&amp;#34;} writer.Write(header) // 模拟一些文件名和相对路径数据 fileData := []struct { FileName string RelativePath string }{ {&amp;#34;video.MP4&amp;#34;, &amp;#34;./d/video.MP4&amp;#34;}, // 添加更多文件名和相对路径 } // 写入文件名和相对路径数据到 CSV 文件 for _, data := range fileData { // 构建超链接字符串 hyperlinkFormula := `=HYPERLINK(&amp;#34;` + data.</description>
    </item>
    
    <item>
      <title>并发编程</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/</guid>
      <description>sync.Pool # Go 并发相关库 sync 里面有一个有趣的 package Pool，sync.Pool 是个有趣的库，用很少的代码实现了很巧的功能。第一眼看到 Pool 这个名字，就让人想到池子，元素池化是常用的性能优化的手段（性能优化的几把斧头：并发，预处理，缓存）。比如，创建一个 100 个元素的池，然后就可以在池子里面直接获取到元素，免去了申请和初始化的流程，大大提高了性能。释放元素也是直接丢回池子而免去了真正释放元素带来的开销。
但是再仔细一看 sync.Pool 的实现，发现比我预期的还更有趣。sync.Pool 除了最常见的池化提升性能的思路，最重要的是减少 GC 。常用于一些对象实例创建昂贵的场景。注意，Pool 是 Goroutine 并发安全的。
对象池是在什么时候适合引入？
一个对象会被大量创建，比如高并发场景。 该场景是会被稳定触发的，而不是一次性的。也不能是间隔很久才触发一次。 初始化 Pool 实例 New # 第一个步骤就是创建一个 Pool 实例，关键一点是配置 New 方法，声明 Pool 元素创建的方法。
bufferpool := &amp;amp;sync.Pool { New: func() interface {} { println(&amp;#34;Create new instance&amp;#34;) return struct{}{} } } 申请对象 Get # buffer := bufferPool.Get()
Get 方法会返回 Pool 已经存在的对象，如果没有，那么就走慢路径，也就是调用初始化的时候定义的 New 方法（也就是最开始定义的初始化行为）来初始化一个对象。
释放对象 Put # bufferPool.Put(buffer)</description>
    </item>
    
    <item>
      <title>开发实例</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/%E5%BC%80%E5%8F%91%E5%AE%9E%E4%BE%8B/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/%E5%BC%80%E5%8F%91%E5%AE%9E%E4%BE%8B/</guid>
      <description>继续执行主线程 # from concurrent.futures import ThreadPoolExecutor # 使用 ThreadPoolExecutor 创建线程池 executor = ThreadPoolExecutor(max_workers=len(models) + 1) # 提交生产者任务到线程池 executor.submit( DetectionService._producer_frames, read_data, mode, interval_seconds, start_time, end_time, queue_manager ) # 提交消费者任务到线程池 for key, value in CV_MODEL.items(): with queue_manager.lock: q = queue_manager.queues.get(key) executor.submit( DetectionService._consumer_frames, q, mode, diff, name, key, value ) 主线程等待 # from concurrent.futures import ThreadPoolExecutor # 使用 ThreadPoolExecutor 创建线程池，并设置最大工作线程数为 4 with ThreadPoolExecutor(max_workers=4) as executor: # 提交生产者任务到线程池 executor.submit( DetectionService._producer_frames, read_data, mode, interval_seconds, start_time, end_time, queue_manager ) # 提交消费者任务到线程池 for key, value in CV_MODEL.</description>
    </item>
    
    <item>
      <title>微服务</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BE%AE%E6%9C%8D%E5%8A%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%BE%AE%E6%9C%8D%E5%8A%A1/%E5%BE%AE%E6%9C%8D%E5%8A%A1/</guid>
      <description>微服务 # 微服务框架是将复杂的系统使用组件化的方式进行拆分，并使用轻量级通讯方式进行整合的一种设计方法。
微服务是通过这种架构设计方法拆分出来的一个独立的组件化的小应用。
微服务架构和整体式架构的区别？ # 开发单体式（整体式）应用的不足之处 # 三层架构（MVC）的具体内容如下：
表示层（view）： 用户使用应用程序时，看到的、听见的、输入的或者交互的部分。
业务逻辑层（controller）： 根据用户输入的信息，进行逻辑计算或者业务处理的部分。
数据访问层（model）： 关注有效地操作原始数据的部分，如将数据存储到存储介质（如数据库、文件系统）及从存储介质中读取数据等。
虽然现在程序被分成了三层，但只是逻辑上的分层，并不是物理上的分层。也就是说，对不同层的代码而言，经过编译、打包和部署后，所有的代码最终还是运行在同一个进程中。而这，就是所谓的单块架构。
单体架构在规模比较小的情况下工作情况良好，但是随着系统规模的扩大，它暴露出来的问题也越来越多，主要有 以下几点：
复杂性逐渐变高
比如有的项目有几十万行代码，各个模块之间区别比较模糊，逻辑比较混乱，代码越多复杂性越高，越难解决遇到的问题。
技术债务逐渐上升
公司的人员流动是再正常不过的事情，有的员工在离职之前，疏于代码质量的自我管束，导致留下来很多坑，由于单体项目代码量庞大的惊人，留下的坑很难被发觉，这就给新来的员工带来很大的烦恼，人员流动越大所留下的坑 越多，也就是所谓的技术债务越来越多。
维护成本大
当应用程序的功能越来越多、团队越来越大时，沟通成本、管理成本显著增加。当出现 bug 时，可能引起 bug 的原因组合越来越多，导致分析、定位和修复的成本增加；并且在对全局功能缺乏深度理解的情况下，容易在修复 bug 时引入新的 bug。
持续交付周期长
构建和部署时间会随着功能的增多而增加，任何细微的修改都会触发部署流水线。新人培养周期长：新成员了解背景、熟悉业务和配置环境的时间越来越长。 技术选型成本高 单块架构倾向于采用统一的技术平台或方案来解决所有问题，如果后续想引入新的技术或框架，成本和风险都很 大。
可扩展性差
随着功能的增加，垂直扩展的成本将会越来越大；而对于水平扩展而言，因为所有代码都运行在同一个进程，没办法做到针对应用程序的部分功能做独立的扩展。
微服务架构的特性 # 单一职责
微服务架构中的每个服务，都是具有业务逻辑的，符合高内聚、低耦合原则以及单一职责原则的单元，不同的服务通过“管道”的方式灵活组合，从而构建出庞大的系统。
轻量级通信
服务之间通过轻量级的通信机制实现互通互联，而所谓的轻量级，通常指语言无关、平台无关的交互方式。
对于轻量级通信的格式而言，我们熟悉的 XML 和 JSON，它们是语言无关、平台无关的；对于通信的协议而言，通 常基于 HTTP，能让服务间的通信变得标准化、无状态化。目前大家熟悉的 REST（Representational State Transfer）是实现服务间互相协作的轻量级通信机制之一。使用轻量级通信机制，可以让团队选择更适合的语言、 工具或者平台来开发服务本身。
问：REST是什么和restful一样吗？
答：REST 指的是一组架构约束条件和原则。满足这些约束条件和原则的应用程序或设计就是 RESTful。
独立性
每个服务在应用交付过程中，独立地开发、测试和部署。
在单块架构中所有功能都在同一个代码库，功能的开发不具有独立性；当不同小组完成多个功能后，需要经过集成 和回归测试，测试过程也不具有独立性；当测试完成后，应用被构建成一个包，如果某个功能存在 bug，将导致整 个部署失败或者回滚。
在微服务架构中，每个服务都是独立的业务单元，与其他服务高度解耦，只需要改变当前服务本身，就可以完成独立的开发、测试和部署。
进程隔离
单块架构中，整个系统运行在同一个进程中，当应用进行部署时，必须停掉当前正在运行的应用，部署完成后再重启进程，无法做到独立部署。
有时候我们会将重复的代码抽取出来封装成组件，在单块架构中，组件通常的形态叫做共享库（如 jar 包或者 DLL），但是当程序运行时，所有组件最终也会被加载到同一进程中运行.</description>
    </item>
    
    <item>
      <title>数据库基础-牛客</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80-%E7%89%9B%E5%AE%A2/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80-%E7%89%9B%E5%AE%A2/</guid>
      <description>MySQL 的存储引擎有哪些？它们之间有什么区别？默认使用哪个？ # MySQL的存储引擎是用于存储、处理和数据保护的核心组件。
InnoDB 特点：支持事物、行级锁、外键约束、聚簇索引，不支持行数保存 优点：数据完整性高，适合高并发读写，崩溃后自动恢复 缺点：需要权衡其性能和资源消耗 应用场景：需要事务支持的应用 MyISAM 特点：不支持事务，外键、聚簇索引；使用表级锁，不支持行级锁，访问速度快；支持行数保存 优点：读操作性能高，占用空间小 缺点：数据完整性差，奔溃后恢复困难 应用场景：读多写少的场景 SQLite 与 InnoDB 数据库引擎的主要区别 # SQLite 和 InnoDB 都是流行的数据库引擎，但它们在设计理念、架构和应用场景上有显著差异：
1. 基本架构差异 # 特性 SQLite InnoDB 架构类型 嵌入式、无服务器 客户端-服务器(作为MySQL存储引擎) 进程模型 库形式直接链接到应用程序 作为MySQL服务器的一部分运行 部署方式 单文件数据库 多文件存储系统 2. 并发处理能力 # 特性 SQLite InnoDB 锁机制 数据库级锁(写独占) 行级锁 并发写 同一时间只允许一个写操作 支持多事务并发写入 隔离级别 默认SERIALIZABLE 支持多种隔离级别 3. 存储与性能 # 特性 SQLite InnoDB 存储格式 单个磁盘文件 表空间文件(ibd)系统 索引类型 B-tree B+tree 缓存 页面缓存 缓冲池(更复杂的内存管理) 全文搜索 需要FTS扩展 内置FULLTEXT索引支持 4.</description>
    </item>
    
    <item>
      <title>数据库规范化</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A7%84%E8%8C%83%E5%8C%96/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%A7%84%E8%8C%83%E5%8C%96/</guid>
      <description>数据库规范化 # 数据库规范化是设计数据库的方式，注重最小化数据重复和确保数据完整性。
优点：
最大限度地减少数据重复，从而最大限度地减少存储。 易于更新，确保数据完整性。 缺点：
可能会降低查询性能，尤其是对于需要连接表的查询。 用例：
数据完整性非常重要的系统，例如银行系统。</description>
    </item>
    
    <item>
      <title>数据库设计</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1/</guid>
      <description>数据库设计案例 # 案例实操
数据库规范化 # 数据库规范化是设计数据库的方式，注重最小化数据重复和确保数据完整性。
优点：
最大限度地减少数据重复，从而最大限度地减少存储。 易于更新，确保数据完整性。 缺点：
可能会降低查询性能，尤其是对于需要连接表的查询。 用例：
数据完整性非常重要的系统，例如银行系统。</description>
    </item>
    
    <item>
      <title>日常小脚本</title>
      <link>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/%E6%97%A5%E5%B8%B8%E5%B0%8F%E8%84%9A%E6%9C%AC/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/%E5%9F%BA%E7%A1%80/%E6%97%A5%E5%B8%B8%E5%B0%8F%E8%84%9A%E6%9C%AC/</guid>
      <description>继续执行主线程 # 抖音下载小视频 # import requests import os from tqdm import tqdm # 用于显示下载进度条 def download_douyin_video(video_url, output_filename=&amp;#34;douyin_video.mp4&amp;#34;): &amp;#34;&amp;#34;&amp;#34; 下载抖音视频，通过模拟浏览器User-Agent和Referer头绕过检查。 Args: video_url (str): 抖音视频的CDN链接。 output_filename (str): 保存视频的文件名。 &amp;#34;&amp;#34;&amp;#34; # 模拟一个常见的浏览器User-Agent headers = { &amp;#39;User-Agent&amp;#39;: &amp;#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36&amp;#39;, # 这是关键！模拟请求来源，告诉服务器请求来自抖音的网站。 # 你可以尝试 https://www.douyin.com/ 或者包含该视频的具体的抖音页面URL &amp;#39;Referer&amp;#39;: &amp;#39;https://www.douyin.com/&amp;#39;, &amp;#39;Accept&amp;#39;: &amp;#39;video/webm,video/ogg,video/*;q=0.9,application/ogg;q=0.7,audio/*;q=0.6,*/*;q=0.5&amp;#39;, &amp;#39;Accept-Encoding&amp;#39;: &amp;#39;gzip, deflate, br&amp;#39;, &amp;#39;Accept-Language&amp;#39;: &amp;#39;en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7&amp;#39;, &amp;#39;Connection&amp;#39;: &amp;#39;keep-alive&amp;#39;, } print(f&amp;#34;尝试下载视频从: {video_url}&amp;#34;) print(f&amp;#34;保存到文件: {output_filename}&amp;#34;) print(&amp;#34;正在使用的请求头：&amp;#34;) for key, value in headers.</description>
    </item>
    
    <item>
      <title>正则表达式</title>
      <link>https://chain-code.github.io/docs/python/package/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/python/package/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/</guid>
      <description>正则表达式 # 字符串是编程时涉及到的最多的一种数据结构，对字符串进行操作的需求几乎无处不在。比如判断一个字符串是否是合法的Email地址，虽然可以编程提取@前后的子串，再分别判断是否是单词和域名，但这样做不但麻烦，而且代码难以复用。
正则表达式是一种用来匹配字符串的强有力的武器。它的设计思想是用一种描述性的语言来给字符串定义一个规则，凡是符合规则的字符串，我们就认为它“匹配”了，否则，该字符串就是不合法的。
所以我们判断一个字符串是否是合法的Email的方法是：
创建一个匹配Email的正则表达式； 用该正则表达式去匹配用户的输入来判断是否合法。 因为正则表达式也是用字符串表示的，所以，我们要首先了解如何用字符来描述字符。
在正则表达式中，如果直接给出字符，就是精确匹配。用\d可以匹配一个数字，\w可以匹配一个字母或数字，所以：
&#39;00\d&#39;可以匹配&#39;007&#39;，但无法匹配&#39;00A&#39;； &#39;\d\d\d&#39;可以匹配&#39;010&#39;； &#39;\w\w\d&#39;可以匹配&#39;py3&#39;； .可以匹配任意字符，所以：
&#39;py.&#39;可以匹配&#39;pyc&#39;、&#39;pyo&#39;、&#39;py!&#39;等等。 要匹配变长的字符，在正则表达式中，用*表示任意个字符（包括0个），用+表示至少一个字符，用?表示0个或1个字符，用{n}表示n个字符，用{n,m}表示n-m个字符：
来看一个复杂的例子：\d{3}\s+\d{3,8}。
我们来从左到右解读一下：
\d{3}表示匹配3个数字，例如&#39;010&#39;； \s可以匹配一个空格（也包括Tab等空白符），所以\s+表示至少有一个空格，例如匹配&#39; &#39;，&#39; &#39;等； \d{3,8}表示3-8个数字，例如&#39;1234567&#39;。 综合起来，上面的正则表达式可以匹配以任意个空格隔开的带区号的电话号码。
如果要匹配&#39;010-12345&#39;这样的号码呢？由于&#39;-&#39;是特殊字符，在正则表达式中，要用&#39;\&#39;转义，所以，上面的正则是\d{3}\-\d{3,8}。
但是，仍然无法匹配&#39;010 - 12345&#39;，因为带有空格。所以我们需要更复杂的匹配方式。
进阶 # 要做更精确地匹配，可以用[]表示范围，比如：
[0-9a-zA-Z\_]可以匹配一个数字、字母或者下划线； [0-9a-zA-Z\_]+可以匹配至少由一个数字、字母或者下划线组成的字符串，比如&#39;a100&#39;，&#39;0_Z&#39;，&#39;Py3000&#39;等等； [a-zA-Z\_][0-9a-zA-Z\_]*可以匹配由字母或下划线开头，后接任意个由一个数字、字母或者下划线组成的字符串，也就是Python合法的变量； [a-zA-Z\_][0-9a-zA-Z\_]{0, 19}更精确地限制了变量的长度是1-20个字符（前面1个字符+后面最多19个字符）。 A|B可以匹配A或B，所以(P|p)ython可以匹配&#39;Python&#39;或者&#39;python&#39;。
^表示行的开头，^\d表示必须以数字开头。
$表示行的结束，\d$表示必须以数字结束。
你可能注意到了，py也可以匹配&#39;python&#39;，但是加上^py$就变成了整行匹配，就只能匹配&#39;py&#39;了。
re模块 # 有了准备知识，我们就可以在Python中使用正则表达式了。Python提供re模块，包含所有正则表达式的功能。由于Python的字符串本身也用\转义，所以要特别注意：
s = &amp;#39;ABC\\-001&amp;#39; # Python的字符串 # 对应的正则表达式字符串变成 &amp;#39;ABC\-001&amp;#39; 因此我们强烈建议使用Python的r前缀，就不用考虑转义的问题了：
s = r&amp;#39;ABC\-001&amp;#39; # Python的字符串 # 对应的正则表达式字符串不变：&amp;#39;ABC\-001&amp;#39; 先看看如何判断正则表达式是否匹配：
&amp;gt;&amp;gt;&amp;gt; import re &amp;gt;&amp;gt;&amp;gt; re.match(r&amp;#39;^\d{3}\-\d{3,8}$&amp;#39;, &amp;#39;010-12345&amp;#39;) &amp;lt;_sre.SRE_Match object; span=(0, 9), match=&amp;#39;010-12345&amp;#39;&amp;gt; &amp;gt;&amp;gt;&amp;gt; re.</description>
    </item>
    
    <item>
      <title>每日一题（一）</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E4%B8%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98%E4%B8%80/</guid>
      <description>简单 # 旅行终点站 # 给你一份旅游线路图，该线路图中的旅行线路用数组 paths 表示，其中 paths[i] = [cityAi, cityBi] 表示该线路将会从 cityAi 直接前往 cityBi 。请你找出这次旅行的终点站，即没有任何可以通往其他城市的线路的城市*。*
题目数据保证线路图会形成一条不存在循环的线路，因此恰有一个旅行终点站。
输入：paths = [[&amp;#34;B&amp;#34;,&amp;#34;C&amp;#34;],[&amp;#34;D&amp;#34;,&amp;#34;B&amp;#34;],[&amp;#34;C&amp;#34;,&amp;#34;A&amp;#34;]] 输出：&amp;#34;A&amp;#34; 解释：所有可能的线路是： &amp;#34;D&amp;#34; -&amp;gt; &amp;#34;B&amp;#34; -&amp;gt; &amp;#34;C&amp;#34; -&amp;gt; &amp;#34;A&amp;#34;. &amp;#34;B&amp;#34; -&amp;gt; &amp;#34;C&amp;#34; -&amp;gt; &amp;#34;A&amp;#34;. &amp;#34;C&amp;#34; -&amp;gt; &amp;#34;A&amp;#34;. &amp;#34;A&amp;#34;. 显然，旅行终点站是 &amp;#34;A&amp;#34; 。 func destCity(paths [][]string) string { //合并区间 for i, j := 0, 1; j &amp;lt; len(paths); { if len(paths) == 1 { break } if paths[i][1] == paths[j][0] { paths[i][1] = paths[j][1] paths = append(paths[:j], paths[j+1:].</description>
    </item>
    
    <item>
      <title>牛客八股</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E7%89%9B%E5%AE%A2%E5%85%AB%E8%82%A1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E7%89%9B%E5%AE%A2%E5%85%AB%E8%82%A1/</guid>
      <description>操作系统
计算机网络
HTTP/HTTPS TCP/UDP 网络模型 系统设计</description>
    </item>
    
    <item>
      <title>相关工具库</title>
      <link>https://chain-code.github.io/docs/ai/basic/%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%BA%93/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/%E7%9B%B8%E5%85%B3%E5%B7%A5%E5%85%B7%E5%BA%93/</guid>
      <description>工具集合库 # Awesome DeepSeek Integrations</description>
    </item>
    
    <item>
      <title>编程题</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E7%BC%96%E7%A8%8B%E9%A2%98/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E7%BC%96%E7%A8%8B%E9%A2%98/</guid>
      <description> 用队列实现一个栈 # 用栈实现一个队列 # 有一个包含N个整数的数组，请编写算法，找到其中的两个元素，使他们之差最小。时间复杂度必须为O(n) # 实现一个LRU缓存淘汰策略，支持get,和put操作 # 判断101-200之间有多少个质数，并输出所有的质数 # 给定一个二叉搜索树，请找出其中第K小的元素 # 并发调用三个方法，实现只要有一个成功就立即成功，否则等都失败才失败 # 两个线程，一个打印奇数，一个打印偶数，然后顺序打印出1-100 # ABC 三个协程，A输出a，B输出b，C输出c，怎么让他们按abc这样循环往复按顺序出现？ # AB两个长度为N的有序数组,寻找第N和N+1的数 # 实现一个停车场的类，满足入场停车、出场收费的需求，车位有三种大中小分别对应不同停车费/小时，不足一小时按一小时算。 # 10个线程模拟赛马，所有马都就绪后才能开始，所有马到达终点后裁判宣布赛马成绩 # 5个线程abide,想先执行a，再执行bcd，bcd执行完后执行e如何做？ # </description>
    </item>
    
    <item>
      <title>获取内网活跃IP</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%8E%B7%E5%8F%96%E5%86%85%E7%BD%91%E6%B4%BB%E8%B7%83ip/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%8E%B7%E5%8F%96%E5%86%85%E7%BD%91%E6%B4%BB%E8%B7%83ip/</guid>
      <description>获取内网活跃IP # https://rogerzhu.gitbooks.io/-tcp-udp-ip/content/chapter1/arp-lian-jie-mac-he-ip.html
内网广播ARP Request # ARP（Address Resolution Protocol），地址解析协议，是根据IP地址获取物理地址的一个TCP/IP协议。主机发送信息时将包含目标IP地址的ARP请求广播到网络上的所有主机，并接收返回信息，以此确认目标的物理地址。
当我们要向以太网中另一台主机发送IP数据时，我们本地会根据目的主机的IP地址在ARP高速缓存中查询相应的以太网地址，ARP高速缓存是主机维护的一个IP地址到相应以太网地址的映射表。如果查询失败，ARP会广播一个询问（op字段为1）目的主机硬件地址的报文，等待目标主机的响应。 因为ARP高速缓存有时效性，读取到目标主机的硬件地址后，最好发送一个ICMP包验证目标是否在线。当然也可以选择不从高速缓存里读取数据，而是直接并发发送arp包，等待在线主机回应ARP报文。
通过内网IP和子网掩码计算内网IP范围 # // 获取所有网卡 func Test_Net(t *testing.T) { // 获取所有网卡接口 interfaces, err := net.Interfaces() if err != nil { fmt.Println(&amp;#34;Error:&amp;#34;, err) return } for _, iface := range interfaces { fmt.Printf(&amp;#34;Name: %s\n&amp;#34;, iface.Name) fmt.Printf(&amp;#34;MTU: %d\n&amp;#34;, iface.MTU) fmt.Printf(&amp;#34;HardwareAddr: %s\n&amp;#34;, iface.HardwareAddr) fmt.Printf(&amp;#34;Flags: %s\n&amp;#34;, iface.Flags) // 获取每个接口的地址信息 addrs, err := iface.Addrs() if err != nil { fmt.Println(&amp;#34;Error:&amp;#34;, err) continue } for i, addr := range addrs { fmt.</description>
    </item>
    
    <item>
      <title>视频超分</title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/%E8%A7%86%E9%A2%91%E8%B6%85%E5%88%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/%E8%A7%86%E9%A2%91%E8%B6%85%E5%88%86/</guid>
      <description>https://github.com/xinntao/ESRGAN?tab=readme-ov-file
主打动漫类，最新代码7年前提交的，淘汰
https://github.com/xinntao/Real-ESRGAN?tab=readme-ov-file#online-inference ****
也是主打动漫类，star 30.3万
文件保存在Real-ESRGAN
https://github.com/megvii-research/NAFNet?tab=readme-ov-file ****
图片处理效果还行，备选 目前仅支持图像增强 文件保存在NAFNet
https://github.com/JingyunLiang/SwinIR ****
文件保存在SwinIR
https://github.com/DmitryUlyanov/deep-image-prior
六七年前的老项目，淘汰
https://github.com/cszn/BSRGAN *****
文件保存在BSRGAN 感觉还行
https://github.com/open-mmlab/mmagic?tab=readme-ov-file
多个图片、视频处理的集成库 代码更新到2023年12月18日
https://github.com/XPixelGroup/BasicSR
继承库，但是代码更新到2022年8月31日
https://github.com/ckkelvinchan/RealBasicVSR
支持视频，也支持图片，具体要测
https://github.com/TencentARC/GFPGAN
注重人脸超分，仅能运行在Linux
https://github.com/Fanghua-Yu/SUPIR
许可协议只限个人使用 不让商用，可以借鉴里面的东西
https://github.com/sczhou/CodeFormer
许可协议 S-Lab 许可证 1.0 不让商用
https://github.com/upscayl/upscayl
底层用的 Real-ESRGAN 淘汰
https://github.com/XPixelGroup/DiffBIR
测试效果不理想，而且速度贼慢
https://github.com/philz1337x/clarity-upscaler/
AI 图像升级器和增强器 但是通过ai 变了原有图画信息，与需求不符，淘汰
https://github.com/AaronFeng753/Waifu2x-Extension-GUI
主要使用机器学习进行照片/视频/GIF 放大和视频帧插值 淘汰
https://github.com/ohayonguy/PMRF
注重于人脸处理，可以先放着，后续加 ***
https://github.com/k4yt3x/video2x
视频的暂时不看</description>
    </item>
    
    <item>
      <title>通过子网掩码计算IP地址范围</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E9%80%9A%E8%BF%87%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%E8%AE%A1%E7%AE%97ip%E5%9C%B0%E5%9D%80%E8%8C%83%E5%9B%B4/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E9%80%9A%E8%BF%87%E5%AD%90%E7%BD%91%E6%8E%A9%E7%A0%81%E8%AE%A1%E7%AE%97ip%E5%9C%B0%E5%9D%80%E8%8C%83%E5%9B%B4/</guid>
      <description>前言 # 在因特网中，计算机与计算机之间的通信都是通过网络来完成的，那么他们直接是如何完成通信的呢？大多数人都知道，计算机通信使用的是当前最流行的Internet分组交换传输协议，即TCP/IP的协议簇或者它的的变种。
在使用TCP/IP进行通信的时候，我们经常会使用到网段和子网掩码，子网掩码用来区分IP地址的网络地址和主机地址，相同网络号地址的IP发包情况是不同的。同一个网络发包可以通过相关的协议把数据包直接发送到目标主机，而不同网络的则会通过路由器发包。划分一个合适的子网是重要的，过少的主机数目可能无法满足你的要求，而过多的主机数目无疑会导致局域网访问量过大，频繁，会影响通信效率。
IP网段 # 通常IP网段分为四种：
A类IP段 0.0.0.0 到 127.255.255.255 即首位为‘0’的IP地址。 B类IP段 128.0.0.0 到 191.255.255.255 即首位为‘10’的IP地址。 C类IP段 192.0.0.0 到 223.255.255.255 即首位为‘110’的IP地址。 D类IP段 224.0.0.0 到 239.255.255.255 即首位为‘1110’的IP地址。 一个A类的默认子网掩码是 255.0.0.0 ，即一个子网最多可以容纳1677万多台电脑，B类是 255.255.0.0，默认最多可以容纳6万台电脑，C类是255.255.255.0，默认最多可以容纳254台电脑。
如何分辨IP的网络和主机号，我们先来看一个IP的例子，192.168.0.1/24，这个IP的网络号和主机号是多少，可以容纳的主机数目怎么计算，接下来我们一起来看一下。
子网掩码计算 # 通过IP地址(192.168.0.1)换算成二进制为11000000.10101000.00000000.00000001，24表示子网掩码为24位，即二进制为11111111.11111111.11111100.00000000的数字。
网络号通过IP地址与子网掩码的按位与可以得到11000000.10101000.00000000.00000000，即192.168.0.0,显然，IP地址的主机号为00000001，那它可以容纳的主机数目是多少呢？这里有个简便的方法计算，即看子网掩码0的个数，这里是10，即可以容纳的主机数目是2的10次方，也就是最多可以容纳1024台主机。
问题： 计算网段 172.16.0.0/23 的IP地址段是多少到多少？
解答： 1、由题可得起始IP地址为：172.16.0.1 2、其中23为子网掩码用“位数”的简写方式，意思是子网掩码的二进制为从左到右23个1组成的二进制 11111111.11111111.11111110.00000000，转换为十进制结果为255.255.254.0，并得出右侧为0的有9位可以表示主机段 3、计算广播地址：按如下方法将IP地址段和子网掩码的二进制格式对齐进行计算，垂直都是1的得1否则得0，然后将右侧9位0全部设置为1，如下所示
10101100-00010000-00000000-0000000011111111-11111111-11111110-00000000-----------------------------------10101100-00010000-00000001-11111111 4、将计算结果转换为十进制，得出广播地址为172.16.1.255 5、由此可以得出本题IP地址段的范围是 172.16.0.1 至 172.16.1.254 6、可用IP数量数速算为2的9次方减2=510
代码 # import ( &amp;#34;errors&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;net&amp;#34; &amp;#34;sync&amp;#34; &amp;#34;time&amp;#34; &amp;#34;vmodel/network&amp;#34; param2 &amp;#34;vmodel/param&amp;#34; probing &amp;#34;github.com/prometheus-community/pro-bing&amp;#34; ) func Test_GetNetworkList(t *testing.</description>
    </item>
    
    <item>
      <title>配置 Kylin V10</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E9%85%8D%E7%BD%AEkylinv10/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E9%85%8D%E7%BD%AEkylinv10/</guid>
      <description>配置KylinV10 # 设置“root”登录密码 # sudo su -passwd# 设置登录密码 允许“root”远程登录 # sudo vim /etc/ssh/sshd_config # ↓↓↓↓修改的内容↓↓↓↓PermitRootLogin yes# ↑↑↑↑修改的内容↑↑↑↑ sudo systemctl restart sshd 允许通过图像界面登录到“root” # sudo vim /usr/share/lightdm/lightdm.conf.d/95-ukui-greeter.conf 95-ukui-greeter.conf
greeter-session=ukui-greeteruser-session=ukuigreeter-setup-script=/usr/lib/ukui-greeter/ukui-greeter-nm-start.sh# ↓↓↓↓追加的内容↓↓↓↓allow-guest=falsegreeter-show-manual-login=true# ↑↑↑↑追加的内容↑↑↑↑ 开机自动登录到“root” # sudo vim /etc/lightdm/lightdm.conf lightdm.conf
[SeatDefaults]autologin-guest=false# ↓↓↓↓修改的内容↓↓↓↓autologin-user=root# ↑↑↑↑修改的内容↑↑↑↑autologin-user-timeout=0 关闭“麒麟安全授权认证” # sudo vim /etc/default/grub grub
# ...GRUB_DEFAULT=0GRUB_TIMEOUT=5GRUB_DISTRIBUTOR=`lsb_release -i -s 2&amp;gt; /dev/null || echo Debian`GRUB_DISTRIBUTOR_RELEASE=`lsb_release -d -s | awk -F&amp;#34; &amp;#34; &amp;#39;{print $2 &amp;#34; &amp;#34; $3}&amp;#39; 2&amp;gt; /dev/null || echo &amp;#34;&amp;#34;`GRUB_CMDLINE_LINUX_DEFAULT=&amp;#34;quiet splash&amp;#34;GRUB_CMDLINE_LINUX=&amp;#34;&amp;#34;# ↓↓↓↓修改的内容↓↓↓↓# GRUB_CMDLINE_LINUX_SECURITY=&amp;#34;audit=0 security=kysec&amp;#34;GRUB_CMDLINE_LINUX_SECURITY=&amp;#34;audit=0 security=&amp;#34;# ↑↑↑↑修改的内容↑↑↑↑# .</description>
    </item>
    
    <item>
      <title>面视题总结（一）</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E8%A7%86%E9%A2%98%E6%80%BB%E7%BB%93%E4%B8%80/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E8%A7%86%E9%A2%98%E6%80%BB%E7%BB%93%E4%B8%80/</guid>
      <description>新⼿常犯的50个错误 # https://blog.csdn.net/gezhonglei2007/article/details/52237582
nil切⽚和空切⽚ # nil切片和空切片指向的地址不一样。nil空切片引用数组指针地址为0（无指向任何实际地址） 空切片的引用数组指针地址是有的，且固定为一个值 //切片数据结构 type SliceHeader struct { Data uintptr //引用数组指针地址 Len int // 切片的目前使用长度 Cap int // 切片的容量 } 字符串转成byte数组，会发⽣内存拷⻉吗？ # 字符串转成切片，会产生拷贝。严格来说，只要是发生类型强转都会发生内存拷贝。
有没有什么办法可以在字符串转成切片的时候不用发生拷贝呢？
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;reflect&amp;#34; &amp;#34;unsafe&amp;#34; ) func main() { a :=&amp;#34;aaa&amp;#34; ssh := *(*reflect.StringHeader)(unsafe.Pointer(&amp;amp;a)) b := *(*[]byte)(unsafe.Pointer(&amp;amp;ssh)) fmt.Printf(&amp;#34;%v&amp;#34;,b) } 解释：
StringHeader 是字符串在go的底层结构。 type StringHeader struct { Data uintptr Len int } SliceHeader 是切片在go的底层结构。 type SliceHeader struct { Data uintptr Len int Cap int } 那么如果想要在底层转换二者，只需要把 StringHeader 的地址强转成 SliceHeader就行。那么go有个很强的包叫 unsafe 。 1.</description>
    </item>
    
    <item>
      <title>面视题总结（二）</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E8%A7%86%E9%A2%98%E6%80%BB%E7%BB%93%E4%BA%8C/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E9%9D%A2%E8%A7%86%E9%A2%98%E6%80%BB%E7%BB%93%E4%BA%8C/</guid>
      <description>uintptr和unsafe.Pointer的区别 # unsafe.Pointer只是单纯的通用指针类型，用于转换不同类型指针，它不可以参与指针运算； 而uintptr是用于指针运算的，GC 不把 uintptr 当指针，也就是说 uintptr 无法持有对象， uintptr 类型的目标会被回收； unsafe.Pointer 可以和 普通指针 进行相互转换； unsafe.Pointer 可以和 uintptr 进行相互转换。 举例
通过一个例子加深理解，接下来尝试用指针的方式给结构体赋值。 package main import ( &amp;#34;fmt&amp;#34; &amp;#34;unsafe&amp;#34; ) type W struct { b int32 c int64 } func main() { var w *W = new(W) //这时w的变量打印出来都是默认值0，0 fmt.Println(w.b,w.c) //现在我们通过指针运算给b变量赋值为10 b := unsafe.Pointer(uintptr(unsafe.Pointer(w)) + unsafe.Offsetof(w.b)) *((*int)(b)) = 10 //此时结果就变成了10，0 fmt.Println(w.b,w.c) } uintptr(unsafe.Pointer(w)) 获取了 w 的指针起始值 unsafe.Offsetof(w.b) 获取 b 变量的偏移量 两个相加就得到了 b 的地址值，将通用指针 Pointer 转换成具体指针 ((*int)(b))，通过 * 符号取值，然后赋值。*((*int)(b)) 相当于把 (*int)(b) 转换成 int了，最后对变量重新赋值成 10，这样指针运算就完成了。 怎么避免内存逃逸？ # 内存逃逸（Escape Analysis） 是指编译器在编译阶段分析变量的生命周期，决定将其分配在栈（stack）还是堆（heap）上。如果变量逃逸到堆上，会增加 GC 压力，降低性能。</description>
    </item>
    
    <item>
      <title>项目收藏</title>
      <link>https://chain-code.github.io/docs/ai/basic/%E9%A1%B9%E7%9B%AE%E6%94%B6%E8%97%8F/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/%E9%A1%B9%E7%9B%AE%E6%94%B6%E8%97%8F/</guid>
      <description>https://github.com/Yuliang-Liu/MonkeyOCR
系统性的 LLM 学习教程</description>
    </item>
    
    <item>
      <title>高并发高可用</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%AB%98%E5%B9%B6%E5%8F%91%E9%AB%98%E5%8F%AF%E7%94%A8/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E5%88%86%E5%B8%83%E5%BC%8F/%E9%AB%98%E5%B9%B6%E5%8F%91%E9%AB%98%E5%8F%AF%E7%94%A8/</guid>
      <description>如何设计一个支持高并发的系统？ # 设计一个能够支持高并发的系统需要考虑多方面的因素，包括架构、性能优化、容错和可伸缩性等。以下是一些一般性的建议和实践。
分布式架构：将系统分解成多个模块，采用分布式架构来降低单点故障的风险，并提高系统的可伸缩性和性能。
集群部署：将一个服务通过集群进行部署，来提升系统整体的吞吐量及响应速度，并使用负载均衡技术将请求均衡分配给多个服务器，以提高系统的性能和可用性。
利用缓存：使用缓存、NoSQL等技术，以提高数据读写的性能和可靠性。
异步处理：采用异步处理机制，如使用消息队列、事件驱动等技术，以降低请求响应时间和提高系统吞吐量。
预加载：使用预加载技术来提前加载需要的资源，以减少用户等待时间。
代码优化和调优：对系统代码进行优化和调优，如采用异步I/O、避免锁（减小锁的粒度）、减少循环和递归、避免长事务等，以提高系统性能。
数据库优化：合理的数据库设计和优化，包括合理的索引设计、分库分表、读写分离、缓存优化等，可以有效提高系统的并发度和响应速度。
读写分离：读写分离是一种常用的数据库优化技术，它将读操作和写操作分配到不同的数据库实例上处理。通过读写分离，主库主要负责写操作，从库则负责读操作，从而提高了系统的并发度和可扩展性。同时，读写分离还可以提高系统的可用性和容错能力，因为即使主库出现故障，从库仍然可以提供读服务。
防止雪崩：通过使用限流、熔断、降级等技术，可以防止系统因为某个组件出现故障而导致整个系统崩溃的雪崩效应。
容错和监控：实现容错机制，如备份、容灾、负载降级等，以保障系统的可用性。同时，使用监控工具来实时监测系统的运行状况和性能瓶颈，及时做出调整和优化。
测试和评估：进行全面的性能测试和评估，包括压力测试、负载测试、安全测试等，以发现并解决系统的性能瓶颈和安全隐患。
什么是服务降级？ # 限流和降级都是对系统的保护功能，一般用户在流量高峰时期，比如双十一大促。
降级是通过开关配置将某些不重要的业务功能屏蔽掉，以提高服务处理能力。在大促场景中经常会对某些服务进行降级处理，大促结束之后再进行复原。
区别于熔断机制，降级一般并不是彻底功能不可用，而是用一种默认返回、异步执行、延迟处理等方式进行降低处理。
比如：在大促期间减少反馈问卷等功能推送
降级方式 # 异步处理：先让用户填手机号，等用户离店后，再短信推送调查问卷
延迟处理：门口放一个问卷表，用户离店时自愿去填写。
什么是熔断？ # 现在很多网站的背后都是一个庞大的分布式系统，多个系统之间的交互大多数都是采用RPC的方式，但是因为是远程调用，所以被调用者的服务的可用情况其实是不可控的。
而越是庞大的系统，上下游的调用链就会越长，而如果在一个很长的调用链中，某一个服务由于某种原因导致响应时间很长，或者完全无响应，那么就可能把整个分布式系统都拖垮。
如果其中某一个服务由于自身原因导致响应很慢，那么就可能导致上游的服务相应也很慢，这样循环往复，就会导致整个系统全线崩溃，这就是服务雪崩。
在服务的依赖调用中，当被调用方出现故障时，出于自我保护的目的，调用方会主动停止调用，并根据业务需要进行相应处理。调用方这种主动停止调用的行为我们称之为熔断。
为什么需要熔断 # 其实，在分布式系统中，为了保证整体服务可用性和一致性，很多系统都会引入重试机制，在有些情况下，重试其实是可以解决问题的，比如网络问题等，都可以通过重试来解决。
但是，有些情况下，重试并不能解决问题，反而会加剧问题的严重性，比如下游系统因为请求量太大，导致CPU已经被打满，数据库连接池被占满，这时候上游系统调不通就会不断进行重试，这种重试请求，对于下游系统来说，无疑是雪上加霜，给下游系统造成二次伤害。
一个比较完善的熔断器，一般包含三种状态：
关闭：熔断器在默认情况下下是呈现关闭的状态，而熔断器本身带有计数功能，每当错误发生一次，计数器也就会进行“累加”的动作，到了一定的错误发生次数断路器就会被“开启”，这个时候亦会在内部启用一个计时器，一旦时间到了就会切换成半开启的状态。 开启：在开启的状态下任何请求都会“直接”被拒绝并且抛出异常讯息。 半开启：在此状态下断路器会允许部分的请求，如果这些请求都能成功通过，那么就意味着错误已经不存在，则会被切换回关闭状态并重置计数。倘若请求中有“任一”的错误发生，则会恢复到“开启”状态，并且重新计时，给予系统一段休息时间。 什么是预热？有何作用？ # 缓存预热是指在系统启动之前或系统达到高峰期之前，通过预先将常用数据加载到缓存中，以提高缓存命中率和系统性能的过程。缓存预热的目的是尽可能地避免缓存击穿和缓存雪崩，还可以减轻后端存储系统的负载，提高系统的响应速度和吞吐量。
减少冷启动影响：当系统重启或新启动时，缓存是空的，这被称为冷启动。冷启动可能导致首次请求处理缓慢，因为数据需要从慢速存储（如数据库）检索。 提高数据访问速度：通过预先加载常用数据到缓存中，可以确保数据快速可用，从而加快数据访问速度。 平滑流量峰值：在流量高峰期之前预热缓存可以帮助系统更好地处理高流量，避免在流量激增时出现性能下降。 保证数据的时效性：定期预热可以保证缓存中的数据是最新的，特别是对于高度依赖于实时数据的系统。 减少对后端系统的压力：通过缓存预热，可以减少对数据库或其他后端服务的直接查询，从而减轻它们的负载。 什么是限流？常见的限流算法有哪些？ # 漏桶算法（常用）：系统请求先进入漏桶，再从漏桶中逐一取出请求执行，控制漏桶的流量。 令牌桶算法（常用）：系统请求会得到一个令牌，从令牌桶中取出一个令牌执行，控制令牌桶中令牌的数量。 计数器算法（简单）：系统请求被计数，通过比较当前请求数与限流阈值来判断是否限流。 可以阻塞算法：当系统达到限流阈值时，不再接受新请求，等到限流阈值降下来再接受请求。 令牌环算法：与令牌桶算法类似，但是在多个令牌桶之间形成环形结构，以便在不同的请求处理速率之间进行平衡。 最小延迟算法：基于预测每个请求的处理时间，并在处理完请求后进行延迟，以控制请求的速率。 滑动窗口（常用）：基于一个固定大小的时间窗口，允许在该时间窗口内的请求数不超过设定的阈值。这个时间窗口随着时间的推移不断滑动，以适应不同时间段内的请求流量。 单机限流和集群限流的区别是什么？ # 一个是针对单台服务器限流，一个是针对整个集群做限流。比如单机限100，那么就是单机最大就能抗100QPS，如果是集群限100，那么就意味着集群不管有多少台机器，总共 QPS 只能抗100。
集群限流通常需要使用分布式的限流算法和工具，比如 Redis、Sentinel、Hystrix等，以确保每个服务实例或节点都遵守全局的流量控制策略。
有了集群限流，还需要做单机限流吗？ # 需要
因为单机限流主要关注保护单个服务节点。即使集群级别有流量控制，单个实例依然可能因为本地的请求过多而出现性能问题或崩溃。
有了单机限流，还需要做集群限流吗？ # 一般来说也需要</description>
    </item>
    
    <item>
      <title>Go避坑指南</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%81%BF%E5%9D%91%E6%8C%87%E5%8D%97/</guid>
      <description>问题1：字符串序列化后，&amp;amp;符号 转成 “\u0026“ # 问题描述： # 从CGO拿到的字符串，序列化后存入数据库后，&amp;amp; &amp;lt; &amp;gt; 符号变成了类似 &amp;ldquo;\u0026&amp;quot;的形式，但编译器、界面等其他地方看到的确实原始的&amp;amp; &amp;lt; &amp;gt; 符号。
解决方案： # 数据结构中的值 带有 &amp;amp; &amp;gt; &amp;lt; 等符号，当我们要将 struct map 转成json时，使用
json.Marshal() 函数，此函数会将 值中的 &amp;amp; &amp;lt; &amp;gt; 符号转义 为 类似 &amp;ldquo;\u0026&amp;rdquo;
parm := make(map[string]string) parm[&amp;#34;path&amp;#34;] = &amp;#34;http://baidu.com?a=djflks&amp;amp;b=1231131&amp;#34; //转成json 不转义特殊字符 这段替换原来的json.marshal bf := bytes.NewBuffer([]byte{}) jsonEncoder := json.NewEncoder(bf) jsonEncoder.SetEscapeHTML(false) jsonEncoder.Encode(parm) fmt.Println(bf.String()) 问题2：IDEA大面积报红，但编译能通过 # 问题描述： # 在使用IDEA编译代码的伙伴，偶合会发现个别函数报红，鼠标悬停显示函数未声明或者找不到等等，但是go build 却可以通过。
解决方案： # IEDA缓存混乱的问题，点击 文件-&amp;gt;修复IDE-&amp;gt;
根据右下角提示，一路点到底，重新建立索引就好了。
问题4：CGO不支持在函数参数列表中使用默认参数 # 问题描述： # 使用CGO调用别人的动态库时，经常出现这种问题：</description>
    </item>
    
    <item>
      <title>Go高阶 语言类库</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E8%AF%AD%E8%A8%80%E7%B1%BB%E5%BA%93/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E8%AF%AD%E8%A8%80%E7%B1%BB%E5%BA%93/</guid>
      <description> unsafe # 利用unsafe包修改私有成员 # 利用unsafe获取slice和map的长度 # 实现字符串和byte切片的零复制转换 # context # 译作“上下文”，准确说它是goroutine的上下文。主要用来在goroutine之间传递上下文信息，包括：取消信号、超时时间、截止时间、k-v等。
使用context几乎成为并发控制和超时控制的标准做法，与它协作的API都可以由外部控制执行“取消”操作，例如：取消一个HTTP请求的执行。
另外，context.Context可以协调多个goroutine中的代码执行“取消”操作，并且可以存储键值对，最重要的是它是并发安全的操作。
在Go的server里，对每个Request(请求)都会启动若干个goroutine同时工作：有些去内存查一些数据，有些去数据库拿数据，有些调用第三方接口获取相关数据等。
这些goroutine需要共享请求的基本信息：例如登陆token，处理请求的最大超时时间（如果超过此值再返回数据，请求方会因为超时接收不到）等。当请求被取消或是处理时间太长，这有可能是使用者关闭了浏览器或是已经超过了请求方规定的超时时间，请求方直接放弃了这次请求结果。这时，所有正在为这个请求工作的goroutine需要快速退出，因为它们的“工作成果”不再被需要了。
**Go语言中的server实际上是一个“协程模型”，处理一个请求需要多个协程。**例如在业务的高峰期，某个下游服务器的响应速度变慢，而当前系统的请求又没有超时控制，或者超过时间设置过大，那么等待下游服务器返回数据的协程就会越来越多。而协程师要消耗资源的，后果就是协程数激增，内存占用飙涨，Go调度器和GC不堪重用，甚至导致服务不可用。更严重的会导致雪崩效应，整个服务器对外表现为不可用，这肯定是P0级别的事故。
其实前面描述的P0级别的事故，通过设置“允许下游最长处理时间”就可以避免。例如，给下游设置timeout是50ms，如果超过这个值还没有接收到返回数据，就直接向客户端返回一个默认值或者错误。例如返回商品的一个默认库数量。注意，这里设置的超时时间和创建一个HTTP client设置的读写超时时间不一样，后者表示一次TCP传输的时间，而一次请求可能包含多次TCP传输，前者则表示所有传输的总时间。
而context包就是为了解决上面所说的问题开发的：在一组goroutine之间传递共享的值、取消信号、deadline等。
在Go里，不能直接杀死协程，协程的关闭一般采用channel和select的方式来控制。但是在某些场景下，例如处理一个请求衍生了很多协程，这些协程之间是相互关联的：需要共享一些全局变量、有共同的deadline等，而且可以同时被关闭。用channel和select就会比较麻烦，这时可以通过context来实现。
context用来解决goroutine之间退出通知、元数据传递的功能问题。
context会在函数中间传递，只需要在适当的时间调用Cancel函数向goroutine发出取消信号或者调用Value函数取出context中的值。
对使用context的几点建议：
不要将context塞到结构体里。直接将context类型作为函数的第一参数，而且一般都命名为ctx。 不要向函数传入一个含有nil属性的context，如果实在不知道传什么，标准库准备好了一个context：todo。 不要把本应该作为函数参数的类型塞到context中，context存储的应该是一些共同的数据。例如，登陆的session、cookie等。 同一个context可能会传递到多个groutine，但别担心，context是并发安全的。 如何使用context # 传递共享的数据 # 定时取消 # 防止goroutine泄漏 # context底层原理 # error # 计时器 # 反射 # 反射是指计算机程序在运行时可以访问、检测和修改它本身状态或行为的一种能力。
用比喻来说，反射就是程序在运行的时候能够观察并纠正自己的行为。
Go语言提供了一种机制在运行时更新变量和检查它们的值、调用它们的方法，但是在编译时并不知道这些变量的具体类型，这称为反射机制。
使用场景 # 不能明确接口调用那个函数，需要根据传入的参数在运行时决定。 不能明确传入参数的参数类型，需要在运行时处理任意对象。 不推荐使用原因 # 与反射相关的代码，难以阅读。 编译器无法提前发现一些类型错误，可能会运行很久后才会出错，会造成严重后果。 反射影响程序性能，比正常代码运行速度慢一到两个数量级。 </description>
    </item>
    
    <item>
      <title>结构型设计模式</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%BB%93%E6%9E%84%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid>
      <description>结构型设计模式 # 组合模式 # 介绍 # 组合模式是指将一组相似的对象当作一个单一对象的设计模式。
组合模式描述了一组对象，这些对象被视为相同类型对象的单个实例。组合模式可以将对象组合成树形结构，从而表示部分或整体的层次结构。
组合模式允许开发者拥有一个树形结构，并且要求树形结构中的每个节点都执行一项任务。组合模式的主要功能是在整个树形结构中递归调用方法并对结果进行汇总。
使用场景：
当客户需要忽略组合对象和单个对象之间的差异时。如果开发者以相同的方式使用多个对象，并且用几乎相同的代码处理每个对象。 如果需要实现树形结构。只需要通过请求树的顶层对象，就可以对整棵树进行统一操作。在组合模式中，添加和删除数的节点非常方便，并且遵循开闭原则。 如果开发者希望客户端可以以统一的方式处理简单或复杂的元素。 接口隔离原则要求开发者尽量将臃肿庞大的接口拆分成更小、更具体的接口，使接口中只包含客户端感兴趣的方法。
// 组件接口 type Component interface { Execute() } // 叶节点，用于描述层次结构中的原始叶节点对象 type Leaf struct { value int } // 创建一个新的叶节点对象 func NewLeaf(value int) *Leaf { return &amp;amp;Leaf{value} } // 打印叶节点对象的值 func (l *Leaf) Execute() { fmt.Printf(&amp;#34;%v &amp;#34;, l.value) } 定义组件类，用于表示复杂元素。该数组必须能同时存储叶节点和组合，因此需要确保将其声明为组件接口类型。在实现组件接口中的方法时，组合应该将大部分工作交给其子元素完成。
// 组件的组合 type Composite struct { children []Component } // 创建一个新的组合对象 func NewComposite() *Composite { return &amp;amp;Composite{make([]Component, 0)} } // 将一个新组件添加到组合中 func (c *Composite) Add(component Component) { //传入就将结构体赋值给接口 c.</description>
    </item>
    
    <item>
      <title>计算机网络基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 16 Sep 2022 10:03:35 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/</guid>
      <description>BS架构和CS架构 # CS（Client/Server）：客户端&amp;mdash;-服务器结构。CS结构在技术上很成熟，它的主要特点是交互性强、具有安全的存取模式、网络通信量低、相应速度快、利于处理大量数据。因为客户端要负责绝大多数的业务逻辑和UI展示，又称为胖客户端。它充分利用两端硬件，将任务分配到Client和Server两端，降低了系统的通讯开销。
CS架构是一种典型的两层架构，其客户端包含一个或多个在用户电脑上运行的程序，而服务端游两种，一种是数据库服务器端，客户端通过数据库连接访问服务器端的数据；另一种是Socket服务器端，服务器端的程序通过Socket与客户端的程序通信。
BS（Browser/Server）：浏览器&amp;mdash;-服务器结构，是目前应用系统的发展方向。BS是伴随着Internet技术的兴起，对CS架构的改进，为了区别于传统的CS 模式，特意称为BS模式。在这种结构下，通过浏览器来进入工作界面，极少部分事务逻辑在前端（Browser）实现，主要事务逻辑在服务器端（Server）实现，形成三层结构。这样使得客户端电脑负荷大大简化（因此被称为瘦客户端），减轻了系统维护、升级的支出成本，降低了用户的总体成本（TCO）。 BS的主要特点是分布性强、维护方便、开发简单且共享性强、总体拥有成本低。但存在数据安全性问题、对服务器要求过高、数据传输速度慢、软件的个性化特点明显降低，难以实现传统模式下的特殊功能要求。它是瘦客户端，对大量的数据输入以及报表的应答等都需要通过浏览器与服务器进行交互，通信开销大，而且对于实现复杂的应用构造有较大的困难。
小结：CS响应速度快，安全性强，一般应用于局域网中，但是开发维护成本高；BS可以实现跨平台，客户端零维护，但是个性化能力低，响应速度较慢。所以有些单位日常办公应用BS，在实际生产中使用CS结构。
HTTP # HTTP（HyperText Transfer Protocol）是超文本传输协议
HTT报文结构 # 请求行 # 请求行的格式为：Method Request-URI HTTP-version CRLF
method为大写，有以下几种：GET、POST、HEAD、OPTIONS、PUT、DELETE
Request-URI是一个统一资源标识符
HTTP-version为请求的HTTP的协议版本
请求头 # 请求头的格式为键值对。一般常见的请求头如下：
User-Agent:PostmanRuntime/7.26.8 表示产生请求的客户端程序
Accept:/ 表示可接受的响应的类型为全部类型
Accept-Language:zh 表示可接受的响应的语言为中文
Accept-Encoding:gzip 表示客户端请求的压缩方式
Cookie:value 值由登陆之后服务端下发
存储于浏览器，用于维持用户会话状态，可存储少量客户端信息（用户偏好，id)，无法轻松跨域
token:value 值由登陆之后服务端下发
自带用户信息和签名，客户端手动存储，可以跨域
请求正文 # 一般为空
HTTP五大类状态码 # 1xx 提示信息，表示目前协议处理的中间状态，还需要后续的操作
2xx 成功，报文已经收到并被正确处理
3xx 重定向，资源位置发生变动，需要客户端重新发送请求
4xx 客户端错误，请求报文有误，服务器无法处理
5xx 服务器错误，服务器在处理请求时内部发生了错误
HTTP的特性 # 1、简单，易于理解
2、灵活和易于扩展
​	HTTP协议里的各类请求方法、URI/UPL、状态码、头字段等每个组成要求都没有被固定死，都允许开发人员自定义和扩充。
​	同时，HTTP由于是工作在应用层（OSI第七层)，则它下层可以随意变化。
3、应用广泛和跨平台
缺点
无状态双刃剑
无状态的好处：因为服务器不回去记忆HTTP的状态，所以不需要额外的资源来记录状态信息，这能减轻服务器的负担，能够把更多的CPU和内存用来对外提供服务。
无状态的坏处：既然服务器没有记忆能力，它在完成有关联性的操作时会非常麻烦。</description>
    </item>
    
    <item>
      <title>golang力扣刷题（二）</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-11-04-golang%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E4%BA%8C/</link>
      <pubDate>Thu, 04 Nov 2021 10:06:27 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-11-04-golang%E5%8A%9B%E6%89%A3%E5%88%B7%E9%A2%98%E4%BA%8C/</guid>
      <description>力扣刷题（二） # 力扣刷题 全部题目模块（101～200）
简单 # 对称二叉树 # 给你一个二叉树的根节点 root ， 检查它是否轴对称。
输入：root = [1,2,2,3,4,4,3]输出：true //不能使用中序遍历后看其是否对称，例如[1,2,2,2,null,2] func isSymmetric(root *TreeNode) bool { return metric(root.Left,root.Right) } func metric(left *TreeNode,right *TreeNode) bool{ if left==nil&amp;amp;&amp;amp;right==nil{ //如果都为nil证明到底了返回true return true } if left==nil||right==nil{ //一个为nil一个不为nil返回false return false } if left.Val!=right.Val{ //不相等返回false return false } return metric(left.Left,right.Right)&amp;amp;&amp;amp;metric(left.Right,right.Left) //将两边同时放进去递归 } 执行用时：0 ms, 在所有 Go 提交中击败了100.00%的用户 内存消耗：2.7 MB, 在所有 Go 提交中击败了67.20%的用户 相交链表 # 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。</description>
    </item>
    
    <item>
      <title>go语言基础（二）</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-10-26-go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%BA%8C/</link>
      <pubDate>Tue, 26 Oct 2021 15:04:57 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-10-26-go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%BA%8C/</guid>
      <description>copy函数 # Go语言的内置函数 copy() 可以将一个数组切片复制到另一个数组切片中，如果加入的两个数组切片不一样大，就会按照其中较小的那个数组切片的元素个数进行复制。
//1.不同类型的切片无法复制 //2.如果s1的长度大于s2的长度，将s2中对应位置上的值替换s1中对应位置的值 //3.如果s1的长度小于s2的长度，多余的将不做替换 func main() { s1 := []int{1, 2, 3} s2 := []int{4, 5} s3 := []int{6, 7, 8, 9} copy(s1, s2) fmt.Println(s1) //[4 5 3] copy(s2, s3) fmt.Println(s2) //[6 7] } l:=make([]string,len(s)) copy(h,s) var, :=, new() ， make()的区别 # 说明 # go语言中，提供了多种变量声明和初始化的方法。这里着重一一说明。并提供一个简单的指南。
指南 # 使用make()，来初始化slice，map 和channel 。 大多数场合，类型明确的场合下，使用短变量声明方式:=。 当使用文字方式初始化一个变量，并且需要指明类型时，使用var变量声明方式。 避免使用new()，除非你需要一个指针变量。 变量声明方式 # go语言可以使用 var 来声明一个变量，并指明变量的数据类型。
// 初始化整数变量，值为10。 var v int = 10 fmt.</description>
    </item>
    
    <item>
      <title>fabric网络中的报错（二）</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-22-fabric%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%8A%A5%E9%94%99%E4%BA%8C/</link>
      <pubDate>Mon, 22 Mar 2021 18:51:50 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-22-fabric%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E6%8A%A5%E9%94%99%E4%BA%8C/</guid>
      <description>重要声明 ，早期写的博客，后面发现里面的某些解决办法是错误的，看个开心就好 ，我也懒得改。 # 问题一： # fatal: unable to access &amp;lsquo;https://github.com/hyperledger/fabric-samples.git/&#39;: Failed to connect to github.com port 443: 拒绝连接
解决办法： # 命令行输入： git config --global --unset http.proxy git config --global --unset https.proxy 问题二； # Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.40/images/create?fromImage=hyperledger%2Ffabric-ca&amp;tag=1.4.9: dial unix /var/run/docker.sock: connect: permission denied
解决办法： # 用VPN下载
Fabric2.3.0版本测试网络运行问题解决办法 # 问题一： # Starting nodes with CLI timeout of &amp;lsquo;5&amp;rsquo; tries and CLI delay of &amp;lsquo;3&amp;rsquo; seconds and using database &amp;rsquo;leveldb&amp;rsquo; with crypto from &amp;lsquo;cryptogen&amp;rsquo;</description>
    </item>
    
    <item>
      <title>go性能优化</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:27 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/</guid>
      <description>go性能优化 # 10亿级for循环 # 获取一个整数5000，然后生成一个随机数，接着通过两层循环对一个数组的每个元素进行累加，最终输出该数组中以随机数为下标对应的数组元素的值。
func Test_one(t *testing.T) { input := 5000 u := int32(input) r := int32(rand.Intn(10000))//使用更快的rand实现 var a [10000]int32 //固定大小数组，栈上分配内存 避免逃逸 for i := int32(0); i &amp;lt; 10000; i++ { for j := int32(0); j &amp;lt; 10000; j++ { a[i] = a[i] + j%u } a[i] += r } fmt.Println(a[r]) return } === RUN Test_one25004351--- PASS: Test_one (0.14s)PASS 将数组元素累积到一个临时变量中，并在外层循环结束后写回数组，这样做可以减少内层循环中的内存读写操作，充分利用CPU缓存和寄存器，加速数据处理。
func Test_two(t *testing.T) { input := 5000 u := int32(input) r := int32(rand.</description>
    </item>
    
    <item>
      <title>Go高阶 高级特性</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:27 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/go%E9%AB%98%E9%98%B6-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/</guid>
      <description>调度机制 # goroutine与线程的区别 # 内存消耗 创建一个goroutine的栈内存消耗为2KB，世纪运行过程中，如果栈空间不够用，会自动进行扩容。创建一个线程则需要消耗1MB栈内存，而且还需要一个被称为“a gurad page“的区域用于和其他thread的栈空间进行隔离。
对于一个用Go构建的HTTP server而言，对到来的每个请求，分别创建一个goroutine用来处理是一个非常轻松的事情。而对于一个使用线程作为并发原语的语言（例如java）构建的服务来说，每个请求对应一个线程则太浪费资源了，如果不加限制，可能会出OOM错误（Out Of Mermory Error)。
创建和销毁 线程创建和销毁都会产生巨大的消耗，因为要和操作系统打交道，是内核级的。通常解决的办法就是使用线程池，尽量复用，减小重复创建和销毁的开销。而goroutine由Go runtime负责管理，创建和销毁的消耗非常小，是用户级的。
切换 当线程切换时，需要保存各种寄存器，以便将来恢复。
而goroutine切换时只需要保存三个寄存器：Program Counter、Stack Pointer和BP。
一般而言，线程切换回消耗1000～1500ns，而goroutine的切换约为200ns，goroutine的切换成本比threads小的多。
Go sheduler # Go程序的执行有两个层面：Go Program 和Runtime，即用户程序和运行时。它们之间通过函数调用来实现内存管理、channel通信、goroutine创建等功能。用户程序进行的系统调用都会被Runtime拦截，以此来帮助它进行调度以及垃圾回收相关的工作。
Go sheduler的目标：将goroutine调度到内核线程上。
Go sheduler的核心思想：
重用线程 限制同时运行（不包括阻塞）的线程数为N，N等于CPU的核心数目。 线程私有runqueues，并且可以从其他线程偷取goroutine来运行，线程阻塞后，可以将runqueues传递给其他线程。 Go scheduler会启动一个后台线程sysmon，用来检测长时间（超过10ms)运行到goroutine，将其“停靠”到global runqueues。这是一个全局的runqueues，优先级比较低，以示惩罚。
G goroutine协程
P processor处理器
M thread线程
Processor 它包含了运行goroutine的资源，如果线程想运行goroutine,必须先获取P，P中还包含了可运行的G队列。
在Go中，线程是运行goroutine的实体，调度器的功能是把可运行的goroutine分配到工作线程上。
全局队列：存放等待运行的G P的本地队列：同全局队列类似，存放的也是等待运行的G，存的数量有限，不超过256个。新建 G’时，G’优先加入到 P 的本地队列，如果队列满了，则会把本地队列中一半的 G 移动到全局队列。 P列表：所有的P都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个。 M：线程想运行任务就得获取P，从P的本地队列获取G，P队列为空时，M 也会尝试从全局队列拿一批 G 放到 P 的本地队列，或从其他 P 的本地队列偷一半放到自己 P 的本地队列。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。 Goroutine 调度器和 OS 调度器是通过 M 结合起来的，每个 M 都代表了 1 个内核线程，OS 调度器负责把内核线程分配到 CPU 的核上执行。</description>
    </item>
    
    <item>
      <title>易错细节</title>
      <link>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/%E6%98%93%E9%94%99%E7%BB%86%E8%8A%82/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:27 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E9%AB%98%E9%98%B6/%E6%98%93%E9%94%99%E7%BB%86%E8%8A%82/</guid>
      <description>容易出错的细节 # 创建对象 # 新建一个对象在go里面有好几种方法，让人迷惑，而且似乎和简洁这一设计原则违背。我们按照对象类型讨论一下：
对于结构体，new(T)和&amp;amp;T{}是等价的，都会给对象赋零值（一般人很少用new）。 Note：直接var obj T;&amp;amp;T也是等价的，只不过变量有可能在堆上，有可能在栈上 对于slice、map、chan，make(map[string]int)和map[string]int{}等价，会对对象进行初始化。 var a []int // nil a := []int{} // not nil a := *new([]int) // nil a := make([]int,0) // not nil 零值 # 零值和未初始化的值并不相同。不同类型的零值是什么？
布尔类型是false，整型是0，字符串是&amp;quot;&amp;quot; 指针、函数、interface、slice、channel和map的零值都是nil 结构体的零值是递归生成的，每个成员都是对应的零值 我们来看一个例子。一个为nil的slice和map能做什么操作：
// 一个为nil的slice，除了不能索引外，其他的操作都是可以的 // Note: 如果这个slice是个指针，不适用这里的规则 var a []int fmt.Printf(&amp;#34;len(a):%d, cap(a):%d, a==nil:%v\n&amp;#34;, len(a),cap(a), a == nil) //0 0 true for _, v := range a{// 不会panic fmt.Println(v) } aa := a[0:0] // 也不会panic，只要索引都是0 // nil的map，我们可以简单把它看成是一个只读的map var b map[string]string if val, ok := b[&amp;#34;notexist&amp;#34;];ok{// 不会panic fmt.</description>
    </item>
    
    <item>
      <title>行为型设计模式</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%A1%8C%E4%B8%BA%E5%9E%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/</guid>
      <description>行为型设计模式 # 策略模式 # 介绍 # 策略模式可以让开发者定义一系列的算法，并且将每种算法分别放入独立的类，从而使算法的对象能够相互替换。策略模式可以将一组行为转换为对象，并且使其在原始对象内部能够相互替换。策略模式可以将一组行为转换为对象，并且使其在原始对象内部能够相互替换。原始对象称为上下文，包含指向策略对象的引用并将执行行为的任务分派给策略对象。为了改变上下文完成其工作的方式，其他对象可以使用另一个对象替换当前链接的策略对象。
当开发者需要使用对象中各种不同算法的变体，并且希望能在运行时切换算法时，可以使用策略模式。策略模式让开发者能够将对象关联至能以不同方式执行特定子任务的不同子对象，从而以间接方式在运行时更改对象行为。 当开发者有许多仅在执行某些行为时略有不同的相似类时，可以使用策略模式。策略模式让开发者能够将不同的行为抽取到一个独立类层次结构中，并且将原始类组合成同一个类，从而减少重复代码。 如果算法在上下文的逻辑中不是特别重要，那么使用策略模式可以将类的业务逻辑与算法实现细节分开。 如果类中使用了复杂条件运算符，用于在同一个算法的不同变体中切换，则可以使用策略模式。策略模式将所有继承自同一个接口的算法抽取到独立类中，因此不需要条件语句。 （1）从上下文类中找出修改频率较高的算法，定义该算法所有变体的通用策略接口
//策略接口 type Strategy interface { Execute() } （2）定义具体策略类及其方法
// 具体策略 A type strategyA struct { } // 具体策略 A 的方法 func (s *strategyA) Execute() { fmt.Println(&amp;#34;执行策略 A&amp;#34;) } // 具体策略B type strategyB struct { } // 具体策略B的方法 func (s *strategyB) Execute() { fmt.Println(&amp;#34;执行策略 B&amp;#34;) } // 创建策略 A 的新对象 func NewStrategyA() Strategy { return &amp;amp;strategyA{} } // 创建策略 B 的新对象 func NewStrategyB() Strategy { return &amp;amp;strategyB{} } （3）定义上下文类及其方法</description>
    </item>
    
    <item>
      <title>数据库基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 16 Sep 2022 10:04:24 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E6%95%B0%E6%8D%AE%E5%BA%93%E5%9F%BA%E7%A1%80/</guid>
      <description>数据库 # 索引 # 索引是什么，有什么作用，有何优缺点？ # 索引是帮助Mysql高效获取数据的一种数据结构，通常用B树，B+树实现（Mysql不支持hash）
数据库索引，hash索引与B+树索引的适用场景，为什么用B+树索引 # B+树是一个平衡的多叉树，从根结点到每个叶子结点的高度差不超过1，而且同层级的结点间有指针相互连接。
在B+树上的常规检索，从根结点到叶子结点的搜索效率基本相当，不会出现大幅的波动，而且基于索引的顺序扫描时，也可以利用双指针快速左右移动，效率非常高。因此，B+树索引被广泛应用于数据库、文件系统等场景。
Hash索引，就是采用一定的Hash算法，把键值换算成新的Hash值，检索时不需要类似B+树那样从根结点到叶子结点逐级查找，只需要一次Hash算法即可立即定位到相应的位置，速度非常快。
对比
如果是等值查询，那么Hash索引明显有绝对优势，因为只需要经过一次算法即可找到相应的键值。前提是键值唯一。 如果是范围查询检索，这时候Hash索引就毫无用武之地了。 同理，Hash索引也无法利用索引完成排序，以及Like这样的部分模糊查询，这种模糊查询本质上也是范围查询。 Hash索引不支持复合索引，对于复合索引来说，Hash索引再计算Hash值的时候是将索引键合并后再一起计算Hash值，不会对每个索引单独计算Hash值。因此，如果用到复合索引的一个或者几个索引时，索引会失效。 B+树索引的关键字检索效率比较平均，不像B树那样波动幅度大，在有大量重复键的情况下，Hash索引的效率也是极低的，因为存在哈希冲突问题。 应用场景
B+树索引结构适用于绝大多数场景 如果数据离散型高、基数大，且为等值查询的时候，Hash索引特别有优势 B 树与 B+ 树的对比
在单行查询的时候，B+ 树会自顶向下逐层查找结点，最终找到匹配的叶子结点。这看起来和 B 树差不多，但其实有两点不同。首先，B+ 树的中间结点没有具体数据，所以同样大小的磁盘页可以容纳更多的结点元素，这就意味着，数据量相同的情况下，B+ 树的结构比 B 树更加 “矮胖”，因此查询时 IO 次数也更少。其次，B+ 树的查询必须最终查找到叶子结点，而 B 树只要找到匹配元素即可，无论匹配元素处于中间结点还是叶子结点。因此，B 树的查找性能并不稳定（最好情况是只查根结点，最坏情况是查到叶子结点）。而 B+ 树的每一次查找都是稳定的
我们再来看看范围查询。B 树做范围查询只能依靠繁琐的中序遍历，而 B+ 树只需要在链表上做遍历即可：即先自顶向下找到范围的下限，再通过链表指针遍历到目标元素
除了查询，还有插入和删除操作，因为 B+ 树的叶子结点包含所有元素，并且以有序的链表结构存储，这样大大提高了增删结点的效率
综上，B+ 树相比 B 树的优势：
磁盘 IO 次数更少 查询性能稳定 范围查询简便 增删结点时，效率更高
主键与非主键和索引的关系 # 主键索引指的就是在主键上做索引，而非主键索引也就是在非主键上加索引。主键索引和非主键索引是有区别的，主键索引存放的值是整行字段的数据，而非主键索引上存放的值不是整行字段的数据，而存放主键字段的值。
因此在使用主键索引查询的时候，直接就可以获得想要的数据，而用非主键索引则会先查询到主键，之后根据主键查询到具体的信息。
非主键索引又称为二级索引，主键索引又称为聚簇索引。
聚簇索引定义：
索引和数据是放在一块的（一个文件存储，主键索引的B+树的叶子节点中存放了索引值和数据行所有字段） 索引的顺序和数据的物理存储一致（因为字段也在B+树的叶子节点中，因此索引按序则整个数据行也是按序的） 非聚簇索引定义： 索引和数据是分开存放的（两个文件存储，索引的B+树的叶子节点中只存放了索引值和指向对应数据行的物理地址） 索引的顺序和数据的物理存储不一致（B+树中的索引值是按序的，但指针中的对应数据行的物理地址并不是按序的） 记住一个结论：</description>
    </item>
    
    <item>
      <title>go语言基础（三）</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%B8%89/</link>
      <pubDate>Wed, 14 Sep 2022 09:58:30 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%B8%89/</guid>
      <description>Golang易错知识点 # GoMock # GoMock可以对interface打桩 GoMock可以对类成员函数打桩 GoMock可以对函数打桩 GoMock打桩后的依赖注入可以通过GoStub完成 GoStub # GoStub可以对全局变量打桩 GoStub可以对函数打桩 GoStub可以动态打桩，比如对一个函数打桩后，多次调用该函数会有不同的行为 作用域 # func main() { a := 12 { a := 13 _ = a // make compiler happy } fmt.Println(a) } 输出 12。 在作用域内的 a 在作用域外失效，所以输出 12。
添加方法 # 可以给任意类型添加相应的方法。这一说法是否正确 false
如果直接给int添加method会报错
任意自定义类型(包括内置类型，但不包括指针类型)添加相应的方法。
序列化 # type S struct { A int B *int C float64 d func() string e chan struct{} } func main() { s := S{ A: 1, B: nil, C: 12.</description>
    </item>
    
    <item>
      <title>数据结构-go</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-go/</link>
      <pubDate>Sun, 27 Mar 2022 19:41:51 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-go/</guid>
      <description>链表 # 链表定义 # type ListNode struct{//单链表 Val int Next *ListNode } type DoubleNode struct{//双链表Val intPrev *DoubleNodeNext *DoubleNode} 创建链表 # func CreatListNode(list []int) (tai *ListNode ){ head := &amp;amp;ListNode{Val: list[0]} //无头节点情况 head:=&amp;amp;ListNode{} tail := head for i := 1; i &amp;lt; len(list); i++ { //有头节点，这里i=0 head.Next = &amp;amp;ListNode{Val: list[i]} head = head.Next } return tail } func CreatDoubleNode(list []int) (head *DoubleNode) { //创建双链表 p := &amp;amp;DoubleNode{} q := p for i := 0; i &amp;lt; len(list); i++ { p.</description>
    </item>
    
    <item>
      <title>智能合约</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-10-%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/</link>
      <pubDate>Mon, 10 May 2021 09:27:50 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-10-%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/</guid>
      <description>智能合约 # 1.什么是链码 # 链码是程序，用Go，Node.js，Java其中一种语言编写的，提供分布式账本的状态处理逻辑。链码运行在Peer的独立进程中，负责初始化账本，管理账本状态。
链码能够独立运行在具有安全特性的受保护的Docker容器中，以gRPC协议与相应的Peer节点进行通信，并操作（初始化或管理）分布式账本中的数据。
链码通常用来处理网络成员同意的逻辑事务，所以它也被称为“智能合约”。可以调用链码更新或者查询交易。如果有合适的权限，两码可以调用另一个链码，无论是否在一个channel中，获取账本状态。
注意如果被调用的链码和链码处于不同的channel中，只有读权限。也就是说被调用链码只有读功能，不参与后续事务的验证和检查。
在hyperledger fabric中链码一般分为系统链码和用户链码。
（1）系统链码
系统链码负责fabric节点自身的处理逻辑，包括系统配置、背书、校验等工作。hypgeledger fabric 系统链码仅支持go语言，在peer节点启动时会自动完成注册和部署。
配置系统链码 生命周期系统链码 查询系统链码 背书管理系统链码 验证系统链码 （2）用户链码
开发人员编写的基于区块链分布式账本状态的业务处理逻辑代码运行在链码容器中。通过hyperledger fabric 提供的接口与账本状态进行交互。
生命周期 # install：安装在指定的Peer节点中。 instantiate：进行实例化 //过时了 upgrade：链码升级 package：对链码进行打包 singnpackage：对打包的文件进行签名 链码安装在一个节点中还是安装在多个节点中？有什么区别？
​	在实际生产环境中，必须在应用通道上每一个要运行链码的背书节点上安装链码，其他未安装链码的节点不能执行链码，但仍可以验证交易并提交到账本中。
链码执行查询与执行事务的流程相同吗？
不同，执行查询操作，则客户端接收到背书的交易提案响应后不会再将交易请求提交给Orderer节点。
背书策略具体指的是什么？
背书策略是一种在实例化链码时指定由当前通道中的那些成员节点进行背书签名的策略。
如果在实例化链码时没有指定背书策略，那么会有节点进行背书吗？
会，默认的背书策略时MSP标识DEFAULT成员的签名
CORE_PEER_ADDRESS=peer:7052中的7052端口指的是什么，为什么不是7051？
7052是用于指定链码的专用监听地址及端口号，而7051是peer节点监听的网络端口。
2.初始整理 # 首先创建一个存放链码的目录，我们使用以下命令在GOPATH下创建一个目录
cd $GOPATH mkdir chaincode cd chaincode 接着使用以下命令初始化这个项目并创建一个go文件
go mod init chaincode touch sacc.go 链代码的包名的必须是main
package main 必须要引入的包shim 和peer，用于客户端与Fabric框架通信
import ( &amp;#34;github.com/hyperledger/fabric-chaincode-go/shim&amp;#34; &amp;#34;github.com/hyperledger/fabric-protos-go/peer&amp;#34; ) 自定义一个结构体, 基于这个结构体实现一些接口函数</description>
    </item>
    
    <item>
      <title>fabric-sdk-go详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-08-fabric-sdk-go%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sat, 08 May 2021 20:47:33 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-08-fabric-sdk-go%E8%AF%A6%E8%A7%A3/</guid>
      <description>fabric-go-sdk # 1、概述 # ​	Fabric的Peer节点和Orderer节点都提供了基于GRPC协议(Google开发的远程过程调用RPC)的接口，通过这些接口可以和Peer节点与Orderer节点进行命令/数据交互，为了简化开发，官方提供了多语言版本的SDK。
fabric-go-sdk官方网址为https://github.com/hyperledger/fabric-sdk-go
pkg目录是fabric-go-sdk的主要实现，internel目录和third_party目录包含了fabric-go-sdk依赖的一些代码。
pkg/fabsdk：Fabric SDK 的主包。此包支持基于配置创建上下文。这些上下文由下面列出的客户端包使用。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/fabsdk
pkg/client/channel：提供通道事务能力。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/client/channel
pkg/client/event：提供通道事件能力。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/client/event
pkg/client/ledger：启用对通道底层账本的查询。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/client/ledger
pkg/client/resmgmt：提供安装链码等资源管理能力。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/client/resmgmt
pkg/client/msp：启用身份管理功能。参考：https ://godoc.org/github.com/hyperledger/fabric-sdk-go/pkg/client/msp
基本工作流程
1) 使用配置实例化一个 fabsdk 实例。注意：fabsdk 维护缓存，因此您应该最小化 fabsdk 本身的实例。2) 使用您的 fabsdk 实例创建基于用户和组织的上下文。注意：通道上下文还需要通道 ID。3) 使用它的 New func 创建一个客户端实例，传递上下文。注意：您为所需的每个上下文创建一个新的客户端实例。4）使用每个客户提供的功能来创建您的解决方案！5) 调用 fabsdk.Close() 释放资源和缓存。 2、准备网络环境 # 准备证书文件 # 具体参照solo节点测试
在$GOPATH/src目录下创建一个名为sdktest的文件夹做为项目根目录,在此目录下创建名为fixtures的文件夹存放我们网络相关配置文件。
编辑crypto-config.yaml的文件（这里为一个组织两个节点）
cryptogen generate --config=crypto-config.yaml 生成证书 在fixtures路径下创建一个名为configtx.yaml的文件
编辑configtx.yaml文件
生成创世块文件
生成通道文件
锚节点更新（两个组织都要更新）
完成后：channel-artifacts文件夹channel.tx Org1MSPanchors.tx Org1MSPanchors.tx genesis.block 配置docker-compose文件</description>
    </item>
    
    <item>
      <title>fabric环境搭建</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-24-fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Wed, 24 Mar 2021 17:12:42 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-24-fabric%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid>
      <description>Hyperledger Fabric基础环境之Docker # Docker 是一个开源的应用容器引擎，基于Go语言并遵从Apache2.0 协议开源。Docker 可以让开发者打包他们的应用以及依赖包到一个轻量级、可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口,更重要的是容器性能开销极低。Docker 从 17.03 版本之后分为 CE（Community Edition: 社区版） 和 EE（Enterprise Edition: 企业版），我们用社区版就可以了。
1.Docker安装与配置 # 使用 Docker 仓库进行安装在新主机上首次安装 Docker Engine-Community 之前，需要设置 Docker 仓库。之后，您可以从仓库安装和更新 Docker 。
使用apt-get命令更新包索引。
sudo apt-get update 使用apt-get命令安装依赖包，用于通过HTTPS来获取仓库。
sudo apt-get install \ apt-transport-https \ ca-certificates \ curl \ git \ gnupg-agent \ software-properties-common 使用curl命令添加 Docker 的官方 GPG 密钥。
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 使用以下命令通过搜索指纹的后8个字符，验证您现在是否拥有带有指纹的密钥。</description>
    </item>
    
    <item>
      <title>设计模式扩展</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%89%A9%E5%B1%95/</link>
      <pubDate>Thu, 03 Nov 2022 13:46:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E6%89%A9%E5%B1%95/</guid>
      <description> 设计模式扩展 # 空对象模式 # 介绍 # 优点 # 缺点 # 示例 # 规格模式 # 介绍 # 优点 # 缺点 # 示例 # 领域驱动设计 # 介绍 # 优点 # 缺点 # 示例 # </description>
    </item>
    
    <item>
      <title>fabric-ca详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-15-fabric-ca%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Thu, 15 Apr 2021 14:30:23 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-15-fabric-ca%E8%AF%A6%E8%A7%A3/</guid>
      <description>MSP # msp定义 # MSP是hyperleger fabric对网络中的组成成员进行身份管理与验证的模块组件。
作用：
管理用户ID
验证想要加入网络的节点
为客户发起的交易提供凭证
MSP 在Hyperledger Fabric中按级别分类如下：
网络MSP：对整个hyperledger fabric网络中的成员进行管理；定义参与组织的MSP，以及组织成员中的那些成员被授权执行管理任务（如创建通道）
通道MSP：对一个通道中的成员进行管理，通道在特定的一组组织之间提供私有通信；在该通道的MSP环境中（通道策略）定义了谁有权限参与通道上的某些行为（如添加组织或实例化链码）。
Peer MSP：每个Peer节点都有一个单独的MSP实例，执行与通道MSP完全相同的功能，其限制是它仅适用于定义它的Peer节点。
Orderer MSP：与Peer MSP相同，Orederer节点的本地MSP也在其节点的文件系统上定义，仅适用于该Orderer节点。
User MSP：每个组织都可以拥有多个不同的用户，都在其Organization节点的文件系统上定义，仅适用于定义它的Peer节点。
在Hyperledger Fabric中，各个网络参与者之间的通信安全依赖于PKI（Public Key Infrastructure,公钥基础结构）标准实现，并确保在区块链上发布的消息得到相应的认证。
PKI只是一个体系结构，负责生成及颁发证书。在H yperledger fabric 中，默认MSP实际上使用符合X.509标准的证书作为身份，采用传统的PKI分层模型来实现。
PKI的四个关键要素：
数字证书：最常见的证书类型符合X.509标准的证书。
公钥和私钥：
证书颁发机构：这些证书由CA进行数字签名，CA是为组织的参与者提供可验证的数字身份的基础。
证书撤销列表：
MSP的组成结构 # MSP
RCA 根CA ：文件夹包含根CA的自签名X.509证书列表，用于自签名及给中间CA证书签名。 ICA 中间CA ：包含根CA颁发的证书列表。 OU 组织单位：这些单位列在$FABRIC_CFG_PATH/msp/config.yaml文件中，包含一个组织单位列表，其成员被视为该MSP所代表的组织的一部分。 B 管理页：此文件夹包含一个标识列表，用于定义具有此组织管理员角色的角色。 ReCA 撤销证书：保存已被撤销参与者身份的信息。 SCA 签名证书：背书节点在交易提案响应中的签名证书。 KeyStore 私钥： TLS RCA TLS根CA TLS ICA TLS中间CA Fabric-ca # fabric-ca 项目是专门为了解决Fabric账号问题而发起的一个开源项目, 它非常完美的解决了fabric账号生成的问题。fabric-ca项目由 fabric-server 和fabric-client这两个模块组成。其中fabric-server在 fabric中占有非常重要的作用。我们使用cryptogen命令可以同配置文件生成一些账号信息, 但是如果有动态添加账号的需求, 就无法满足, 所以这个时候我们就应该在项目中引入fabric-ca。</description>
    </item>
    
    <item>
      <title>solo节点测试</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-25-fabric-solo%E8%8A%82%E7%82%B9%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Thu, 25 Mar 2021 19:18:57 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-25-fabric-solo%E8%8A%82%E7%82%B9%E6%B5%8B%E8%AF%95/</guid>
      <description>生成Fabric证书 # Hyperledger Fabric通过证书文件来配置组织、节点以及用户。证书文件（实际上，数字证书就是经过CA认证过的公钥）的标准为X.509，编码格式为PEM，以—–BEGIN开头,以—–END结尾。X.509 数字证书不但包括用户名和公共密钥，而且还包括有关该用户的其他信息。除了扩展名为PEM的还有以下这些：
CRT ：应该是certificate的三个字母，还是证书的意思。打开看也是PEM编码格式。 KEY： 用来存放一个公钥或私钥，并非X.509证书。打开看依然PEM格式。 证书的默认签名算法为ECDSA，Hash算法为SHA-256。Fabric中设计中考虑了三种类型证书:
登记证书（ECert）：颁发给提供了注册凭证的用户或节点实体，长期有效。（主要就是通ECert对实体身份检验） 通信证书（TLSCert）：TLS证书用来保障通信链路安全，控制对网络层的接入访问，可以对远端实体身份校验，防止窃听。 交易证书（TCert）：颁发给用户，控制每个交易的权限，一般针对某个交易，短期有效。 1.证书的文件的编写 # 首先我们使用以下命令在进入~/hyperledger目录并创建一个项目目录solotest。
cd ~/hyperledger mkdir solotest cd solotest 我们可以使用以下命令来查看生成证书文件的模板文件。
cryptogen showtemplate 使用以下命令将模板文件复制到当前目录下。
cryptogen showtemplate &amp;gt; crypto-config.yaml 配置文件的模板如下：
OrdererOrgs:	- Name: Orderer	Domain: example.com	Specs: - Hostname: orderer - Hostname: orderer2 PeerOrgs: - Name: Org1	Domain: org1.example.com	EnableNodeOUs: true	Template:	Count: 2 Users:	Count: 2 - Name: Org2 Domain: org2.example.com EnableNodeOUs: true Template: Count: 2 Specs: - Hostname: hello Users: Count: 1 OrdererOrgs 排序节点组织信息</description>
    </item>
    
    <item>
      <title>动态路由与静态路由</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%E4%B8%8E%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1/</link>
      <pubDate>Tue, 27 Dec 2022 21:22:31 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/%E5%8A%A8%E6%80%81%E8%B7%AF%E7%94%B1%E4%B8%8E%E9%9D%99%E6%80%81%E8%B7%AF%E7%94%B1/</guid>
      <description>静态路由 # 静态路由：静态路由是在路由器中设置固定的路由表；除非网络管理员进行干预，否则静态路由表不会发生变化。
动态路由 # 动态路由：由网络中的路由器之间相互通信，传递路由信息，利用收到的路由信息更新路由表的路由方式。
动态路由是与静态路由相对的一个概念，指路由器能够根据路由器之间交换的特定路由信息自动建立自己的路由表，并且能够根据链路和节点的变化适时地进行自动调整。
当网络节点或节点间的链路发生故障，或者存在其它可用路由时候，动态路由可以自行选择“最佳”的可用路由。
换句话说，动态路由就好比我们选择自由行，我们根据目的地和每个景区的情况实时地变更我们的旅行安排。比如深圳遇到交通管控，我们可以选择从广州绕行，不会因为一些意外情况耽误旅行。但是也要承担自由的“代价”，就是需要根据变化实时费心安排。
相似的，动态路由可以自动根据网络拓扑结构变化进行调整，同时也会占用路由器的CPU、内存和链路带宽。
常见的动态路由协议有：
RIP（Routing Information Protocol，路由信息协议）、OSPF（Open Shortest Path First，开放最短路径优先）、IS-IS（Intermediate System-to-Intermediate System，中间系统到中间系统）、BGP（Border Gateway Protocol，边界网关协议）。
每种动态路由协议的工作方式、选路原则等都有所不同，想要理解它们的工作原理需要更深的专业知识。
想进一步了解各种动态路由协议的话，请给文档君留言，让我看到你们的双手~动态路由协议虽然有很多，但是有两条通用规则：
（1）路由器之间需要实时地交换路由信息。你的路由表给我看看，我的路由表给你看看，你好我也好~
动态路由之所以能够根据网络的情况自动计算路由、选择转发路径，是由于当网络发生变化时，路由器之间彼此交换的路由信息会告知对方网络的这种变化，通过信息扩散使得所有路由器都能得知网络的变化。
（2）路由器根据路由算法把收集到的路由信息加工成路由表，供路由器在转发IP报文时查阅。
在网络发生变化时，路由器收集到最新的路由信息后，重新计算路由，从而可以得到最新的路由表。
需要说明的是， 路由器之间的路由信息在不同路由协议中交换的过程和原则是不同的。交换路由信息的最终目的在于通过路由表找到一条转发IP报文的最佳路径。
每一种路由算法都有其衡量”最佳“的一套原则，大多数是在综合多个特性的基础上进行计算。
这些特性有：路径所包含的路由节点数（hop count）、网络传输费用（cost）、带宽（bandwidth）、延迟（delay）、负载（load）、可靠性（reliability）和最大传输单元MTU（maximum transmission unit）。
特征对比 # 动态路由和静态路由的特点对比如下：
静态路由 动态路由 配置复杂性 随着网络规模的增大而越趋复杂 通常不受网络规模限制 管理员所需知识 不需要额外的专业知识 需要了解动态路由协议和技能 拓扑结构变化 需要管理员参与 自动根据拓扑变化进行调整 可扩展性 适合简单的网络拓扑结构 简单拓扑结构和复杂拓扑结构都适合 安全性 更安全 没有静态路由安全 资源占用 不需要额外的资源 占用CPU、内存和链路带宽 可预测性 总是通过同一路径到达目的地 根据当前网络拓扑结构确定路径 优点： # 静态路由：简单、高效、可靠、网络安全、转发效率高。
动态路由：灵活，能够适时适应网络结构的变化，无需管理员手工维护，减轻了管理员的工作负担。
缺点： # 静态路由：不能灵活的适应网络的动态变化。
动态路由：占用网络带宽（用于传输路由更新信息）。
使用场景： # 静态路由：网络规模不大，拓扑结构固定的网络中。</description>
    </item>
    
    <item>
      <title>gin框架</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/gin%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Mon, 27 Dec 2021 21:22:31 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/gin%E6%A1%86%E6%9E%B6/</guid>
      <description>简介 # 介绍 # Gin是一个golang的微框架，封装比较优雅，API友好，源码注释比较明确，具有快速灵活，容错方便等特点 对于golang而言，web框架的依赖要远比Python，Java之类的要小。自身的net/http足够简单，性能也非常不错 借助框架开发，不仅可以省去很多常用的封装带来的时间，也有助于团队的编码风格和形成规范 安装 # 要安装Gin软件包，您需要安装Go并首先设置Go工作区。
1.首先需要安装Go（需要1.10+版本），然后可以使用下面的Go命令安装Gin。
go get -u github.com/gin-gonic/gin
2.将其导入您的代码中：
import &amp;ldquo;github.com/gin-gonic/gin&amp;rdquo;
3.（可选）导入net/http。例如，如果使用常量，则需要这样做http.StatusOK。
import &amp;ldquo;net/http&amp;rdquo;
hello word # package main import ( &amp;#34;net/http&amp;#34; &amp;#34;github.com/gin-gonic/gin&amp;#34; ) func main() { // 1.创建路由 r := gin.Default() // 2.绑定路由规则，执行的函数 // gin.Context，封装了request和response r.GET(&amp;#34;/&amp;#34;, func(c *gin.Context) { c.String(http.StatusOK, &amp;#34;hello World!&amp;#34;) }) // 3.监听端口，默认在8080 // Run(&amp;#34;里面不指定端口号默认为8080&amp;#34;) r.Run(&amp;#34;:8000&amp;#34;) } Gin路由基础 # 1 路由的基本使 # gin 框架中采用的路由库是基于httprouter做的 地址为：https://github.com/julienschmidt/httprouter
1.1 基本路由 # package main import ( &amp;#34;github.</description>
    </item>
    
    <item>
      <title>流式数据</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/%E6%B5%81%E5%BC%8F%E6%95%B0%E6%8D%AE/</link>
      <pubDate>Mon, 27 Dec 2021 21:22:31 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/%E6%B5%81%E5%BC%8F%E6%95%B0%E6%8D%AE/</guid>
      <description> 流式数据 # </description>
    </item>
    
    <item>
      <title>fabric多机搭建</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-25-fabric%E5%A4%9A%E6%9C%BA%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Thu, 25 Mar 2021 19:26:42 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-25-fabric%E5%A4%9A%E6%9C%BA%E6%90%AD%E5%BB%BA/</guid>
      <description>多机搭建前准备 # 这部分实验内容使用的是Ubuntu操作系统，所需要的实验环境与单节点搭建部分相同，包括docker的安装golang的安装fabric的安装等。为了方便，以上环境已在虚拟机中安装完成。
1.网络结构 # 这部分课程我们要搭建一个多机多节点的网络，结构如下。网络中有两个组织分别为org1、org2，每个组织各有一个peer节点，同时还有一个orderer节点。
名称 IP hosts 组织机构 Orderer 172.17.0.10 orderer.test.com orderer Org1peer0 172.17.0.11 peer0.org1.test.com org1 Org2peer0 172.17.0.12 peer0.org2.test.com org2 2.设置网络host # 使用以下命令，我们在三台虚拟机中分别查看当前虚拟机的IP，其中最后一行为本机IP。
cat /etc/hosts 127.0.0.1	localhost ::1	localhost ip6-localhost ip6-loopback fe00::0	ip6-localnet ff00::0	ip6-mcastprefix ff02::1	ip6-allnodes ff02::2	ip6-allrouters 172.17.0.10	1cbb99f39f9a 配置所有服务器网络host,在三台虚拟机中都进行以下操作。
vi /etc/hosts 在最后插入（IP与host任意指定，确定后不能更改），写入以下内容后，按 Esc 键退回一般模式，然后输入 :wq 命令并回车保存退出文件。
172.17.0.10 orderer.test.com 172.17.0.11 peer0.org1.test.com 172.17.0.12 peer0.org2.test.com 3.ssh安装 # 在多机搭建的过程中我们会使用到scp命令。Linux scp 命令用于 Linux 之间复制文件和目录。
scp 是 secure copy 的缩写, scp 是 Linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。</description>
    </item>
    
    <item>
      <title>Beego框架</title>
      <link>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/beego%E6%A1%86%E6%9E%B6/</link>
      <pubDate>Sun, 07 Mar 2021 13:48:22 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/web%E6%A1%86%E6%9E%B6/beego%E6%A1%86%E6%9E%B6/</guid>
      <description>beego框架 # beego框架了解 # Beego框架是go语言开发的web框架。
beego是中国人开发的，开发文档比较详细
beego网址 MVC架构 # Beego是MVC架构。MVC 是一种应用非常广泛的体系架构，几乎所有的编程语言都会使用到，而且所有的程序员在工作中都会遇到！用 MVC 的方式开发程序，可以让程序的结构更加合理和清晰。 我们画图说明
环境搭建 # 这里默认大家已经搭建好了go语言的开发环境。
需要安装Beego go get -u github.com/beego/bee/v2@master go install github.com/beego/bee/v2@master //上面没用就试试这个 而后运行 bee version | ___ \| |_/ / ___ ___| ___ \ / _ \ / _ \| |_/ /| __/| __/\____/ \___| \___| v2.0.2├── Beego : Beego is not installed. Please do consider installing it first: https://github.com/beego/beego/v2. If you are using go mod, and you don&amp;#39;t install the beego under $GOPATH/src/github.</description>
    </item>
    
    <item>
      <title>Atomic</title>
      <link>https://chain-code.github.io/docs/golang/package/atomic/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/atomic/</guid>
      <description>快速 # Go语言中的 sync/atomic 包提供了一组原子操作函数，用于在多线程或并发环境下执行对共享变量的原子操作。这些操作是原子的，不会受到其他并发操作的干扰，从而避免了竞态条件和数据竞争问题。sync/atomic 包通常用于同步和管理共享资源，以确保线程安全。
Load 操作 读取 # atomic.LoadInt32(&amp;amp;addr int32) int32 原子性地读取一个 int32 值。 atomic.LoadInt64(&amp;amp;addr int64) int64 原子性地读取一个 int64 值。 atomic.LoadUint32(&amp;amp;addr uint32) uint32 原子性地读取一个 uint32 值。 atomic.LoadUint64(&amp;amp;addr uint64) uint64 原子性地读取一个 uint64 值。 atomic.LoadUintptr(&amp;amp;addr uintptr) uintptr 原子性地读取一个 uintptr 值。 atomic.LoadPointer(&amp;amp;addr unsafe.Pointer) unsafe.Pointer 原子性地读取一个指针值。 Store 操作 设置 # atomic.StoreInt32(&amp;amp;addr int32, val int32) 原子性地设置一个 int32 值。 atomic.StoreInt64(&amp;amp;addr int64, val int64) 原子性地设置一个 int64 值。 atomic.StoreUint32(&amp;amp;addr uint32, val uint32) 原子性地设置一个 uint32 值。 atomic.</description>
    </item>
    
    <item>
      <title>MySql相关问题1</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%981/</link>
      <pubDate>Tue, 30 Apr 2024 16:16:50 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%981/</guid>
      <description>有了关系型数据库，为什么还需要NOSQL? # nosql数据库无需提前设计表结构，数据可以根据需要自由存储和组织，相较于关系型数据库，nosql高效灵活，非常适合复杂化、高效化、高并发的场景中。
关系型数据库
数据以行和列的方式存储
采用结构化查询语言来对数据进行查询
强调ACID规则：原子性atomicity、一致性consistency、隔离性isolation、持久性durability
强调数据一致性，因此牺牲了读写性能
通常存储在硬盘中
mysql也可以基于内存，即内存表技术。它运行将数据和索引存储在内存中，从而提高查询和修改效率。
如何创建？
与创建普通表一样，使用CREATE TABLE，需要将存储引擎设置为：ENGINE=MEMORY
非关系型数据库
以键值对来存储 没有固定的要求和限制 事务一般无法回滚（部分可以，MongoDB) 需要通过key来查询 基于内存存储（MongoDB基于磁盘） 不支持范围查询 数据库存储引擎 # 数据库引擎是用于存储、处理和保护数据的核心服务。利用数据库引擎可以控制访问权限并快速处理事物，从而满足业务需求。
查看当前使用什么引擎
show engines
查看当前默认存储引擎
show variables like %storage_engine%
查看某表用什么引擎
show create table 表名
结果参数中有
如何指定引擎
创建表时：create语句后面加上engine=“” 修改表时：alter table 表名 engine=&amp;quot;&amp;quot; InnoDB和MyISM的区别 # InnoDB和MyISM的区别
mysql存储引擎是基于表的吗 # 是，不是基于数据库
联合索引 和 mysql 调优的关系 # mysql 调优的一个核心动作，就是通过联合索引实现索引覆盖。
在MySQL中，合理使用联合索引可以提高查询效率，通过 联合索引 实现 索引覆盖 ，常常需要注意一些技巧：
选择合适的列：联合索引的列顺序非常重要。应该优先选择最频繁用于查询条件的列，以提高索引效率。其次考虑选择性高的列，这样可以过滤出更少的数据。 避免冗余列：联合索引的列应该尽量避免包含冗余列，即多个索引的前缀相同。这样会增加索引的维护成本，并占用更多的存储空间。 避免过度索引：不要为每个查询都创建一个新的联合索引。应该根据实际情况，分析那些查询是最频繁的，然后创建针对这些查询的索引。 覆盖索引：如果查询的列都包含在联合索引中，并且不需要访问表的其他列，那么MySql可以直接使用索引来执行查询，不必访问表，这种索引称为覆盖索引，可以提高查询性能。 使用EXPLAIN进行查询计划分析： 使用MySQL的EXPLAIN语句可以查看MySQL执行查询的执行计划，以便优化查询语句和索引的使用。 定期优化索引： 随着数据库的使用，索引的效率可能会下降，因此需要定期进行索引的优化和重建，以保持查询性能的稳定性。 分析查询日志： 监控数据库的查询日志，分析哪些查询是最频繁的，以及它们的查询模式，可以帮助确定需要创建的联合索引。 避免过度索引更新： 避免频繁地更新索引列，因为每次更新索引都会增加数据库的负载和IO操作。 综上所述，联合索引是mysql 调优的一个核心动作， 通过 联合索引进行mysql 调优时，需要综合考虑列的选择、索引的覆盖、查询的频率和模式等因素，以提高MySQL数据库的查询性能。</description>
    </item>
    
    <item>
      <title>MySql锁相关总结</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E9%94%81%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 30 Apr 2024 16:16:50 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mysql%E9%94%81%E7%9B%B8%E5%85%B3%E6%80%BB%E7%BB%93/</guid>
      <description>MySql锁总体结构 # MySql的锁总体分为三类：总体、类型、用途
总体上分为：乐观锁和悲观锁 类型上分为：读锁和写锁 粒度上分为：表锁、行锁、页面锁、间隙锁、临键锁 悲观锁 # 悲观锁对数据库中的数据读写持悲观态度，即在整个数据处理过程中，他会悲观的认为数据不会保持一致性，所以是会将相应的数据锁定。在数据库中，悲观锁的实现是依赖数据库提供的锁机制。
如果加上了悲观锁，那么就无法对这些数据进行读取操作。
乐观锁 # 乐观锁对于数据库的数据的读写持乐观态度，即在整个数据处理的过程中，他会很乐观的认为数据会保持一致性，所以不加锁，而是通过数据版本记录机制实现。
MySQL中的MVCC多版本控制就是乐观锁的一种实现方式。
往往会在数据表中增加一个类型version的版本号字段。 在查询数据库中的数据时，会将版本号字段的值一起读取出来。 当更新数据时，会令版本号字段的值加1。将提交数据的版本与数据库表对应记录的版本进行对比。 如果提交的数据版本号大于数据表中当前要修改的数据的版本号，则数据进行修改操作。 否则不修改数据库表中的数据。 读锁 # 读写又称为共享锁或者S锁（Shared Lock），针对同一份数据，可以加多个读锁而互不影响。
写锁 # 写锁又称为排他锁或者X锁（Exclusive Lock），如果当前写锁未释放，他会阻塞其他的写锁和读锁。
表锁 # 表锁也称为表级锁，就是在整个数据表上对数据进行加锁和释放锁。特点：开销小，加速快，粒度大，并发度最低，发生锁冲突概率高。
在MySQL中，有两种表锁模式：一种是表共享锁（Table Shard Lock），另一种是表独占写锁（Table Write Lock）。
当一个线程获取到一个表的读锁后，其他线程仍然可以进行读操作，但不能对表进行写操作。那么对应的如果一个线程获取到一个表的写锁后，只有这个线程可以进行读写操作，其他线程无法对表进行读写操作，直到写锁被释放为止。
在mysql中可以通过以下命令手动添加表锁
LOCK TABLE 表名称 read(write); eg: 添加读表锁
LOCK TABLE user_table read; eg: 添加写表锁
LOCK TABLE user_table write; 使用如下命令可以查看数据表上增加的锁
SHOW OPEN TABLES; 删除表锁：
UNLOCK TABLES; 行锁 # 行锁也称为行级别，就是在数据行上对数据进行加锁和释放锁。特点：开销大，加锁慢，粒度小，并发度高，锁冲突概率最小。
在mysql的InnoDB存储引擎中有两种行锁，排他锁和共享锁。
共享锁：允许一个事务读取一行数据，但不允许一个事务对加了共享锁的当前行增加排他锁。 排他锁：允许当前事务对数据行进行增删改查操作，不允许其他事务对增加了排他锁的数据行增加共享锁和排他锁。 注意：
行锁主要加在索引上，如果对非索引字段设置条件进行更新，行锁可能会变成表锁。例如UPDATE user_table SET number = 2 WHERE name = &#39;fanone&#39; 如果name没有加索引，那么可能会进行表锁。所以我们一般建议使用主键id作为更新数据的查询条件。 InnoDB的行锁是针对索引加锁，不是针对记录加锁，并且加锁的索引不能失效，否则行锁也有可能变成表锁。而导致索引失效的有很多，比如联合索引不遵循最左匹配原则会失效、OR会失效等等&amp;hellip; 锁定某一行时，可以使用lock in share mode命令来指定共享锁，使用for update命令来指定排他锁。 UPDATE user_table SET number = 2 WHERE name = &amp;#39;fanone&amp;#39; LOCK IN SHARE MODE;UPDATE user_table SET number = 2 WHERE name = &amp;#39;fanone&amp;#39; FOR UPDATE; 页面锁 # 页级锁定是 MySQL 中比较独特的一种锁定级别。特点：锁定颗粒度介于行级锁定与表级锁之间，锁开销和加锁时间界于表锁和行锁之间，并发处理能力也同样是介于上面二者之间，并发度一般。</description>
    </item>
    
    <item>
      <title>Os</title>
      <link>https://chain-code.github.io/docs/golang/package/os/</link>
      <pubDate>Fri, 05 May 2023 11:51:49 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/os/</guid>
      <description>Go文件操作 # 文件系统简介 # 文件系统是计算机用于存储和组织数据的一种方式，它定义了如何在计算机硬件上存储、读取、写入和管理文件和目录。文件系统通常由操作系统提供，它们可以支持不同的文件格式和存储设备，如磁盘驱动器、闪存驱动器、CD-ROM和网络驱动器等。文件系统可以帮助用户和应用程序组织和管理计算机中的文件和文件夹，使它们易于访问和处理。它们还提供了安全性和数据完整性方面的保护，以确保用户的数据不会被意外删除或破坏。一些常见的文件系统包括Windows的NTFS和FAT32、Linux的EXT4和Btrfs，以及Mac OS X的HFS+和APFS。
Windows操作系统支持 NTFS, FAT32, and exFAT三种不同文件系统。NTFS是目前Windows系统中一种现代文件系统，目前使用最广泛，内置的硬盘大多数都是NTFS格式。FAT32是一种相对老旧的文件系统，不能像NTFS格式支持很多现代文件格式的属性，但对于不同系统平台具有良好的兼容性，可以在Linux、Mac或Android系统平台上通用。exFAT是FAT32文件格式的替代品，很多设备和操作系统都支持该文件系统，但是目前用的不多。
目前的大部分 Linux 文件系统都默认采用 ext4 文件系统，正如以前的 Linux 发行版默认使用 ext3、ext2 以及更久前的 ext。
NTFS # NTFS（New Technology File System）是Windows操作系统中使用的一种先进的文件系统，是Windows NT家族的标准文件系统。NTFS支持更高级的文件管理功能，如文件和目录的权限、加密、压缩、磁盘配额等，也支持更大的磁盘容量和更大的文件大小。以下是NTFS的一些特点：
安全性：NTFS支持文件和文件夹的权限控制，可以为每个用户或组设置不同的访问权限，确保数据的安全性和隐私性。 可靠性：NTFS使用日志记录和故障容错技术，可以检测并修复磁盘上的错误和损坏。 空间利用率：NTFS使用动态存储分配和簇大小调整，使得文件系统可以更有效地利用磁盘空间。 文件压缩：NTFS支持文件和文件夹的压缩，可以节省磁盘空间，并且对于大量文本数据可以获得更高的数据压缩比。 数据加密：NTFS支持文件和文件夹的加密，可以保护数据的机密性。 大文件支持：NTFS支持极大的文件和分区大小，最大文件大小为16EB（EB表示艾字节，1EB=1024PB），最大分区大小为256TB。 总之，NTFS是一个高级的、功能强大的文件系统，提供了许多重要的功能和优势，因此它被广泛用于Windows操作系统和应用程序中。
FAT32 # FAT32（File Allocation Table 32），用于在Windows操作系统中格式化存储设备，如磁盘、USB驱动器等。FAT32是FAT文件系统的一种升级版本，它支持更大的磁盘空间和文件大小，并且具有更好的兼容性。以下是FAT32的一些特点：
兼容性：FAT32是一种通用的文件系统，几乎可以在所有操作系统和设备上进行访问和读取，例如Windows、Mac、Linux、Android和其他平台。 可移植性：FAT32格式化的设备可以轻松地从一台计算机或设备移动到另一台计算机或设备，这是它在可移动存储设备上广泛使用的原因之一。 支持大容量存储设备：FAT32支持最大容量为2TB的存储设备，因此它被广泛用于外部硬盘、闪存驱动器等大容量存储设备上。 支持大文件：FAT32支持最大文件大小为4GB，这是相对较小的文件大小限制，但对于大多数常见文件类型而言足够了。 简单：FAT32是一个相对简单的文件系统，易于实现和使用。 总之，FAT32是一种简单、兼容性强、可移植性好的文件系统，它被广泛应用于可移动存储设备、外部硬盘和其他大容量存储设备上。虽然它有一些限制，例如文件大小限制，但对于普通用户而言，它仍然是一种可靠和方便的文件系统。
EXFAT # exFAT（Extended File Allocation Table）是一种用于可移动存储设备的文件系统，由Microsoft开发，它是FAT文件系统的一种升级版本。exFAT支持更大的文件和存储设备容量，也具有更好的兼容性。以下是exFAT的一些特点：
大文件支持：exFAT支持极大的文件大小，最大文件大小为16EB，这是比FAT32更高的限制，对于处理大型媒体文件等需要大文件支持的应用程序非常有用。 大容量支持：exFAT支持极大的存储设备容量，最大容量为128PB，这使得它非常适合用于大型存储设备，如高容量的移动硬盘或SD卡。 兼容性：exFAT文件系统可以在Windows、Mac OS X、Linux和其他操作系统上进行访问和读取，这使得它非常适合在跨平台环境中使用。 文件系统简单：exFAT文件系统比NTFS更简单，因此更易于实现和使用。 文件碎片化更少：与FAT32相比，exFAT可以减少文件碎片化的问题，从而提高文件访问速度。 总之，exFAT是一种高效、可靠、具有更大文件和存储设备容量限制的文件系统，特别适合用于可移动存储设备，如SD卡、U盘等。由于其更好的兼容性，它在跨平台数据共享和数据传输方面非常有用。
EXT4 # EXT4是Linux操作系统中使用的一种高性能的日志式文件系统。它是EXT3文件系统的后继版本，支持更大的文件和文件系统容量，并且具有更好的文件系统安全性和稳定性。以下是EXT4的一些特点：
支持大文件和大容量：EXT4支持极大的文件和文件系统容量，最大文件大小为16TB，最大文件系统容量为1EB，这使得它非常适合于处理大型数据库和媒体文件等应用程序。 快速的文件系统检查和修复：EXT4引入了一个称为ext4fsck的新工具，它可以更快地检查和修复文件系统错误，这可以大大减少系统恢复的时间。 可靠性和稳定性：EXT4使用日志式文件系统技术，它记录文件系统操作，可以在文件系统崩溃或意外断电等情况下恢复数据。此外，EXT4还使用了额外的检查和纠正功能，可以减少数据损坏和丢失的可能性。 高性能：EXT4的读取和写入速度比EXT3更快，它采用了新的文件分配方式，提高了文件系统的性能，特别是在处理大型文件和大容量数据的情况下。 支持多种操作系统：EXT4文件系统可以在Linux、BSD和其他一些操作系统上进行访问和读取，这使得它非常适合在跨平台环境中使用。 总之，EXT4是一种高性能、可靠和稳定的文件系统，支持大文件和大容量，特别适合于处理大型数据库和媒体文件等应用程序。它的快速检查和修复功能可以提高文件系统的可用性，同时它也具有更好的数据安全性和稳定性。由于它可以在多种操作系统上进行访问和读取，它在跨平台环境中的使用也越来越广泛。</description>
    </item>
    
    <item>
      <title>Strconv</title>
      <link>https://chain-code.github.io/docs/golang/package/strconv/</link>
      <pubDate>Fri, 05 May 2023 11:36:52 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/strconv/</guid>
      <description>strconv 字符串和数字相互转换 # 需引入&amp;quot;strconv&amp;quot;包
string到int
int,err:=strconv.Atoi(string) string到int64
int64, err := strconv.ParseInt(string, 10, 64) int到string
string:=strconv.Itoa(int) int64到string
string:=strconv.FormatInt(int64,10) 10进制转16进制
strconv.FormatInt(int64, 16) 想保留前面的数
func main() {decimal := 2hex := fmt.Sprintf(&amp;#34;%02x&amp;#34;, decimal)fmt.Println(hex) // 输出：02} 字符串转float64
func ParseFloat(s string, bitSize int) (float64, error) func main() {s := &amp;#34;3.14&amp;#34;f, err := strconv.ParseFloat(s, 64) //32if err != nil {fmt.Println(&amp;#34;解析错误:&amp;#34;, err)return}fmt.Printf(&amp;#34;转换成功: %.2f (类型: %T)\n&amp;#34;, f, f) // 输出: 3.</description>
    </item>
    
    <item>
      <title>Sort</title>
      <link>https://chain-code.github.io/docs/golang/package/sort/</link>
      <pubDate>Fri, 05 May 2023 11:36:34 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/sort/</guid>
      <description>sort —— 排序算法 # sort包提供了对[]int切片、[]float64切片和[]string切片完整支持，主要包括：
对基本数据类型切片的排序支持 基本数据元素查找 判断基本数据类型切片是否已经排好序 对排好序的数据集合逆序 对[]int切片排序是更常使用sort.Ints()，而不是直接使用IntSlice类型。
s := []int{5, 2, 6, 3, 1, 4} // 未排序的切片数据sort.Ints(s)fmt.Println(s) //将会输出[1 2 3 4 5 6] 如果要使用降序排序，显然要用前面提到的Reverse()方法：
s := []int{5, 2, 6, 3, 1, 4} // 未排序的切片数据sort.Sort(sort.Reverse(sort.IntSlice(s)))fmt.Println(s) //将会输出[6 5 4 3 2 1] 如果要查找整数x在切片a中的位置，相对于前面提到的Search()方法，sort包提供了SearchInts():
func SearchInts(a []int, x int) int 注意，SearchInts()的使用条件为：切片a已经升序排序
s := []int{5, 2, 6, 3, 1, 4} // 未排序的切片数据sort.Ints(s) //排序后的s为[1 2 3 4 5 6]fmt.</description>
    </item>
    
    <item>
      <title>Strings</title>
      <link>https://chain-code.github.io/docs/golang/package/strings/</link>
      <pubDate>Fri, 05 May 2023 11:36:12 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/strings/</guid>
      <description>strings.Split # func Split(s, sep string) []string strings.Split 函数用于通过指定的分隔符切割字符串，并返回切割后的字符串切片。
func main() {fmtPrintln(stringsSplit(&amp;#34;Linux, Unix, Windows, Android&amp;#34;, &amp;#34;, &amp;#34;))fmtPrintln(stringsSplit(&amp;#34; Linux is very very very good! &amp;#34;, &amp;#34; &amp;#34;))}输出：返回的是字符串数组。[Linux Unix Windows Android][ Linux is very very very good! ] strings.Split(s, sep)1s：待分割的字符串（字符串类型的参数）sep：分隔符 （字符串类型的参数）返回值：返回一个字符串切片。 strings.Join # func Join(elems []string, sep string) string 作用：使用 sep 作为分隔符，将elems 中的所有字符连接起来：
func main() { elems := []string{&amp;#34;I&amp;#34;, &amp;#34;like&amp;#34;, &amp;#34;golang&amp;#34;, &amp;#34;!</description>
    </item>
    
    <item>
      <title>fts</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/fts/</link>
      <pubDate>Sun, 30 Apr 2023 16:16:50 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/fts/</guid>
      <description>Fts # fts是sqlite的一个支持全文检索的扩展模块，是个五脏俱全的全文搜索引擎
拼音数据的来源是 https://github.com/mozillazg/pinyin-data
项目地址：https://github.com/wangfenjin/simple
以GO为例，其他语言的操作方式可能不一样，但理论是通用的。sqlite的fts5模块默认是不开启的，在编译时要加上fts5编译标签。
假如原表名是 student_info 其中有 id name age addr 四个字段
需要作为搜索目标的字段是 name addr
需要搜索出的字段有 name age addr
下载 # 从发布页面下载预编译的链接库或自己编译，用预编译的版本方便快速测试，实际发布时要测一下目标平台是否表现正常。 比如我们遇到的预编译版在win7上缺少依赖库，在debian10上glibc版本过低，需要自己编译。
发布页地址：https://github.com/wangfenjin/simple/releases
注册驱动 # sql.Register( &amp;#34;sqlite3_simple&amp;#34;, &amp;amp;sqlite3.SQLiteDriver{ Extensions: []string{ &amp;#34;simple.dll&amp;#34;, }, }, ) 在打开数据库时选择上面注册的驱动
db, err := gorm.Open(sqlite.Dialector{ DriverName: &amp;#34;sqlite3_simple&amp;#34;, DSN: &amp;#34;test_fts5.db&amp;#34;, }) 创建fts5虚拟表 # 虚拟表是fts5需要的 具体如何操作自行查阅fts5文档，关键点是通过tokenize=&amp;quot;simple&amp;quot;指定分词器。 文档地址:https://www.sqlite.org/fts5.html
CREATE VIRTUAL TABLE IF NOT EXISTS student_info_fts USING fts5( name, -- 需要分词搜索的字段 addr, -- 需要分词搜索的字段 age UNINDEXED, -- 无需分词的字段 避免回表 content=student_info, -- 原表名 content_rowid=id, -- 原表id tokenize=&amp;#34;simple&amp;#34; -- 使用simple分词器 ) 创建触发器 # 通过触发器来同步修改索引表是推荐的方案，如果表的更新频次比较高，应该使用定时同步刷新的机制，而不是触发器。</description>
    </item>
    
    <item>
      <title>Sqlite</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/sqlite/</link>
      <pubDate>Sun, 30 Apr 2023 16:16:50 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/sqlite/</guid>
      <description>SQLite # SQLite 是一个开源的嵌入式关系数据库，实现自包容、零配置、支持事务的 SQL 数据库引擎。其特点是高度便携、使用方便、结构紧凑、高效、可靠。 与其他数据库管理系统不同，SQLite 的安装和运行非常简单，在大多数情况下，只要确保 SQLite 的二进制文件存在即可开始创建、连接和使用数据库。如果您正在寻找一个嵌入式数据库项目或解决方案，SQLite 是绝对值得考虑。</description>
    </item>
    
    <item>
      <title>Xorm</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/xorm/</link>
      <pubDate>Sun, 30 Apr 2023 16:16:50 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/xorm/</guid>
      <description>https://lunny.gitbooks.io/xorm-manual-zh-cn/content/chapter-01/index.html
https://xorm.io/docs/chapter-01/readme/
创建Orm引擎 # 在xorm里面，可以同时存在多个Orm引擎，一个Orm引擎称为Engine，一个Engine一般只对应一个数据库。Engine通过调用xorm.NewEngine生成，如：
import ( _ &amp;#34;github.com/go-sql-driver/mysql&amp;#34; &amp;#34;github.com/go-xorm/xorm&amp;#34; ) var engine *xorm.Engine func main() { var err error engine, err = xorm.NewEngine(&amp;#34;mysql&amp;#34;, &amp;#34;root:[email protected]/test?charset=utf8&amp;#34;) } or
import ( _ &amp;#34;github.com/mattn/go-sqlite3&amp;#34; &amp;#34;github.com/go-xorm/xorm&amp;#34; ) var engine *xorm.Engine func main() { var err error engine, err = xorm.NewEngine(&amp;#34;sqlite3&amp;#34;, &amp;#34;./test.db&amp;#34;)//数据库文件路径 } 一般情况下如果只操作一个数据库，只需要创建一个engine即可。engine是GoRutine安全的。
创建完成engine之后，并没有立即连接数据库，此时可以通过engine.Ping()来进行数据库的连接测试是否可以连接到数据库。另外对于某些数据库有连接超时设置的，可以通过起一个定期Ping的Go程来保持连接鲜活。
对于有大量数据并且需要分区的应用，也可以根据规则来创建多个Engine，比如：
var err error for i:=0;i&amp;lt;5;i++ { engines[i], err = xorm.NewEngine(&amp;#34;sqlite3&amp;#34;, fmt.Sprintf(&amp;#34;./test%d.db&amp;#34;, i)) } engine可以通过engine.Close来手动关闭，但是一般情况下可以不用关闭，在程序退出时会自动关闭。
NewEngine传入的参数和sql.Open传入的参数完全相同，因此，在使用某个驱动前，请查看此驱动中关于传入参数的说明文档。以下为各个驱动的连接符对应的文档链接：
sqlite3 mysql dsn mymysql postgres 在engine创建完成后可以进行一些设置，如：</description>
    </item>
    
    <item>
      <title>Vite&#43;Vue快速搭建项目</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vite&#43;vue%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E9%A1%B9%E7%9B%AE/</link>
      <pubDate>Wed, 19 Apr 2023 20:07:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vite&#43;vue%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E9%A1%B9%E7%9B%AE/</guid>
      <description>Vite+Vue快速搭建项目 # npm init vite@latest 输入项目名，选择Vue 选择TypeScript 回车 &amp;gt; npx&amp;gt; create-vite│◇ Project name:│ ApmDataAnalytics│◇ Package name:│ apmdataanalytics│◇ Select a framework:│ Vue│◇ Select a variant:│ TypeScript│◇ Use rolldown-vite (Experimental)?:│ No│◇ Install with npm and start now?│ Yes 再运行
npm install -y 快速创建一个默认的包信息
npm init -y -D开发环境中的依赖，加载vite
npm i vite -D 生成&amp;quot;devDependencies&amp;quot;，开发环境下的依赖</description>
    </item>
    
    <item>
      <title>Vue3</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vue3/</link>
      <pubDate>Wed, 19 Apr 2023 20:07:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vue3/</guid>
      <description> Vue3 # watch函数 # watch函数：这是Vue 3提供的一个响应式API，用于观察和响应reactive状态的变化。
watch(() =&amp;gt; store.page,(newPage, oldPage) =&amp;gt; {getRawItem(store.page)} a. 第一个参数：() =&amp;gt; store.page
这是一个箭头函数，返回被监视的值（store.page）。 每当store.page发生变化时，这个函数会被重新执行。 b. 第二个参数：(newPage, oldPage) =&amp;gt; { getRawItem(store.page) }
这是一个回调函数，当被监视的值变化时会被调用。 newPage参数代表store.page的新值。 oldPage参数代表store.page的旧值。 函数体内调用了getRawItem(store.page)。 </description>
    </item>
    
    <item>
      <title>Vue环境搭建</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vue%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Wed, 19 Apr 2023 20:07:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/vue%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</guid>
      <description>Vue环境搭建 # VueJS 是一个开源的渐进式 JavaScript 框架，用于开发交互式 Web 界面。
它是用于简化 Web 开发的着名框架之一，VueJS 专注于视图层。它可以很容易地集成到大型项目前端开发没有任何问题。
Vue 中文网：https://cn.vuejs.org/ 安装 node.js # 安装 # 下载地址：https://nodejs.org/en/download/
windows 版一路往下点
安装完成：
$ npm -v 6.14.6 设置路径 # 设置 nodejs prefix（全局）和 cache（缓存）路径
在 nodejs 安装路径下，新建 node_global 和 node_cache 两个文件夹
设置缓存文件夹
npm config set cache &amp;#34;C:\Program Files\nodejs\node_cache&amp;#34; 设置全局模块存放路径
npm config set prefix &amp;#34;C:\Program Files\nodejs\node_global&amp;#34; 设置成功后，之后用命令 npm install XXX -g 安装以后模块就在C:\Program Files\nodejs\node_global 里
安装镜像 # 基于 Node.js 安装 cnpm（淘宝镜像）
npm install -g cnpm --registry=https://registry.</description>
    </item>
    
    <item>
      <title>Web Socket</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/websocket/</link>
      <pubDate>Wed, 19 Apr 2023 20:07:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/websocket/</guid>
      <description>WebSocket # WebSocket - Web API 接口参考 |多核 (mozilla.org)
WebSocket API是一种先进的技术，可以在用户的浏览器和服务器之间打开双向交互通信会话。使用此 API，您可以向服务器发送消息并接收事件驱动的响应，而无需轮询服务器以获取答复。
官方示例 # Chat Example
官方示例可参照synk项目结合gin框架
官方介绍
main.go # package main import ( &amp;#34;flag&amp;#34; &amp;#34;log&amp;#34; &amp;#34;net/http&amp;#34; ) var addr = flag.String(&amp;#34;addr&amp;#34;, &amp;#34;:8080&amp;#34;, &amp;#34;http service address&amp;#34;) func serveHome(w http.ResponseWriter, r *http.Request) { log.Println(r.URL) if r.URL.Path != &amp;#34;/&amp;#34; { http.Error(w, &amp;#34;Not found&amp;#34;, http.StatusNotFound) return } if r.Method != http.MethodGet { http.Error(w, &amp;#34;Method not allowed&amp;#34;, http.StatusMethodNotAllowed) return } http.ServeFile(w, r, &amp;#34;home.html&amp;#34;) } func main() { flag.</description>
    </item>
    
    <item>
      <title>WebStorm-debug</title>
      <link>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/webstorm-debug/</link>
      <pubDate>Wed, 19 Apr 2023 20:07:07 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%89%8D%E7%AB%AF/webstorm-debug/</guid>
      <description> WebStorm-debug # 1. 运行项目，查看运行url # 比如我的测试项目使用npm run serve运行后展示的端口是5174
2. 配置JavaScript Debug # 3. Debug # </description>
    </item>
    
    <item>
      <title>Reflect</title>
      <link>https://chain-code.github.io/docs/golang/package/reflect/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:56 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/reflect/</guid>
      <description>Reflect # reflect包是Go语言标准库中的一个包，它提供了一组功能，允许我们在运行时动态地查看和操作Go程序中的变量、函数和类型。通过使用reflect包，我们可以以一种通用的方式处理和操作各种类型的值，而无需知道它们的具体类型。
反射三大定律 # Go 语言中的反射，其归根究底都是在实现三大定律：
Reflection goes from interface value to reflection object. Reflection goes from reflection object to interface value. To modify a reflection object, the value must be settable. 我们将针对这核心的三大定律进行介绍和说明，以此来理解 Go 反射里的各种方法是基于什么理念实现的。
第一定律 # 反射的第一定律是：“反射可以从接口值（interface）得到反射对象”。
示例代码：
func main() {var x float64 = 3.4fmt.Println(&amp;#34;type:&amp;#34;, reflect.TypeOf(x))} 输出结果：
type: float64 可能有读者就迷糊了，我明明在代码中传入的变量 x，他的类型是 float64。怎么就成从接口值得到反射对象了。
其实不然，虽然在代码中我们所传入的变量基本类型是 float64，但是 reflect.TypeOf 方法入参是 interface{}，本质上 Go 语言内部对其是做了类型转换的。这一块会在后面会进一步展开说明。
第二定律 # 反射的第二定律是：“可以从反射对象得到接口值（interface）”。其与第一条定律是相反的定律，可以是互相补充了。
示例代码：
func main() {vo := reflect.</description>
    </item>
    
    <item>
      <title>context</title>
      <link>https://chain-code.github.io/docs/golang/package/context/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/context/</guid>
      <description>context # Context本质 # golang标准库里Context实际上是一个接口（即一种编程规范、 一种约定）。
type Context interface { Deadline() (deadline time.Time, ok bool) Done() &amp;lt;-chan struct{} Err() error Value(key any) any } 通过查看源码里的注释，我们得到如下约定：
1、Done()函数返回一个只读管道，且管道里不存放任何元素(struct{})，所以用这个管道就是为了实现阻塞
2、Deadline()用来记录到期时间，以及是否到期。
3、Err()用来记录Done()管道关闭的原因，比如可能是因为超时，也可能是因为被强行Cancel了。
4、Value()用来返回key对应的value，你可以想像成Context内部维护了一个map。
Context实现 # go源码里提供了Context接口的一个具体实现，遗憾的是它只是一个空的Context，什么也没做。
type emptyCtx int func (*emptyCtx) Deadline() (deadline time.Time, ok bool) { return } func (*emptyCtx) Done() &amp;lt;-chan struct{} { return nil } func (*emptyCtx) Err() error { return nil } func (*emptyCtx) Value(key any) any { return nil } emptyCtx以小写开头，包外不可见，所以golang又提供了Background和TODO这2个函数让我们能获取到emptyCtx。</description>
    </item>
    
    <item>
      <title>diskqueue</title>
      <link>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/diskqueue/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/diskqueue/</guid>
      <description>diskqueue # github.com/nsqio/go-diskqueue 是 NSQ（一个实时分布式消息平台）官方团队开发的一个 Go 语言库，主要用于实现基于磁盘的持久化队列。它的核心作用是为消息系统提供可靠的消息存储和异步处理能力，尤其适合需要高吞吐、持久化和故障恢复的场景。
https://github.com/nsqio/go-diskqueue?tab=readme-ov-file
nsq # https://github.com/nsqio
NSQ是一个实时分布式消息传递平台，旨在大规模运行，每天处理数十亿条消息。
它支持分布式和去中心化拓扑，消除单点故障，实现容错和高可用性，并提供可靠的消息传递保证。请参阅功能和保障。
NSQ操作简单，易于配置和部署（所有参数均可在命令行中指定，且编译后的二进制文件无运行时依赖）。为了实现最大的灵活性，它与数据格式无关（消息可以是 JSON、MsgPack、Protocol Buffers 或其他任何格式）。官方的 Go 和 Python 库开箱即用（以及许多其他客户端库），如果您有兴趣构建自己的客户端，可以参考协议规范。
案例： # 实现队列缓冲，当有大量的数据需要缓存，并且需要保证前后顺序一致，就能用到这个
// Helper for serialization (using gob as an example)func serializeDetectionResult(data *file2.DetectionResult) ([]byte, error) {var buf bytes.Bufferencoder := gob.NewEncoder(&amp;amp;buf)if err := encoder.Encode(data); err != nil {return nil, fmt.Errorf(&amp;#34;failed to serialize DetectionResult: %w&amp;#34;, err)}return buf.Bytes(), nil}func deserializeDetectionResult(data []byte) (*file2.</description>
    </item>
    
    <item>
      <title>filepath</title>
      <link>https://chain-code.github.io/docs/golang/package/filepath/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/filepath/</guid>
      <description>filepath # Base返回路径的最后一个元素 # func main() { path := filepath.Base(&amp;#34;/path/to/file.txt&amp;#34;) fmt.Println(path) // 输出: file.txt } Clean清理路径，去掉冗余元素 # func main() { path := filepath.Clean(&amp;#34;/path/../to/file.txt&amp;#34;) fmt.Println(path) // 输出: /to/file.txt } Abs返回路径的绝对路径 # func main() { path, err := filepath.Abs(&amp;#34;relative/path/to/file&amp;#34;) if err != nil { log.Fatal(err) } fmt.Println(path) //C:\Users\...\go\src\VideoForensic\test\relative\path\to\file } Dir除去最后一个元素 # func main() { path := filepath.Dir(&amp;#34;/path/to/file.txt&amp;#34;) fmt.Println(path) // 输出: /path/to } Ext返回路径的扩展名 # func main() { ext := filepath.Ext(&amp;#34;/path/to/file.txt&amp;#34;) fmt.Println(ext) // 输出: .</description>
    </item>
    
    <item>
      <title>gofpdf</title>
      <link>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/gofpdf/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93/gofpdf/</guid>
      <description>gofpdf # gofpdf 包实现了一个 PDF 文档生成器，它对文本、绘图和图像具有高级支持。
https://github.com/jung-kurt/gofpdf</description>
    </item>
    
    <item>
      <title>math</title>
      <link>https://chain-code.github.io/docs/golang/package/math/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/math/</guid>
      <description>Math # Abs 绝对值 # func Abs(x float64) float64：返回指定数字的绝对值。适用于负数和正数，确保结果为非负数。
x := -5.5fmt.Println(math.Abs(x)) // 输出: 5.5 Pow 幂运算 # func Pow(x, y float64) float64：计算x的y次幂。常用于指数运算。
base := 2.0exponent := 3.0fmt.Println(math.Pow(base, exponent)) // 输出: 8.0 Sqrt 平方根 # func Sqrt(x float64) float64：返回x的平方根。适用于非负数。
num := 16.0fmt.Println(math.Sqrt(num)) // 输出: 4.0 Sin 正弦 # func Sin(x float64) float64：返回x的正弦值，x以弧度为单位。用于三角函数计算。
radian := math.Pi / 2fmt.Println(math.Sin(radian)) // 输出: 1.0 Cos 余弦 # func Cos(x float64) float64：返回x的余弦值，x以弧度为单位。</description>
    </item>
    
    <item>
      <title>Time</title>
      <link>https://chain-code.github.io/docs/golang/package/time/</link>
      <pubDate>Wed, 19 Apr 2023 20:05:22 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/time/</guid>
      <description>Time # After 在指定的时间间隔后发送当前时间 # After(d Duration) &amp;lt;-chan Time：返回一个通道，该通道将在指定的时间间隔后发送当前时间。可以用它来实现定时器
例如，程序需要等待一段时间后再执行某个操作，可以使用After()函数来实现。示例代码：
select { case &amp;lt;-time.After(5 * time.Second): fmt.Println(&amp;#34;5秒后执行&amp;#34;) } AfterFunc 定时器 # func AfterFunc(d Duration, f func()) *Timer创建一个新的定时器，并在定时器触发时调用指定的回调函数f。参数d是一个time.Duration类型的值，表示定时器的持续时间。返回值是一个指向Timer结构体的指针。
func main() { t := time.AfterFunc(3*time.Second, func() { fmt.Println(&amp;#34;定时器已触发&amp;#34;) }) fmt.Println(&amp;#34;定时器已启动&amp;#34;) time.Sleep(4 * time.Second) t.Stop() fmt.Println(&amp;#34;定时器已停止&amp;#34;) } Sleep 延迟 # Sleep(d Duration)：使当前程序暂停指定的时间间隔。可以用它来实现程序的延迟操作，例如，程序需要在某个操作之后暂停一段时间再继续执行，可以使用Sleep()函数来实现。
示例代码：
fmt.Println(&amp;#34;开始执行&amp;#34;) time.Sleep(2 * time.Second) fmt.Println(&amp;#34;暂停2秒后继续执行&amp;#34;) Tick # Tick(d Duration) &amp;lt;-chan Time：返回一个通道，该通道会定期发送时间，每个时间间隔为指定的时间间隔。可以用它来实现定时器，例如，程序需要每隔一段时间执行某个操作，可以使用Tick()函数来实现。
示例代码：
for t := range time.Tick(2 * time.Second) { fmt.</description>
    </item>
    
    <item>
      <title>Sync</title>
      <link>https://chain-code.github.io/docs/golang/package/sync/</link>
      <pubDate>Thu, 24 Nov 2022 15:24:08 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/package/sync/</guid>
      <description>go中的sync包
在Go语言中，除了使用channel进行goroutine之间的通信和同步操作外，还可以使用syne包下的并发工具。
并发工具类 说明 Mutex 互斥锁 RWMutex 读写锁 WaitGroup 并发等待组 Map 并发安全字典 Cond 同步等待条件 Once 只执行一次 Pool 临时对象池 临界区 # 有时候在Go代码中可能会存在多个goroutine同时操作一个资源区（临界区），这种情况会发生竟态问题（数据竟态）。
临界区：当程序并发地运行时，多个 [Go 协程]不应该同时访问那些修改共享资源的代码。这些修改共享资源的代码称为临界区。
package main import ( &amp;#34;fmt&amp;#34; &amp;#34;sync&amp;#34; ) var x = 10 var wg sync.WaitGroup func add() { for i := 0; i &amp;lt; 5000; i++ { x = x + 1 } wg.Done() } func main() { wg.Add(2) go add() go add() wg.Wait() fmt.Println(x) } 代码中我们开启了两个goroutine去累加变量x的值，这两个goroutine在访问和修改x变量的时候就会存在数据竞争，导致最后的结果与期待的不符。</description>
    </item>
    
    <item>
      <title>MP4格式分析</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/mp4%E6%A0%BC%E5%BC%8F%E5%88%86%E6%9E%90/</link>
      <pubDate>Wed, 02 Nov 2022 20:37:13 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/mp4%E6%A0%BC%E5%BC%8F%E5%88%86%E6%9E%90/</guid>
      <description>MP4格式分析 # MP4(MPEG-4 Part 14)是一种常见的多媒体容器格式，它是在“ISO/IEC 14496-14”标准文件中定义的，属于MPEG-4的一部分，是“ISO/IEC 14496-12(MPEG-4 Part 12 ISO base media file format)”标准中所定义的媒体格式的一种实现，后者定义了一种通用的媒体文件结构标准。MP4是一种描述较为全面的容器格式，被认为可以在其中嵌入任何形式的数据，各种编码的视频、音频等都不在话下，不过我们常见的大部分的MP4文件存放的AVC(H.264)或MPEG-4(Part 2)编码的视频和AAC编码的音频。MP4格式的官方文件后缀名是“.mp4”，还有其他的以mp4为基础进行的扩展或者是缩水版本的格式，包括：M4V, 3GP, F4V等。
mp4是由一个个“box”组成的，大box中存放小box，一级嵌套一级来存放媒体信息。box的基本结构是：
size指明了整个box所占用的大小，包括header部分。如果box很大(例如存放具体视频数据的mdat box)，超过了uint32的最大数值，size就被设置为1，并用接下来的8位uint64来存放大小。
大部分mp4文件没有那么多的box类型，下图就是一个简化了的，常见的mp4文件结构：
在部分box中，还存在version、flags字段，这样的box叫做Full Box。当box body中嵌套其他box时，这样的box叫做container box。
ftyp
在文件的开始位置，描述的文件的版本、兼容协议等
16进制：66 74 79 70
moov
这个box中不包含具体媒体数据，但包含本文件中所有媒体数据的宏观描述信息，moov box下有mvhd和trak box。
​ &amp;gt;&amp;gt;mvhd中记录了创建时间、修改时间、时间度量标尺、可播放时长等信息。
​ &amp;gt;&amp;gt;trak中的一系列子box描述了每个媒体轨道的具体信息。
16进制：6D 6F 6F 76
mdat
实际媒体数据
16进制：6D 64 61 74
5分钟入门MP4文件格式
一般来说，解析媒体文件，最关心的部分是视频文件的宽高、时长、码率、编码格式、帧列表、关键帧列表，以及所对应的时戳和在文件中的位置，这些信息，在mp4中，是以特定的算法分开存放在stbl box下属的几个box中的，需要解析stbl下面所有的box，来还原媒体信息。下表是对于以上几个重要的box存放信息的说明：</description>
    </item>
    
    <item>
      <title>布隆过滤器</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</link>
      <pubDate>Wed, 02 Nov 2022 20:37:13 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8/</guid>
      <description>布隆过滤器 # 布隆过滤器简介 # 布隆过滤器（Bloom Filter）是一个基于hash的概率性的数据结构，它实际上是一个很长的二进制向量，可以检查一个元素可能存在集合中，和一定不存在集合中。它的优点是空间效率高，但是有一定false positive(元素不在集合中，但是布隆过滤器显示在集合中)。
布隆过滤器原理 # 布隆过滤器就是一个长度为m个bit的bit数组，初始的时候每个bit都是0，另外还有k个hash函数。
布隆过滤器加入元素
当加入一个元素时，先用k个hash函数得到k个hash值，将k个hash值与bit数组长度取模得到个k个位置，将这k个位置对应的bit置位1。
在加入了bloom之后，再加入filter。
布隆过滤器查询元素
在布隆过滤器中查询元素比较简单，同样地，先用k个hash函数得到k个hash值，将k个hash值与bit数组长度取模得到个k个位置，然后检查这k个位置的bit是否是1。如果都是1，布隆过滤器返回这个原始存在。
布隆过滤器的false positive
查询元素中，有可能k个hash值对应的位置都已经置一，但这都是其他元素的操作，实际上这个元素并不在布隆过滤器中，这就是false positive。看下面这个例子，添加完bloom,filter后，检查cat是否在 布隆过滤器中。
布隆过滤器的false positive计算
false positive计算，有3个重要的参数。1. m表示bit数组的长度 2. k表示散列函数的个数 3. n表示插入的元素个数
布隆过滤器中，一个元素插入后，某个bit为0的概率是
(1−1/m)^k n元素插入后，某个bit为0的概率是
(1−1/m)^(nk) false positive的概率是
(1−(1−1/m)^nk)^k 因为需要的是k个不同的bit被设置成1，概率是大约是
(1−e^(−kn/m))^k 这个就是false positive的概率</description>
    </item>
    
    <item>
      <title>算法基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 02 Nov 2022 20:37:13 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/</guid>
      <description> AVL树 # 红黑树 # </description>
    </item>
    
    <item>
      <title>虚拟组网</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%99%9A%E6%8B%9F%E7%BB%84%E7%BD%91/</link>
      <pubDate>Wed, 02 Nov 2022 20:37:13 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E8%99%9A%E6%8B%9F%E7%BB%84%E7%BD%91/</guid>
      <description>虚拟组网 # 需要一个云服务器作为灯塔
https://zhw.in/post/virtual-networking/
https://github.com/slackhq/nebula?tab=readme-ov-file</description>
    </item>
    
    <item>
      <title>Linux基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/linux%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 22 Sep 2022 14:56:19 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/linux%E5%9F%BA%E7%A1%80/</guid>
      <description>Unix/Linux操作系统介绍 # Linux和Unix的联系 # UNIX系统是工作站上最常用的操作系统，它是一个多用户、多任务的实时操作系统，允许多人同时访问计算机， 并同时运行多个任务。UNIX系统具有稳定、高效、安全、方便、功能强大等诸多优点，自20世纪70年代开始便运行在许多大型和小型计算机上。
UNIX虽然是一个安全、稳定且功能强大的操作系统，但它也一直是一种大型的而且对运行平台要求很高的操作系统，只能在工作站或小型机上才能发挥全部功能，并且价格昂贵，对普通用户来说是可望而不可及的，这为后来Linux的崛起提供了机会，Linux是一个类UNIX操作系统。
Linux是免费的、不受版权制约、与UNIX兼容的操作系统。
Linux在x86架构上实现了UNIX系统的全部特性，具有多用户多任务的能力，同时保持了高效性和稳定性，Linux具有如下的优秀的特点：
开放性； 完全免费； 多用户，多任务； 设备独立性； 丰富的网络功能； 可靠的系统安全性； Unix/Linux开发应用领域 # Unix/Linux服务器
嵌入式Linux系统
桌面应用
电子政务
文件系统 # 目录和路径 # 目录 # 目录是一组相关文件的集合。
一个目录下面除了可以存放文件之外还可以存放其他目录，即可包含子目录。
在确定文件、目录位置时，DOS和Unix/Linux都采用“路径名+文件名”的方式。路径反映的是目录与目录之间的关系。
路径 # Unix/Linux路径由到达定位文件的目录组成。在Unix/Linux系统中组成路径的目录分割符为斜杠“/”，而DOS则用反斜杠“\”来分割各个目录。
路径分为绝对路径和相对路径：
绝对路径 # 绝对路径是从目录树的树根“/”目录开始往下直至到达文件所经过的所有节点目录。
下级目录接在上级目录后面用“/”隔开。
注意：绝对路径都是从“/”开始的，所以第一个字符一定是“/”。
相对路径 # 相对路径是指目标目录相对于当前目录的位置。
如果不在当前目录下，则需要使用两个特殊目录&amp;quot;.&amp;ldquo;和&amp;rdquo;..&amp;ldquo;了。目录“.”指向当前目录，而目录“..”。
Linux目录结构 # 和Windows操作系统类似，所有Unix/Linux的数据都是由文件系统按照树型目录结构管理的。而且Unix/Linux操作系统同样要区分文件的类型，判断文件的存取属性和可执行属性。
Unix/Linux也采用了树状结构的文件系统，它由目录和目录下的文件一起构成。但Unix/Linux文件系统不使用驱动器这个概念，而是使用单一的根目录结构，所有的分区都挂载到单一的“/”目录上，其结构示意图如图所示：
无论何种版本的 Linux 发行版，桌面、应用是 Linux 的外衣，文件组织、目录结构才是Linux的内心。
结构 # /：根目录，一般根目录下只存放目录，在Linux下有且只有一个根目录。所有的东西都是从这里开始。当你在终端里输入“/home”，你其实是在告诉电脑，先从/（根目录）开始，再进入到home目录。
/bin: /usr/bin: 可执行二进制文件的目录，如常用的命令ls、tar、mv、cat等。
/root：系统管理员root的家目录（宿主目录）。
/etc：系统配置文件存放的目录，不建议在此目录下存放可执行文件，重要的配置文件有 /etc/inittab、/etc/fstab、/etc/init.d、/etc/X11、/etc/sysconfig、/etc/xinetd.d。
/home：系统默认的用户家目录，新增用户账号时，用户的家目录都存放在此目录下，~表示当前用户的家目录，~edu 表示用户 edu 的家目录。
/usr：应用程序存放目录，/usr/bin 存放应用程序，/usr/share 存放共享数据，/usr/lib 存放不能直接运行的，却是许多程序运行所必需的一些函数库文件。/usr/local: 存放软件升级包。/usr/share/doc: 系统说明文件存放目录。/usr/share/man: 程序说明文件存放目录。/usr/include:存放头文件。</description>
    </item>
    
    <item>
      <title>Dockerfile</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/dockerfile/</link>
      <pubDate>Wed, 21 Sep 2022 17:04:41 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/dockerfile/</guid>
      <description>简介 # Dockerfile类似于我们学习过的脚本，将我们在上面学到的docker镜像，使用自动化的方式实现出来。
Dockerfile的作用：
找一个镜像: ubuntu 创建一个容器: docker run ubuntu 进入容器: docker exec -it 容器 命令 操作: 各种应用配置&amp;hellip;. 构造新镜像: docker commit Dockerfile 使用准则：
大: 首字母必须大写D 空: 尽量将Dockerfile放在空目录中。 单: 每个容器尽量只有一个功能。 少: 执行的命令越少越好。 Dockerfile文件内容:
首行注释信息 指令(大写) 参数 #构建镜像命令格式:docker build -t [镜像名]:[版本号][Dockerfile所在目录] #构建样例:docker build -t nginx:v0.2 /opt/dockerfile/nginx/ #参数详解:-t 指定构建后的镜像信息，/opt/dockerfile/nginx/ 则代表Dockerfile存放位置，如果是当前目录，则用 .(点)表示 快速入门 # 接下来我们快速的使用Dockerfile来基于ubuntu创建一个定制化的镜像:nginx。
#创建Dockerfile专用目录$ mkdir ./docker/images/nginx -p$ cd docker/images/nginx/ #创建Dockerfile文件 :~/docker/images/nginx$ vim Dockerfile # 构建一个基于ubuntu的docker定制镜像 # 基础镜像FROM ubuntu# 镜像作者MAINTAINER panda kstwoak47@163.</description>
    </item>
    
    <item>
      <title>必刷top101</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E5%BF%85%E5%88%B7top101/</link>
      <pubDate>Sat, 17 Sep 2022 11:40:17 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/%E5%BF%85%E5%88%B7top101/</guid>
      <description>题目来源：牛客网面试必刷TOP101
链表 # 反转链表 # 给定一个单链表的头结点pHead(该头节点是有值的，比如在下图，它的val是1)，长度为n，反转该链表后，返回新链表的表头。
数据范围： 0≤n≤10000≤n≤1000
要求：空间复杂度 O(1)O(1) ，时间复杂度 O(n)O(n) 。
如当输入链表{1,2,3}时，
经反转后，原链表变为{3,2,1}，所以对应的输出为{3,2,1}。
以上转换过程如下图所示：
输入：{1,2,3}返回值：{3,2,1} func ReverseList( pHead *ListNode ) *ListNode { if pHead==nil||pHead.Next==nil{ return pHead } p:=&amp;amp;ListNode{Val:-1,Next:pHead} //设置一个头节点，防止冲突 pHead=p p=pHead.Next q:=p for p.Next!=nil{ q=p.Next p.Next=q.Next q.Next=pHead.Next //这道题的重点在这里=头节点的下一个 pHead.Next=q } return pHead.Next } # class ListNode: # def __init__(self, x): # self.val = x # self.next = None # # 代码中的类名、方法名、参数名已经指定，请勿修改，直接返回方法规定的值即可 # # # @param head ListNode类 # @return ListNode类 # class Solution: def ReverseList(self , head: ListNode) -&amp;gt; ListNode: if not head: #注意python判断为空的方法 return head # write code here phead=ListNode phead.</description>
    </item>
    
    <item>
      <title>Git基础</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/git%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Fri, 02 Sep 2022 22:11:34 +0800</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E5%85%AB%E8%82%A1%E6%96%87/git%E5%9F%BA%E7%A1%80/</guid>
      <description>简介 # git是目前世界上最先进的分布式版本控制系统。
git的两大特点 # 版本控制：可以解决多人同时开发的代码问题，也可以解决找回历史代码的问题。
分布式：Git是分布式版本控制系统，同一个Git仓库，可以分布到不同的机器上。首先找一台电脑充当服务器的角色，每天24小时开机，其他每个人都从这个“服务器”仓库克隆一份到自己的电脑上，并且各自把各自的提交推送到服务器仓库里，也从服务器仓库中拉取别人的提交。可以自己搭建这台服务器，也可以使用GitHub网站。
安装与配置 # brew install git 创建一个版本库 # 新建一个目录git_test，在git_test目录下创建一个版本库，命令如下：
git init 可以看到在git_test目录下创建了一个.git隐藏目录，这就是版本库目录。
版本创建与回退 # 使用 # 在git_test目录下创建一个文件code.txt，编辑内容如下：
使用如下两条命令可以创建一个版本：
git add code.txtgit commit –m &amp;#39;版本1&amp;#39; 添加身份标识（git不做检查）
git config --global user.email &amp;#34;you@example.com&amp;#34;git config --global user.name &amp;#34;Your Name&amp;#34; 然后再执行git commit -m ‘版本一’
使用如下命令可以查看版本记录：
git log 继续编辑code.txt，在里面增加一行。
使用如下命令再创建一个版本并查看版本记录：
现在若想回到某一个版本，可以使用如下命令：
git reset --hard HEAD^ 其中HEAD表示当前最新版本，HEAD^表示当前版本的前一个版本,HEAD^^表示当前版本的前前个版本，也可以使用HEAD~1表示当前版本的前一个版本,HEAD~100表示当前版本的前100版本。
现在若觉得想回到版本1，可以使用如下命令：
执行命令后使用git log查看版本记录，发现现在只能看到版本1的记录，cat code.txt查看文件内容，现在只有一行，也就是第一个版本中code.txt的内容。
假如我们现在又想回到版本2，这个时候怎么办？
可以使用如下命令：
git reset --hard 版本号 从上面可以看到版本2的版本号为：</description>
    </item>
    
    <item>
      <title>个人博客搭建Hugo</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/2022-08-27-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BAhugo/</link>
      <pubDate>Sat, 27 Aug 2022 12:11:30 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/2022-08-27-%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BAhugo/</guid>
      <description>基于Macbook M2芯片（主要原因是换电脑了，同时自己主学go语言，于是打算将Hexo换成Hugo，练练手)
https://copyfuture.com/blogs-details/20191229203259169ljtxcq9vmlzjyvfhttp://scarletsky.github.io/2019/05/02/migrate-hexo-to-hugo/https://www.tomczhen.com/2019/06/04/getting-start-blog-with-hugo/https://lequ7.com/guan-yu-hugo-bo-ke-qian-yi-zhi-lu-cong-hexo-huan-cheng-hugo.htmlhttps://blog.csdn.net/hqweay/article/details/101233371 搭建过程从头开始
环境安装 # 安装Homebrew # /bin/zsh -c &amp;#34;$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)&amp;#34; 根据提示安装Git
根据提示往下选择
运行source /Users/wangb/.zprofile 运行brew doctor根据提示处理现有的问题
brew doctor 安装Golang # 查看可安装的golang版本
brew search go //最好使用手动安装，m2系列brew安装的go会出一些小问题 没找到什么原因 安装go环境：
brew install go@1.18//改成你喜欢的版本号 在.zshrc 文件中追加配置
vim ~/.zshrc 在文件最后输入一下内容：
export GOROOT=/usr/local/goexport GOPATH=$HOME/goexport GOBIN=$GOPATH/binexport PATH=$PATH:$GOROOT/bin:$GOBINexport GO111MODULE=on 按 Esc 键退回一般模式，然后输入 :wq 命令并回车保存退出文件。再使用以下命令使变量立即生效。
source .zshrc 由于Golang的官方代理源速度慢有时候会出现包不能下载的情况，我们使用以下命令把代理源设置为国内的代理源。
go env -w GOPROXY=https://goproxy.cn,direct 使用以下命令查看Golang版本信息。
go version go1.16.4 linux/amd64 使用以下命令查看Golang环境变量配置。</description>
    </item>
    
    <item>
      <title>区块链安全基础</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2022-08-15-%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AE%89%E5%85%A8%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Mon, 15 Aug 2022 10:16:44 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2022-08-15-%E5%8C%BA%E5%9D%97%E9%93%BE%E5%AE%89%E5%85%A8%E5%9F%BA%E7%A1%80/</guid>
      <description> 双花攻击 # 双花攻击(double spend attack)又叫双重消费攻击。就是一笔资金，攻击者通过不停发起和撤销交易，将一定数额的代币反复在账号之间转账实现获利。
对于双花问题，区块链网络是这么应对的：
1、每笔交易都需要先确认对应比特币之前的状态，如果它之前已经被标记为花掉，那么新的交易会被拒绝。
2、如果先发起一笔交易，在它被确认前，也就是这个时间段的交易还未被记账成区块block时，进行矛盾的第二笔交易，那么在记账时，这些交易会被拒绝。
如果诈骗者可以把第一笔交易向一半网络进行广播，把第二笔交易向另一半网络广播，然后两边正好有两个矿工几乎同时取得记账权，把各自的block发布给大家的话（这个概率很低），网络是不会混乱的。
区块链的规则是这样的：先选择任意一个账本都可以，这时候原来统一的账本出现了分叉：
但是在两个账本中各只有一笔交易，诈骗者不会有好处。接下来，下一个矿工选择在A基础上继续记账的话，A分支就会比B分支更长，根据区块链的规则，最长的分支会被认可，短的分支会被放弃，账本还是会回归为一个，交易也只有一笔有效：
那么这个诈骗犯会这么做：如果是A分支被认可（B也一样），相应交易确认，拿到商品之后，立刻自己变身矿工，争取到连续两次记账权，然后在B分支上连加两个block：
于是B分支成为认可的分支，A被舍弃，A分支中的交易不再成立，但他已经拿到商品，诈骗成功。
在B分支落后的情况下要强行让它超过A分支，其实是挺难的，假设诈骗者掌握了全网1%的计算能力，那么他争取到记账权的概率就是1%，两次就是10的负4次方。但这个概率还没有太低。
如果诈骗者算力占据绝对优势，那么，即使落后很多，他追上也只是时间问题，这就是比特币的“51%攻击”，也就能实现双花攻击了。
区块链网络是一个分布式系统，没有一个绝对的控制中心能够监控整个系统，自然很难发现哪个节点可能会控制超过51%算力。而当某个节点掌控超过51%算力，并且对区块链网络系统进行双花攻击时，人们能够做的仅是让合作的交易所暂时提升交易确认次数。但这并不能从根本上阻止攻击者，只不过提升了其攻击成本。
DDos攻击 # </description>
    </item>
    
    <item>
      <title>共识算法基础</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sun, 14 Aug 2022 11:07:25 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80/</guid>
      <description>PoW # 概念 # PoW（工作量证明，Proof of Work），比特币，俗称挖矿。Pow是指系统为达到某一目标而设置的度量方法。简单理解就是一份证明，用来确认你做过一定量的工作。监测工作的整个过程通常是极为低效的，而通过对工作的结果进行认证来证明完成了相应的工作量，则是一种非常高效的方式。
工作量证明（Pow）通过计算一个数值（nonce），使得拼凑上交易数据后内容的Hash值满足规定的上限。在结点成功找到满足的Hash值之后，会马上对全网进行广播打包区块，网络的结点收到广播打包区块，会立刻对其进行验证。
如何才能创建一个新区块呢？通过解决一个问题：即找到一个nonce值，使得新区块头的哈希值小于某个指定的值，即区块头结构中的“难度目标”。
如果验证通过，则表明已经有结点成功解谜，自己就不再竞争当前区块打包，而是选择接受这个区块，记录到自己的账本中，然后进行下一个区块的竞争。
假如结点有任何的作弊行为，都会导致网络的结点验证不通过，直接丢弃其打包的区块，这个区块就无法记录到总帐本中，作弊的节点耗费的成本就白费了，因此在巨大的挖矿成本下，也使得矿工自觉自愿的遵守比特币系统的共识协议，也就确保了整个系统的安全。
父区块头哈希值：前一区块的哈希值，使用SHA256(SHA256(父区块头))计算。占32字节 版本：区块版本号，表示本区块遵守的验证规则。占4字节 时间戳：该区块产生的近似时间，精确到秒的UNIX时间戳，必须严格大于前11个区块时间的中值，同时全节点也会拒绝那些超出自己2个小时时间戳的区块。占4字节 难度：该区块工作量证明算法的难度目标，已经使用特定算法编码。占4字节 随机数(Nonce)：为了找到满足难度目标所设定的随机数，为了解决32位随机数在算力飞升的情况下不够用的问题，规定时间戳和coinbase交易信息均可更改，以此扩展nonce的位数。占4字节 Merkle根：该区块中交易的Merkle树根的哈希值，同样采用SHA256(SHA256())计算。占32字节
如此，细心的同学会发现，区块头总共占了80字节。
区块体除了筹币交易记录（由一棵Merkle二叉树组成)外，还有一个交易计数。
Pow工作量证明的三要素 # 工作机制
为了使区块链交易数据记录在区块链上并在一定时间内达到一致（共识），Pow提供了一种思路，即所有区块链的网络节点参与者进行竞争记账，所谓竞争记账是指，如果想生成一个新的区块并写入区块链，必须解出比特币网络出的工作量证明谜题，谁先解出答案，谁就活的记账权利，然后开始记账并将解出的答案和交易记录广播给其他节点进行验证，自己则开始下一轮挖矿。如果区块的交易被其他节点参与者验证有效并且谜题的答案正确，就意味着这个答案是可信的，新的节点将被写入验证者的节点区块链，同时验证者进入下一轮竞争挖矿。
这道题关键的三个要素是工作量证明函数、区块及难度值。工作量证明函数是这道题的计算方法，区块决定了这道题的输入数据，难度决定了这道题所需要的计算量。
工作量证明函数
比特币中使用SHA256算法函数，是密码哈希函数家族中输出值为256位的哈希算法。
区块
Merkle树算法：
难度值
关于难度值，我们直接看公式：
新难度值=旧难度值*（过去2016个区块花费时长/20160分钟）
tips：难度值是随网络变动的，目的是为了在不同的网络环境下，确保每十分钟能生成一个块。
新难度值解析：撇开旧难度值，按比特币理想情况每十分钟出块的速度，过去2016个块的总花费接近2016分钟，这样，这个值永远趋近于1。
目标值=最大值/难度值,
目标值解析：最大目标值为一个固定数，若过去2016个区块花费时长少于20160分，那么这个系数会小，目标值将会被调大些，反之，目标值会被调小，因此，比特币的难度和出块速度将成反比例适当调整出块速度。
那如何计算呢？SHA256(SHA256(Block_Header))，即只需要对区块头进行两次SHA256运算即可，得到的值和目标值进行比较，小于目标值即可。
区块头中有一个重要的东西叫MerkleRoot的hash值。这个东西的意义在于：为了使区块头能体现区块所包含的所有交易，在区块的构造过程中，需要将该区块要包含的交易列表，通过Merkle Tree算法生成Merkle Root Hash，并以此作为交易列表的摘要存到区块头中。
至此，我们发现区块头中除过nonce(随机数)以外，其余的数据都是明确的，解题的核心就在于不停的调整nonce的值，对区块头进行双重SHA256运算。
Pow工作量证明流程 # 从流程图中看出，pow工作量证明的流程主要经历三步：
1.生成Merkle根哈希 生成Merkle根哈希，即节点自己生成一笔筹币交易，并且与其他所有即将打包的交易通过Merkle树算法生成Merkle根哈希，所以为什么说区块是工作量证明的三要素之一。
2.组装区块头 区块头将被作为计算出工作量证明输出的一个输入参数，因此第一步计算出来的Merkle根哈希和区块头的其他组成部分组装成区块头。
3.计算出工作量证明的输出 下面我们直接通过公式和一些伪代码去理解工作量证明的输出：
i. 工作量证明的输出=SHA256(SHA256(区块头))
ii. if（工作量证明的输出&amp;lt;目标值），证明工作量完成
iii.if（工作量证明的输出&amp;gt;=目标值）,变更随机数，递归i的逻辑，继续与目标值比对。
Pow共识记账 # 在比特币平台中，中本聪就是运用的pow工作量证明来使全网节点达到51%及以上的共识记账，以下将介绍pow工作量证明共识是如何记账的？
首先，客户端产生新的交易，向全网广播
第二，每个节点收到请求，将交易纳入区块中
第三，每个节点通过上述中描述的进行pow工作量证明
第四，当某个节点找到了证明，向全网广播
第五，当且仅当该区块的交易是有效的且在之前中未存在的，其他节点才认同该区块的有效性
第六，接受该区块且在该区块的末尾制造新的区块
大概时序图如下：
Pow的优缺点 # 优点：
完全去中心化（任何人都可以加入） 结点自由进出，容易实现 破坏系统花费成本巨大 关于破坏系统成本巨大可以分两层意思理解：</description>
    </item>
    
    <item>
      <title>go语言底层基础</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E8%AF%AD%E8%A8%80%E5%BA%95%E5%B1%82%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Wed, 03 Aug 2022 10:46:26 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E8%AF%AD%E8%A8%80%E5%BA%95%E5%B1%82%E5%9F%BA%E7%A1%80/</guid>
      <description>Go语言相关 # GMP模型 # G goroutine协程
P processor处理器
M thread线程
Processor 它包含了运行goroutine的资源，如果线程想运行goroutine,必须先获取P，P中还包含了可运行的G队列。
在Go中，线程是运行goroutine的实体，调度器的功能是把可运行的goroutine分配到工作线程上。
全局队列：存放等待运行的G P的本地队列：同全局队列类似，存放的也是等待运行的G，存的数量有限，不超过256个。新建 G’时，G’优先加入到 P 的本地队列，如果队列满了，则会把本地队列中一半的 G 移动到全局队列。 P列表：所有的P都在程序启动时创建，并保存在数组中，最多有 GOMAXPROCS(可配置) 个。 M：线程想运行任务就得获取P，从P的本地队列获取G，P队列为空时，M 也会尝试从全局队列拿一批 G 放到 P 的本地队列，或从其他 P 的本地队列偷一半放到自己 P 的本地队列。M 运行 G，G 执行之后，M 会从 P 获取下一个 G，不断重复下去。 Goroutine 调度器和 OS 调度器是通过 M 结合起来的，每个 M 都代表了 1 个内核线程，OS 调度器负责把内核线程分配到 CPU 的核上执行。
P 处理器的作用 # 负责调度G
P、M和G的个数问题 # 1、P的数量：
由启动时环境变量 $GOMAXPROCS 或者是由 runtime 的方法 GOMAXPROCS() 决定。这意味着在程序执行的任意时刻都只有 $GOMAXPROCS 个 goroutine 在同时运行。 默认是cpu核心数 2、M的数量：</description>
    </item>
    
    <item>
      <title>Gorm</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/gorm/</link>
      <pubDate>Tue, 24 May 2022 15:47:27 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/gorm/</guid>
      <description>GORM # 特性 # 全功能 ORM 关联 (Has One，Has Many，Belongs To，Many To Many，多态，单表继承) Create，Save，Update，Delete，Find 中钩子方法 支持 Preload、Joins 的预加载 事务，嵌套事务，Save Point，Rollback To Saved Point Context、预编译模式、DryRun 模式 批量插入，FindInBatches，Find/Create with Map，使用 SQL 表达式、Context Valuer 进行 CRUD SQL 构建器，Upsert，数据库锁，Optimizer/Index/Comment Hint，命名参数，子查询 复合主键，索引，约束 Auto Migration 自定义 Logger 灵活的可扩展插件 API：Database Resolver（多数据库，读写分离）、Prometheus… 每个特性都经过了测试的重重考验 开发者友好 安装 # go get -u gorm.io/gormgo get -u gorm.io/driver/sqlite 快速入门 # package main import ( &amp;#34;gorm.io/gorm&amp;#34; &amp;#34;gorm.io/driver/sqlite&amp;#34; ) type Product struct { gorm.</description>
    </item>
    
    <item>
      <title>升级链码</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-04-14-%E5%8D%87%E7%BA%A7%E9%93%BE%E7%A0%81/</link>
      <pubDate>Thu, 14 Apr 2022 11:14:32 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-04-14-%E5%8D%87%E7%BA%A7%E9%93%BE%E7%A0%81/</guid>
      <description>https://blog.csdn.net/xiaohanshasha/article/details/123664164
https://blog.csdn.net/weixin_43839871/article/details/106410693
https://uzshare.com/view/830631
找的博客 突然又不想试了 先记录一下 以后再说</description>
    </item>
    
    <item>
      <title>Raft共识算法</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/2022-03-26-raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</link>
      <pubDate>Sat, 26 Mar 2022 21:22:14 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/2022-03-26-raft%E5%85%B1%E8%AF%86%E7%AE%97%E6%B3%95/</guid>
      <description>分布式共识算法 # 首先我们先明确这个问题：为什么需要分布式共识算法？
这就要从当前的分布式系统设计的缺陷来看了，假设我们的集群现在有两个客户端和三个服务端，如下图：
在这个分布式系统的设计中，往往要满足CAP理论，而分布式共识算法解决的就是CAP理论中的一致性问题。整个一致性问题分为三种问题：
顺序一致性 线性一致性 因果一致性 顺序一致性 # 假设执行结果与这些处理器以某一串行顺序执行的结果相同，同时每个处理器内部操作的执行看起来又与程序描述的顺序一致。满足该条件的多处理器系统我们就认为是顺序一致的。实际上，处理器可以看做一个进程或者一个线程，甚至是一个分布式系统。
这句话并不是很好理解，我们看一下分布式系统中顺序一致性的一个例子：
假设客户端A有两条命令： command1:set(x,1)	//设置x为1 command2:set(x,3) 客户端B有一下两条命令： command3:get(x)	//得到x的当前值 command4:set(x,4) 那么如果服务端那边收到的节点只要满足command2在command1后面执行并且comand4在command3后面执行我们就认为其满足顺序一致性。 线性一致性 # 线性一致性或称原子一致性或严格一致性，指的是程序在执行顺序组合中存在可线性化点P的执行模型，这意味着一个操作将在程序的调用和返回之间的某个点P生效，之后被系统中并发运行的所有其他线程所感知。
通俗来讲，线性一致性可以说是顺序一致性的升级版。其会有一个全局时钟，假设还是上面发送的命令，只不过加上了时间信息： 客户端A发送的命令如下：
[14:01]command1:set(x,1)	//设置x为1 [14:02]command2:set(x,3) 客户端B发送的命令如下：
[14:03]command3:get(x)	//得到x的当前值 [14:04]command4:set(x,4) 注： 这里假设时延可能是几分钟级别的，所以有可能是command3在command1之前到
所以，假设初始值x = 0，而我们到达的顺序如下：
command1-&amp;gt;command3-&amp;gt;command2-&amp;gt;command4command1-&amp;gt;command3-&amp;gt;command4-&amp;gt;command2... 这个顺序确实是满足顺序一致性，但是我们get(x)获得的值可谓是千奇百怪，可能是0，1，3 。为了解决顺序一致性的不足，所以才提出的线性一致性。其要求命令满足全局时钟的时序性。所以很容易就知道，满足线性一致性的一定满足顺序一致性；相反，满足顺序一致性的不一定会满足线性一致性。 因果一致性 # 线性一致性要求所有线程的操作按照一个绝对的时钟顺序执行，这意味着线性一致性是限制并发的，否则这种顺序性就无法保证。由于在真实环境中很难保证绝对时钟同步，因此线性一致性是一种理论。实现线性一致性的代价也最高，但是实战中可以弱化部分线性一致性：只保证有因果关系的事件的顺序，没有因果关系的事件可以并发执行，其指的是假设有两个事件：A事件和B事件，如果A发生在B后面，那么就称A和B具有因果关系。
拜占庭将军问题 # 拜占庭将军问题是一个协议问题，拜占庭帝国军队的将军们必须全体一致的决定是否攻击某一支敌军。问题是这些将军在地理上是分隔开来的，并且将军中存在叛徒。叛徒可以任意行动以达到以下目标：欺骗某些将军采取进攻行动；促成一个不是所有将军都同意的决定，如当将军们不希望进攻时促成进攻行动；或者迷惑某些将军，使他们无法做出决定。如果叛徒达到了这些目的之一，则任何攻击行动的结果都是注定要失败的，只有完全达成一致的努力才能获得胜利。
含义是在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。
Raft共识 # 简介 # Raft实现一致性的机制是：首先选择一个leader全权负责管理日志复制，leader从客户端接收log entries（日志条目），将它们复制给集群中的其他机器，然后负责告诉它机器什么时候将日志应用于它们的状态机。举个例子，leader可以在无需询问其他server的情况下决定把新entries放在哪个位置，数据永远是从leader流向其他机器（leader的强一致性）。一个leader可以fail或者与其他机器失去连接，这种情况下会有新的leader被选举出来。
在任何时刻，每个server节点有三种状态：leader、candidate、follower。
leader：作为客户端的接收者，接收客户端发送的日志复制请求，并将日志信息复制到follower节点中，维持网络各个节点的状态。 candidate：在leader选举阶段存在的状态，通过任期号term和票数进行领导人身份竞争，获胜者将成为下一任期的领导人。 follower：作为leader节点发送日志复制请求的接收者，与leader节点通信，接收账本信息，并确认账本信息的有效性，完成日志信息的提交和存储。 正常运行时，只有一个leader，其余全是follower。follower是被动的：它们不主动提出请求，只是响应leader和candidate的请求。leader负责处理所有客户端请求（如果客户端先连接某个follower，该follower负责将它重定向到leader）。candidate状态用户选举leader节点。
如何让跨网络机器之间协调一致性？
状态的立即一致性 状态的最终一致性 raft来源于paxos，它简化了paxos，以易于理解为首要目标，尽量提供与paxos一样的功能与性能。
提出问题：
1、输入：写入命令
2、输出：所有节点最终处于相同的状态
2、约束</description>
    </item>
    
    <item>
      <title>区块链网络添加组织</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-03-25-%E5%8C%BA%E5%9D%97%E9%93%BE%E7%BD%91%E7%BB%9C%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87/</link>
      <pubDate>Fri, 25 Mar 2022 13:49:25 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2022-03-25-%E5%8C%BA%E5%9D%97%E9%93%BE%E7%BD%91%E7%BB%9C%E6%B7%BB%E5%8A%A0%E7%BB%84%E7%BB%87/</guid>
      <description>介绍configtxlator工具 # configtxlator工具提供了一个真正的无状态REST API，独立于SDK，以简化Hyperledger Fabric区块链网络中的配置任务。该工具可以在不同的等效数据表示/格式之间轻松转换。例如，在一种工具操作模式下，该工具在二进制protobuf格式之间执行转换为人类可读的JSON文本格式，反之亦然。此外，该工具可以根据两组不同配置事务之间的差异计算配置更新。
1、环境配置 # 运行官方测试网络，确保它正常运行，详情请见fabric环境搭建后面测试部分。
进入CLI容器，并使用容器内的以下命令检查对等版本：
docker exec -it cli /bin/bash peer version 运行以下命令，确保JQ工具已在CLI容器中安装并正常工作：
jq --versionjq 运行以下命令，确保configtxlator工具可用，验证工具版本，在后台启动工具，并验证工具在后台、CLI容器内正确运行
configtxlator version 后台启动configtxlator并查看网络状态 （两行一起复制粘贴进去）
configtxlator start &amp;amp; netstat -ap 2、检索当前配置 # 通过在CLI容器中运行以下命令来设置和验证以下环境变量：
export CHANNEL_NAME=mychannel export ORDERER_CA=/opt/gopath/src/github.com/hyperledger/fabric/peer/organizations/ordererOrganizations/example.com/msp/tlscacerts/tlsca.example.com-cert.pem 生成第三个组织的证书文件及配置文件
文章路径主要看你test-network中脚本文件的路径做出修改，根据实际情况改
#生成证书文件 另开一个命令行进入test-network/addOrg3 cryptogen generate --config=org3-crypto.yaml --output=&amp;#34;../organizations&amp;#34; #指定组织3的证书配置文件 #生成json格式的配置文件 configtxgen -printOrg Org3MSP &amp;gt; ../organizations/peerOrganizations/org3.example.com/org3.json configtxgen -printOrg Org3MSP&amp;gt;./channel-artifacts/org3.json #生成的配置文件需要放到cli中使用 3、组织注册 # 1、CLI容器中运行以下命令来检索当前配置的配置块 # peer channel fetch config config_block.pb -o orderer.example.com:7050 -c $CHANNEL_NAME --tls --cafile $ORDERER_CA 报错：</description>
    </item>
    
    <item>
      <title>Redis集群搭建</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2022-03-21-redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Mon, 21 Mar 2022 15:23:09 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2022-03-21-redis%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/</guid>
      <description>有空再说
https://www.cnblogs.com/wuxl360/p/5920330.html
还没整理
为什么要有集群
a) 服务器可能因为代码原因，人为原因，或者自然灾害等造成服务器损坏。数据服务就挂掉了
b) 大公司都会有很多的服务器(华东地区、华南地区、华中地区、华北地区、西北地区、西南地区、东北地区、台港澳地区机房)
集群的概念
集群是一组相互独立的、通过高速网络互联的计算机，它们构成了一个组，并以单一系统的模式加以管理。一个客户与集群相互作用时，集群像是一个独立的服务器。集群配置是用于提高可用性和可缩放性。
当请求到来首先由负载均衡服务器处理，把请求转发到另外的一台服务器上。
百度的ip地址 119.75.217.109/
​ 61.135.169.121/
Redis集群
分类
Ø 软件层面
Ø 硬件层面
软件层面：只有一台电脑，在这台电脑上启动了多台redis服务
硬件层面：存在多台实体电脑,每台电脑都启动了一个redis或者多个redis服务
参考阅读
Redis搭建集群http://www.cnblogs.com/wuxl360/p/5920330.html
go语言redis-cluster开源客户端https://github.com/gitstliu/go-redis-cluster
配置机器1 # Ø 在演示中，192.168.110.37为当前ubuntu机器的ip
Ø 在192.168.110.37上进⼊Desktop⽬录，创建conf⽬录
Ø 在conf⽬录下创建⽂件7000.conf，编辑内容如下
port 7000bind 192.168.110.37daemonize yespidfile 7000.pidcluster-enabled yescluster-config-file 7000_node.confcluster-node-timeout 15000appendonly yese Ø 在conf⽬录下创建⽂件7001.conf，编辑内容如下
port 7001bind 192.168.110.37daemonize yespidfile 7001.pidcluster-enabled yescluster-config-file 7001_node.confcluster-node-timeout 15000appendonly yes Ø 在conf⽬录下创建⽂件7002.conf，编辑内容如下</description>
    </item>
    
    <item>
      <title>Redis基础</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2022-03-20-redis%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Sun, 20 Mar 2022 14:47:04 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2022-03-20-redis%E5%9F%BA%E7%A1%80/</guid>
      <description>Redis # 是一种高性能的Key-Value数据库
NoSQL介绍 # NoSQL：一类新出现的数据库(not only sql)，它的特点：
1.不支持SQL语法
2.存储结构跟传统关系型数据库中的那种关系表完全不同，nosql中存储的数据都是Key-Value形式
3.NoSQL的世界中没有一种通用的语言，每种nosql数据库都有自己的api和语法，以及擅长的业务场景
NoSQL和SQL数据库的比较： # 适用场景不同：sql数据库适合用于关系特别复杂的数据查询场景，nosql反之
两者在不断地取长补短，呈现融合趋势
Redis简介 # Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。从2010年3月15日起，Redis的开发工作由VMware主持。从2013年5月开始，Redis的开发由Pivotal赞助。
Redis是 NoSQL技术阵营中的一员，它通过多种键值数据类型来适应不同场景下的存储需求，借助一些高层级的接口使用其可以胜任，如缓存、队列系统的不同角色。
Redis特性 # Redis 与其他 key - value 缓存产品有以下三个特点：
Redis支持数据的持久化，可以将内存中的数据保存在磁盘中，重启的时候可以再次加载进行使用。 Redis不仅仅支持简单的key-value类型的数据，同时还提供list列表，set集合，zset有序集合，hash等数据结构的存储。 Redis支持数据的备份，即master-slave模式的数据备份。 Redis 优势 # 性能极高 – Redis能读的速度是110000次/s,写的速度是81000次/s 。 丰富的数据类型 – Redis支持二进制案例的 Strings, Lists, Hashes, Sets 及 Ordered Sets 数据类型操作。 原子 – Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行。 丰富的特性 – Redis还支持 publish/subscribe, 通知, key 过期等等特性。 Redis应用场景 # 用来做缓存(ehcache/memcached)——redis的所有数据是放在内存中的（内存数据库） 可以在某些特定应用场景下替代传统数据库——比如社交类的应用 在一些大型系统中，巧妙地实现一些特定的功能：session共享、购物车 只要你有丰富的想象力，redis可以用在可以给你无限的惊喜……. 中文官网
Redis安装（Mac） # 直接brew安装</description>
    </item>
    
    <item>
      <title>比特币相关机制与原理</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E6%AF%94%E7%89%B9%E5%B8%81/2022-02-25-%E6%AF%94%E7%89%B9%E5%B8%81%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8E%9F%E7%90%86/</link>
      <pubDate>Fri, 25 Feb 2022 11:13:16 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E6%AF%94%E7%89%B9%E5%B8%81/2022-02-25-%E6%AF%94%E7%89%B9%E5%B8%81%E7%9B%B8%E5%85%B3%E6%9C%BA%E5%88%B6%E4%B8%8E%E5%8E%9F%E7%90%86/</guid>
      <description>什么是区块链？ # 一个分布式账本
区块链网络的核心是一个分布式账本，记录网络上发生的所有交易。
区块链账本通常被描述为 去中心化的 ，因为它会被复制到许多网络参与者中，每个参与者都在 协作 维护账本。
除了分散和协作之外，信息仅能以附加的方式记录到区块链上，并使用加密技术保证一旦将交易添加到账本就无法修改。这种“不可修改”的属性简化了信息的溯源，因为参与者可以确定信息在记录后没有改变过。这就是为什么区块链有时被描述为 证明系统 。
什么是去中心化？ # 去中心化就是在一个分布有众多节点的系统中，每个节点都具有高度自治的特征。节点之间可以自由连接，形成新的连接单元。任何一个节点都有可能成为阶段性的中心。
优势
容错性能力强 不易被攻击 数据无法篡改 对等网络 # 也称P2P网络，位于网络中的每一个节点都彼此对等，各个节点共同提供网络服务。区块链网络基于国际互联网的P2P网络架构，由对等节点Peer构成，每个节点以扁平的拓扑结构互相连通，不存在任何服务端、中心化服务，以及层级结构，而且必须遵守相同的约定（P2P协议）。
交易池 # 张三验证交易有效后，将交易写入自己的草稿本。这个草稿本也称交易池，存放每个节点收到的有效交易。每个节点的交易池中都有很多交易，可能每个人的交易池不一样，比如并非每一条交易都传递到每个人手中。
挖矿 # 解答数学题的过程叫挖矿，谁解出答案，告知大家，其他人就停止答题，本轮记账权的获胜者已产生。每个人草稿上上的内容是不一样的。谁有记账权，谁将自己草稿本上的内容写入账本。
创币交易 # 为了鼓励大家答题，获胜者获得5元钱，以交易的形式写入账本。实现货币总量的增长，比特币中“获胜矿工”获得的奖励除了创币金额外，还包括交易费。同时，比特币的创币金额是衰减的。
工作量证明 # 如果把挖矿当作一份工作，解题答案也被称为工作量证明。
矿工获得记账权后，翻开自己的账本，到最新页，将奖励作为第一条交易，草稿上的逐个抄入，每次最多只能写满一页，多余的舍弃，少的留白。多余的交易，不算成功，由发起者再度创建，写入纸条继续在大厅传递。交易写入账本后，在大厅黑白上写明。其他人开始验证，验证结束后写人自己的账本。
共识与共识算法 # 所有节点验证成功后记入自己的账本，保证账本数据的一致性，即节点达成了共识。共识通过村民验证解题答案，即工作量证明而达成的。采用工作量证明（POW）来达成共识也被称为工作量证明共识算法或共识机制。
确认 # 交易写入区块链就能得到确认，但由于共识算法自身的原因会导致偶然事件的发生，可能会出现区块链数据在接下来几个区块内数据回滚的情况，比如比特币的偶然分叉。比特币交易的永久生效需要在当前区块上继续添加6个区块。
诚实节点和恶意节点 # 遵守规则的节点和不遵守规则的节点
区块链分叉 # 恶意节点创建无效交易，使得网络中出现节点数据不一致的情形，称为区块链分叉。这种分叉是短暂的，新区块会替换掉旧区块，而且，无效区块会导致记账节点失去奖励，得不偿失。
软分叉
当系统中出现了新版本的软件（或协议），而旧软件能接受新软件的区块，新老双方始终都工作在同一条链上，这称为软分叉。
硬分叉
当系统中出现了新版本的软件（或协议），并且和前版本软件不能兼容，老软件节点无法接受新软件节点挖出的全部或部分区块（认为不合法），导致同时出现两条链。尽管新节点算力较大，比如99%的算力为新节点，1%的老节点依然会维护着不同的一条链，因为新节点产生的区块老节点实在无法接受（尽管它知道网络上99%的节点都接受了），这称为硬分叉。
智能合约 # 为了支持以同样的方式更新信息，并实现一整套账本功能（交易，查询等），区块链使用 智能合约 来提供对账本的受控访问。
智能合约不仅是在网络中封装和简化信息的关键机制，它还可以被编写成自动执行参与者的特定交易的合约。
共识 # 保持账本在整个网络中同步的过程称为 共识 。该过程确保账本仅在交易被相应参与者批准时更新，并且当账本更新时，它们以相同的顺序更新相同的交易。
比特币 # 简介 # 比特币（Bitcoin）的概念最初由中本聪在2008年11月1日提出，并于2009年1月3日正式诞生。
根据中本聪的思路设计发布的开源软件以及建构其上的P2P网络。比特币是一种P2P形式的虚拟的加密数字货币。点对点的传输意味着一个去中心化的支付系统。
与所有的货币不同，比特币不依靠特定货币机构发行，它依据特定算法，通过大量的计算产生，比特币经济使用整个P2P网络中众多节点构成的分布式数据库来确认并记录所有的交易行为，并使用密码学的设计来确保货币流通各个环节安全性。P2P的去中心化特性与算法本身可以确保无法通过大量制造比特币来人为操控币值。基于密码学的设计可以使比特币只能被真实的拥有者转移或支付。这同样确保了货币所有权与流通交易的匿名性。比特币与其他虚拟货币最大的不同，是其总数量非常有限，具有的稀缺性。</description>
    </item>
    
    <item>
      <title>benchmark测试</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-12-20-benchmark%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Mon, 20 Dec 2021 17:44:59 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/2021-12-20-benchmark%E6%B5%8B%E8%AF%95/</guid>
      <description>用于测试函数性能
Go 中的基准测试在许多方面类似于单元测试，但有关键的不同之处，并且服务于不同的目的。由于它们不像 Go 中的单元测试那样广为人知，本文旨在介绍 Go 的基准测试：如何创建、如何运行它们、如何读取结果以及一些指向创建基准测试的一些高级主题的指针在去。
基准测试是测试 Go 代码性能的函数，它们包含testing在标准 Go 库的包中，因此无需任何外部库的依赖即可使用。
执行基准测试时，会向您提供有关执行时间的一些信息，如果需要，还会提供基准测试下代码的内存占用量。
创建基准 # 创建cc_test.go文件
要创建基准测试，您需要在 go 文件中导入testing包并以创建测试函数的类似方式创建基准测试函数。
例如，在定义单元测试时，我们func TestAny(t *testing)以开头的形式编写函数，而在定义基准测试时，我们将创建一个**func BenchmarkAny(b \*testing.B)**.
Go 的基准测试在单元测试方面的一个显着差异是从 0 到b.N. 事实上，基准测试会运行多次，以确保收集到足够的数据以提高基准测试下代码性能测量的准确性。
该字段b.N不是固定值，而是动态调整以确保基准测试功能至少运行 1 秒。
这里展示的是基准和测试函数之间的比较：
func Benchmark1Sort(b *testing.B) {for i := 0; i &amp;lt; b.N; i++ {sort.Ints(generateSlice(1000))}} func Test1Sort(t *testing.T) {slice := generateSlice(1000)if len(slice) != 1000 {t.Errorf(&amp;#34;unexpected slice size: %d&amp;#34;, len(slice))}} 运行基准 # 运行 Go 的基准测试的起点是go test命令，在这里我们将看到我们需要确保我们不只是运行单元测试。</description>
    </item>
    
    <item>
      <title>部署tape测试</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-12-20-%E9%83%A8%E7%BD%B2tape%E6%B5%8B%E8%AF%95/</link>
      <pubDate>Mon, 20 Dec 2021 10:11:10 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-12-20-%E9%83%A8%E7%BD%B2tape%E6%B5%8B%E8%AF%95/</guid>
      <description>安装 # cd hyperledgergit clone https://github.com/Hyperledger-TWGC/tape.gitcd tapego build ./cmd/tape 测试 # 测试前将organizations文件夹放到tape 里面去 复制一下 就是里面包含各种证书的文件夹 配路径
修改config.yaml文件
# Definition of nodes peer1: &amp;amp;peer1 addr: localhost:7051 tls_ca_cert: ./organizations/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/tlscacerts/tlsca.org1.example.com-cert.pem peer2: &amp;amp;peer2 addr: localhost:9051 tls_ca_cert: ./organizations/peerOrganizations/org2.example.com/peers/peer0.org2.example.com/msp/tlscacerts/tlsca.org2.example.com-cert.pem orderer1: &amp;amp;orderer1 addr: localhost:7050 tls_ca_cert: ./organizations/ordererOrganizations/example.com/msp/tlscacerts/tlsca.example.com-cert.pem # Nodes to interact with endorsers: - *peer1 - *peer2 # we might support multi-committer in the future for more complex test scenario, # i.e. consider tx committed only if it&amp;#39;s done on &amp;gt;50% of nodes.</description>
    </item>
    
    <item>
      <title>go-ipfs-api</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-12-05-go-ipfs-api/</link>
      <pubDate>Sun, 05 Dec 2021 21:28:22 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-12-05-go-ipfs-api/</guid>
      <description>json文件 # 上传获取数据 # package main import ( &amp;#34;bytes&amp;#34; &amp;#34;encoding/json&amp;#34; &amp;#34;fmt&amp;#34; &amp;#34;io/ioutil&amp;#34; shell &amp;#34;github.com/ipfs/go-ipfs-api&amp;#34; ) var sh *shell.Shell //交易结构体(未来的通道) type Transaction struct { Person1 string `json:&amp;#34;person1,omitempty&amp;#34; xml:&amp;#34;person1&amp;#34;` Person2 string `json:&amp;#34;person2,omitempty&amp;#34; xml:&amp;#34;person2&amp;#34;` Person1money string `json:&amp;#34;person1Money,omitempty&amp;#34; xml:&amp;#34;person1Money&amp;#34;` Person2money string `json:&amp;#34;person2Money,omitempty&amp;#34; xml:&amp;#34;person2Money&amp;#34;` } //数据上传到ipfs func UploadIPFS(str string) string { sh = shell.NewShell(&amp;#34;localhost:5001&amp;#34;) //连接客户端 hash, err := sh.Add(bytes.NewBufferString(str)) if err != nil { fmt.Println(&amp;#34;上传ipfs时错误：&amp;#34;, err) } return hash } //从ipfs获取数据 只读 func CatIPFS(hash string) string { sh = shell.</description>
    </item>
    
    <item>
      <title>LeetCode算法总结</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-10-28-leetcode%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</link>
      <pubDate>Thu, 28 Oct 2021 22:03:24 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/leetcode/2021-10-28-leetcode%E7%AE%97%E6%B3%95%E6%80%BB%E7%BB%93/</guid>
      <description>动态规划 # 介绍 # 当最优化问题具有重复子问题和最优子结构的时候，适合使用动态规划算法。动态规划算法的核心就是提供了一个memory来缓存重复子问题的结果，避免了递归的过程中的大量的重复计算。动态规划算法的难点在于怎么将问题转化为能够利用动态规划算法来解决。当重复子问题的数目比较小时，动态规划的效果也会很差。如果问题存在大量的重复子问题的话，动态规划的效率较高。
例题 # 正则表达式匹配 # 给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 &amp;lsquo;.&amp;rsquo; 和 &amp;lsquo;*&amp;rsquo; 的正则表达式匹配。
&amp;lsquo;.&amp;rsquo; 匹配任意单个字符 &amp;lsquo;*&amp;rsquo; 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。
示例 1：
输入：s = &amp;#34;aa&amp;#34; p = &amp;#34;a&amp;#34;输出：false解释：&amp;#34;a&amp;#34; 无法匹配 &amp;#34;aa&amp;#34; 整个字符串。 示例 2:
输入：s = &amp;#34;aa&amp;#34; p = &amp;#34;a*&amp;#34;输出：true解释：因为 &amp;#39;*&amp;#39; 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 &amp;#39;a&amp;#39;。因此，字符串 &amp;#34;aa&amp;#34; 可被视为 &amp;#39;a&amp;#39; 重复了一次。 func isMatch(s string, p string) bool { m, n := len(s), len(p) matches := func(i, j int) bool { if i == 0 { return false } if p[j-1] == &amp;#39;.</description>
    </item>
    
    <item>
      <title>ipfs-webui可视化工具搭建</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-07-12-ipfs-webui%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Mon, 12 Jul 2021 20:27:55 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-07-12-ipfs-webui%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E6%90%AD%E5%BB%BA/</guid>
      <description>介绍 # 注意这里是私链搭建webui，公链没有这么麻烦
在IPFS项目的组织架构中，有一个IPFS-GUI工作组，主要目的是开发IPFS可视化工具，并使工具更简单、更易用、更美观。
IPFS WebUI是IPFS的Web界面，可以用来检查您的节点统计信息，展示由IPLD驱动的默克尔树结构，查看世界各地的节点并管理您的文件，而无需触摸命令行工具。
这都是粘贴的，废话不多说，直接开始安装
安装 # 拉取ipfs-webui文件 cd ~git clone https://github.com/ipfs/ipfs-webui.git 进入安装 cd ipfs-webuinpm install 报错：
request to https://dist.ipfs.io/go-ipfs/versions failed, reason: connect ECONNREFUSED 69.171.233.24:443。。。。。。。。。。。。。。。。。。。。。。。。npm ERR! code ELIFECYCLEnpm ERR! errno 1npm ERR! go-ipfs-dep@0.4.18 install: `node src/bin.js`npm ERR! Exit status 1npm ERR! npm ERR! Failed at the go-ipfs-dep@0.4.18 install script.npm ERR! This is probably not a problem with npm.</description>
    </item>
    
    <item>
      <title>IPFS基本原理（一）</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-07-08-ipfs%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%80/</link>
      <pubDate>Thu, 08 Jul 2021 17:46:47 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-07-08-ipfs%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86%E4%B8%80/</guid>
      <description>IPFS基础 # 1.1 IPFS 概述 # IPFS（InterPlanetary File System)是一个基于内容寻址的、分布式的、新型超媒体传输协议。IPFS支持创建完全分布式的应用。它旨在使网络更快、更安全、更开放。IPFS是一个分布式文件系统，它的目标是将所有计算设备连接到同一个文件系统，从而成为一个全球统一的存储系统。
IPFS项目通过整合已有的技术（BitTorrent、DHT、Git和SFS），创建一种点对点超媒体协议，试图打造一个更加快速、安全、开放的下一代互联网，实现互联网中永久可用、数据可以永久保存的全球文件存储系统。同时，该协议有内容寻址、版本化特性，尝试补充甚至最终取代超文本传输协议（HTTP协议）。IPFS是一个协议，也是一个P2P网络，它类似于现在的BT网络，只是拥有更强大的功能，使得IPFS拥有可以取代HTTP的潜力。
它提供了更加便宜、安全、可快速集成的存储解决方案。
1.1.1 HTTP四大问题 # 极易受到攻击，防范攻击成本搞。 数据存储成本高。 数据中心化带来泄露风险。 大规模数据存储、传输和维护难。 1.1.2 IPFS优势 # 下载速度快
IPFS使用了BitTorrent协议作为数据传输方式，使得IPFS系统在数据传输速度上大幅度提高，并且能够节省约60%的网络带宽。
优化全球存储
IPFS采用为数据块内容建立哈希去重的方式存储数据，数据的存储成本将会显著下降。
更加安全
IPFS、Filecoin的分布式特性与加密算法使得数据存储更加安全，甚至可以抵挡黑客攻击。
数据的可持续保存
IPFS提供了一种使互联网数据可以被可持续保存的存储方式，并且提供数据历史版本（Git)的回溯功能。
1.2 IPFS借鉴的技术 # 1.2.1 哈希表DHT # 全称为分布式哈希表（Distributed Hash Table)，是一种分布式存储方法。DHT的原理是：在不需要服务器的情况下，每一个客户端存储一小部分数据，并负责一定区域的检索，进而实现整个DHT网络的寻址和检索。
1.2.2 Kademlia # 在Kademlia网络中，所有信息均以哈希表条目的形式加以存储，这些信息被分散的存储在各个节点上，从而形成一张巨大的分布式哈希表。
1.2.3 Git # Git存储时会把文件拆成若干部分，并计算各个部分的哈希值，利用这些构建起于文件对应的有向无环图（DAG），DAG的根节点也就是该文件的哈希值。
如果需要修改文件，那么只需要修改少数图中节点即可；需要分享文件，等价于分享这个图；需要传输全部的文件，按照图中的哈希值下载合并即可。
1.2.4 默克尔树 # 在IPFS项目里，也借鉴了默克尔树的思想。数据分块存放在有向无环图中，如果数据被修改了，只需要修改对应默克尔有向无环图中的节点数据，而不需要向网络重新更新整个文件。
1.2.5 IPFS 补充区块链两大缺陷 # 区块链存储效率低，成本高。 跨链需要各个链之间协同配合，难以协调。 1.3 IPFS的优势与价值 # 1.3.1 技术优势 # IPFS技术可以分为多层子协议栈，从上至下为身份层、网络层、路由层、交换层、对象层、文件层、命名层，每个协议栈各司其职，又互相协同。
身份层和路由层 # 对等节点身份信息的生成以及路由规则是通过Kademlia协议生成制定的，该协议实质上是构建了一个分布式哈希表，简称DHT。每个加入这个DHT网络的节点都要生成自己的身份信息，然后才能通过这个身份信息去负责存储这个网络里的资源信息和其他成员的联系信息。
网络层 # 比较核心，所使用的Libp2p可以支持主流传输层协议。NAT技术能让哪网中的设备共用同一个外网IP。</description>
    </item>
    
    <item>
      <title>IPFS私有网络搭建</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-06-02-ipfs%E7%A7%81%E6%9C%89%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Wed, 02 Jun 2021 15:42:42 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/ipfs/2021-06-02-ipfs%E7%A7%81%E6%9C%89%E7%BD%91%E7%BB%9C%E6%90%AD%E5%BB%BA/</guid>
      <description>IPFS私有网络集群搭建 # 前期准备 # 对于联盟链的业务中搭建一个私有网络的 IPFS 集群还是很有必要的，私有网络集群允许 IPFS 节点只连接到拥有共享密钥的其他对等节点，网络中的节点不响应来自网络外节点的通信。 IPFS-Cluster 是一个独立的应用程序和一个 CLI 客户端，它跨一组 IPFS 守护进程分配、复制和跟踪 pin。它使用基于 Raft 一致性算法来协调存储，将数据集分布到参与节点上。对于我们要将一个 peer 上的存储同步备份到所有集群上其他的 peers 时，或者对集群的节点管理，这时 IPFS-Cluster 就会起到一个很好的作用。
本人使用三台虚拟机 主机列表
节点 名称 IP 管理节点peer0 Ubuntu1.0 10.211.55.7 peer1 Ubuntu2.0 10.211.55.9 peer2 Ubuntu3.0 10.211.55.10 IPFS 和 IPFS-Cluster 默认的端口: IPFS：
4001 – 与其他节点同学端口 5001 – API server 8080 – Gateway server IPFS-CLUSTER：
9094 – HTTP API endpoint 9095 – IPFS proxy endpoint 9096 – Cluster swarm 集群几点通信端口 安装Golang # IPFS 官方提供的安装方式有安装包方式，ipfs-update 方式，源码编译安装方式，具体可以查看 https://docs.</description>
    </item>
    
    <item>
      <title>redis面试总结</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2021-05-02-redis%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/</link>
      <pubDate>Sun, 02 May 2021 20:45:09 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/redis/2021-05-02-redis%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93/</guid>
      <description>1、什么是redis? 2、Reids的特点 3、使用redis有哪些好处？ 4、redis相比memcached有哪些优势？ 5、Memcache与Redis的区别都有哪些？ 6、redis适用于的场景? 7、redis的缓存失效策略和主键失效机制 8、为什么redis需要把所有数据放到内存中? 9、Redis是单进程单线程的 10、redis的并发竞争问题如何解决? 11、redis常见性能问题和解决方案 12、redis事物的了解CAS(check-and-set 操作实现乐观锁 )? 13、WATCH命令和基于CAS的乐观锁? 14、使用过Redis分布式锁么，它是什么回事？ 15、假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如果将它们全部找出来？ 16、使用过Redis做异步队列么，你是怎么用的？ 17、如果有大量的key需要设置同一时间过期，一般需要注意什么？ 18、Redis如何做持久化的？ 19、Pipeline有什么好处，为什么要用pipeline？ 20、Redis的同步机制了解么？ 21、是否使用过Redis集群，集群的原理是什么？ 1、什么是redis? # redis是一个高性能的key-value数据库，它是完全开源免费的，而且redis是一个NOSQL类型数据库，是为了解决高并发、高扩展，大数据存储等一系列的问题而产生的数据库解决方案，是一个非关系型的数据库
2、Reids的特点 # Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。
Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能，比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。
Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。
3、使用redis有哪些好处？ # 3.1 速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)
3.2 支持丰富数据类型，支持string，list，set，sorted set，hash
String # 常用命令 ：set/get/decr/incr/mget等；
应用场景 ：String是最常用的一种数据类型，普通的key/value存储都可以归为此类；
实现方式：String在redis内部存储默认就是一个字符串，被redisObject所引用，当遇到incr、decr等操作时会转成数值型进行计算，此时redisObject的encoding字段为int。
Hash # 常用命令 ：hget/hset/hgetall等
应用场景 ：我们要存储一个用户信息对象数据，其中包括用户ID、用户姓名、年龄和生日，通过用户ID我们希望获取该用户的姓名或者年龄或者生日；
实现方式：Redis的Hash实际是内部存储的Value为一个HashMap，并提供了直接存取这个Map成员的接口。如图所示，Key是用户ID, value是一个Map。这个Map的key是成员的属性名，value是属性值。这样对数据的修改和存取都可以直接通过其内部Map的Key(Redis里称内部Map的key为field)，也就是通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据。当前HashMap的实现有两种方式：当HashMap的成员比较少时Redis为了节省内存会采用类似一维数组的方式来紧凑存储，而不会采用真正的HashMap结构，这时对应的value的redisObject的encoding为zipmap，当成员数量增大时会自动转成真正的HashMap,此时redisObject的encoding字段为int。
List # 常用命令 ：lpush/rpush/lpop/rpop/lrange等；
应用场景 ：Redis list的应用场景 非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现；</description>
    </item>
    
    <item>
      <title>fabric浏览器搭建</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-02-fabric%E6%B5%8F%E8%A7%88%E5%99%A8%E6%90%AD%E5%BB%BA/</link>
      <pubDate>Sun, 02 May 2021 10:38:23 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-05-02-fabric%E6%B5%8F%E8%A7%88%E5%99%A8%E6%90%AD%E5%BB%BA/</guid>
      <description>fabric浏览器 # Hyperledger Explorer是一个简单，强大，易于使用，维护良好的开源实用程序，可浏览底层区块链网络上的活动。用户可以在MacOS和Ubuntu上配置和构建Hyperledger Explorer。
先要保证你之前的项目已启动
搭建目录结构 # 1、$GOPATH/src目录下创建edu-explorer文件夹
2、edu-explorer文件夹下创建以下目录结构
docker-compose.yamlconfig.jsonconnection-profile/test-network.jsonorganizations/ordererOrganizations/ 第3、4解决organizations/peerOrganizations/ 3、复制自己的项目中crypto-config 文件夹 到edu-explorer文件中
cp -r cp -r $GOPATH/src/education/conf/crypto-config ../edu-explorer 4、改名 把crypto-config改成organizations 保持跟官方目录结构一样
mv crypto-config organizations 官方给出的文件内容 # 复制以下内容到相应文件中去
docker-compose.yaml # # SPDX-License-Identifier: Apache-2.0version: &amp;#39;2.1&amp;#39;volumes:pgdata:walletstore:networks:mynetwork.com:external:name: net_testservices:explorerdb.mynetwork.com:image: hyperledger/explorer-db:latestcontainer_name: explorerdb.mynetwork.comhostname: explorerdb.mynetwork.comenvironment:- DATABASE_DATABASE=fabricexplorer- DATABASE_USERNAME=hppoc- DATABASE_PASSWORD=passwordhealthcheck:test: &amp;#34;pg_isready -h localhost -p 5432 -q -U postgres&amp;#34;interval: 30stimeout: 10sretries: 5volumes:- pgdata:/var/lib/postgresql/datanetworks:- mynetwork.</description>
    </item>
    
    <item>
      <title>手动生成ca证书搭建fabric网络</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-05-01-%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90ca%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAfabric%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Sat, 01 May 2021 17:16:17 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-05-01-%E6%89%8B%E5%8A%A8%E7%94%9F%E6%88%90ca%E8%AF%81%E4%B9%A6%E6%90%AD%E5%BB%BAfabric%E7%BD%91%E7%BB%9C/</guid>
      <description>亲测有效 # 【摘要】 之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具cryptogen直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内。
之前介绍了使用官方脚本自动化启动一个Fabric网络，并且所有的证书都是通过官方的命令行工具cryptogen直接生成网络中的所有节点的证书。在开发环境可以这么简单进行，但是生成环境下还是需要我们自定义对网络中的节点的证书进行配置。 所以在本文中，将会详细介绍一个从手动生成证书一直到启动网络的整体步骤。本文只交代整体的搭建步骤。对于Fabric-Ca的讲解不在本文的范围内，将在另一篇文章中说明。 在这里贴出官方文档地址.
1.整体架构 # 架构图直接贴过来好了： 官方文档采用的是多机环境，这里简洁化一点，所有的操作都在一台机器上进行，至于多机环境，以后再补充好了。 介绍一下本文所采用的整体架构：
三个组织 Org0 -&amp;gt; 组织0 Org1 -&amp;gt; 组织1 Org2 -&amp;gt; 组织2 组织中的成员 Org0 一个Orderer节点，一个Org0的Admin节点 Org1 两个Peer节点，一个Org1的Admin节点，一个Org1的User节点 Org2 两个Peer节点，一个Org2的Admin节点，一个Org2的User节点 共有四台CA服务器 TLS服务器 -&amp;gt; 为网络中所有节点颁发TLS证书，用于通信的加密 Org1的CA服务器 -&amp;gt; 为组织1中所有用户颁发证书 Org2的Ca服务器 -&amp;gt; 为组织2中所有用户颁发证书 Org0的CA服务器 -&amp;gt; 为组织0中所有用户颁发证书 这里的四台CA服务器都是根服务器。彼此之间都是独立的存在，没有任何关系。，也就是说每一个CA服务器生成的证书在其他CA服务器都是不能用的。 介绍完之后，可以进入正题了。
1.1Fabric，Fabric-Ca安装 # 本文默认读者都是对Fabric有一定的了解的，所以一些安装过程这里就没有重复说明。 第一步是安装Fabric-Ca环境，可以参考这里,这篇文章还没有写完，以后慢慢补，不过环境的安装已经有说明。 还有就是Fabric的环境安装，可以参考这里。
完成环境搭建后，我们还需要一个HOME文件夹，用于存放我们生成的证书文件与fabric配置相关的文件。 本文设置HOME文件夹路径为:
$GOPATH/src/github.com/caDemo/ 请读者自行创建，一般不要用太复杂的路径,也不要用中文路径，会为之后的操作带来很多麻烦。在下文中简单称HOME文件夹为工作目录,除非特殊说明，一般命令的执行都是在工作目录进行。
2 CA服务器配置 # 2.1启动TLS CA服务器 # 前期工作准备好之后，我们开始启动第一台CA服务器。本文中使用Docker容器启动。 首先在工作目录创建docker-compose.yaml文件：
touch docker-compose.yaml 并在文件内添加以下内容(tips:内容格式不要乱掉)：
version: &amp;#39;2&amp;#39; networks: fabric-ca: services: ca-tls: container_name: ca-tls image: hyperledger/fabric-ca command: sh -c &amp;#39;fabric-ca-server start -d -b tls-ca-admin:tls-ca-adminpw --port 7052&amp;#39; environment: - FABRIC_CA_SERVER_HOME=/ca/tls - FABRIC_CA_SERVER_TLS_ENABLED=true - FABRIC_CA_SERVER_CSR_CN=ca-tls - FABRIC_CA_SERVER_CSR_HOSTS=0.</description>
    </item>
    
    <item>
      <title>cryptogen生成的证书详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-05-01-cryptogen%E7%94%9F%E6%88%90%E7%9A%84%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Sat, 01 May 2021 14:40:46 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-05-01-cryptogen%E7%94%9F%E6%88%90%E7%9A%84%E8%AF%81%E4%B9%A6%E8%AF%A6%E8%A7%A3/</guid>
      <description>crypto-config # 用cryptogen生成证书后
peerOrganizations # 本文以peerOrganizations组织树为例，打开该目录，可以看到如下两个组织的证书目录：
org1.example.com # 每个组织中又有如下目录：
每个组织都会生成单独的根证书。
ca # ca ：存放了组织的根证书和对应的私钥文件，采用的是EC算法，证书为自签名（自已签发自己的公钥）。组织内的实体将基于该证书作为证书根。
map # msp：存放代表该组织的身份信息。
（1）admincerts：被根证书签名的组织管理员的身份验证证书。
（2）cacerts：组织的根证书，和ca目录下的文件相同。
（3）tlscacerts：用于TLS的ca证书，证书为自签名。
peer # peers：存放该组织下所有peer节点的证书：
peer0.org1.example.com # 每个peer节点的证书结构都是相同的，我们以peer0为例：
msp： # ​ admincerts：组织管理员的身份验证证书，用于验证交易签名者是否为管理员身份。
​ cacerts：存放组织的根证书。
​ keystore：本节点的身份私钥，用来签名。
​ signcerts： 验证本节点签名的证书，被组织根证书签名。
​ tlscacerts：TLS连接用的身份证书，即组织TLS证书。
tls: # 存放tls相关的证书和私钥。
​ ca.crt：组织的根证书。
​ server.crt：验证本节点签名的证书，被组织根证书签名。
​ server.key：本节点的身份私钥，用来签名。
users # users：存放属于该组织的用户实体。
Admin@org1.example.com # Admin：管理员用户的信息，包括其msp证书和tls证书。
msp： # ​
​ admincerts：管理员身份证书。
​ cacerts：存放组织的根证书。
​ keystore：本用户的身份私钥，用来签名。
​ signcerts： 管理员用户的身份验证证书，由组织根证书签名，要放到Peer的msp/admincerts下才会被这个Peer认可。
​ tlscacerts：TLS连接用的身份证书，即组织TLS证书。
tls： # 存放TLS相关的证书和私钥。</description>
    </item>
    
    <item>
      <title>docker常用知识总结</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/2021-04-30-docker%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</link>
      <pubDate>Fri, 30 Apr 2021 19:43:22 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/docker/2021-04-30-docker%E5%B8%B8%E7%94%A8%E7%9F%A5%E8%AF%86%E6%80%BB%E7%BB%93/</guid>
      <description>docker常用基础命令 # docker rmi -f $(docker images -q) 删除镜像docker rm -f .... 删除容器docker exec -it ca.org1.example.com bash 进入容器docker exec -it peer0.org1.example.com shexit 退出容器control+P+Q 退出容器docker stop $(docker ps -q) 停止所有容器docker rm $(docker ps -aq) 删除所有容器sudo docker volume prune sudo docker network prunedocker logs id 查看docker容器日志 docker文件管理 # docker cp 容器 ID 或名称: 容器目录 物理机目录 docker目录拷贝到物理机docker cp 物理机目录 容器 ID 或名称: 容器目录 物理机目录拷贝到dockerdocker cp /home/lishuma b2860e937844:/home/如果是把上一条命令结尾斜杠去掉，那么意思就变成了将物理机/home/lishuma 目录拷贝到容器根目录中，并且拷贝进去的目录重命名为 home。docker cp b2860e937844:/home/lishuma /home/lishuma/test/反过来容器向外拷贝的命令如果去掉最后一个斜杠，那么意思同样是变成拷贝出来后，重命名为 test。 docker文件挂载 # docker run -v /home/tianzhiwei/hyperledger/catest/crypto-config/peerOrganizations/:/etc/hyperledger/fabric-ca-server-config/msp hyperledger/fabric-ca:1.</description>
    </item>
    
    <item>
      <title>MySql基础总结</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/2021-04-20-mysql%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</link>
      <pubDate>Tue, 20 Apr 2021 18:07:08 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/2021-04-20-mysql%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</guid>
      <description>基础概念 # 数据库：是一个以某种有组织的方式存储的数据集合。
表：是一种结构化的文件，可用来存储某种特定类型的数据。
模式：
模式可以用来描述数据库中特定的表以及整个数据库（和其中表的关系） 关于数据库和表的布局及特性的信息 列：表中的一个字段。数据库中每个列都有相应的数据类型，数据类型定义列可以存储的数据种类。
行：表中的数据是按行存储的，表中的一个记录。
**主键：**一列（或一组列），其值能够唯一区分表中每个行。主键用来表示一个特定的行。
满足主键的条件
任意两行都不具有相同的主键值； 每个行都必须具有一个主键值（主键列不允许NULL值） 可以一起使用多个列作为主键。
SQL structured query language 结构化查询语言。
数据库的发展史 # 第一代数据库：层次模型、网状模型
层次模型
缺点：
1、 查找不同类的数据效率低了（导航的结构的缺点）
2、 数据不完整（不能区分到底是一个李白还是两个李白）
网状模型
网状模型解决了层次数据的数据不完整的问题，但是没有解决层次模型的导航问题。
关系型数据库
特点：
每个表都是独立的
表与表之间通过公共字段来建立关系
优点：解决了导航问题，并且数据完整性得到解决
缺点：多表查询效率低了
提示：我们现在用的主流的数据库都是关系模型的。
MySql安装 # 在Ubuntu中，默认情况下，只有最新版本的MySQL包含在APT软件包存储库中,要安装它，只需更新服务器上的包索引并安装默认包apt-get。
#命令1sudo apt-get update#命令2sudo apt-get install mysql-server 初始化配置 # sudo mysql_secure_installation 配置项较多，如下所示：
#1VALIDATE PASSWORD PLUGIN can be used to test passwords...Press y|Y for Yes, any other key for No: N (我的选项)#2Please set the password for root here.</description>
    </item>
    
    <item>
      <title>如何在已有组织中增加节点</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-17-%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%B2%E6%9C%89%E7%BB%84%E7%BB%87%E4%B8%AD%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</link>
      <pubDate>Sat, 17 Apr 2021 19:04:25 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-17-%E5%A6%82%E4%BD%95%E5%9C%A8%E5%B7%B2%E6%9C%89%E7%BB%84%E7%BB%87%E4%B8%AD%E5%A2%9E%E5%8A%A0%E8%8A%82%E7%82%B9/</guid>
      <description>fabric网络在创建时就已经确定了初始的节点数量，而在实际应用场景中可能会需要在某个组织中动态增加节点。
这里讲述两种方式 # 一种是cryptogen工具生成新节点加入到网络中去（现实没有意义）
一种是用fabric-ca生成新节点加入到网络中去
方法一：cryptogen工具 # 一、追加新节点的身份信息 # 在这之前可参照fabric solo节点测试搭建一个fabric网络
首先需要在组织org1的MSP目录中追加新节点的证书和私钥信息，主要是用到cryptogen工具
1.修改crypto-config.yaml文件（或者直接新建一个文件）中Template字段里的count参数，设置为需要该组织中存在的节点总数,可一次增加多个节点。
这里只在org1加入一个节点，所以crypto-config.yaml文件修改部分如下：
PeerOrgs: - Name: Org1 Domain: org1.example.com EnableNodeOUs: true Template: Count: 2 #之前是1 Users: Count: 1 2.执行extend命令完成追加操作 在此文件目录下执行：
cryptogen extend --config=./crypto-config.yaml 可在crypto-config/peerOrganizations/org1.example.com/peers/下发现新增加的peer1.org1.example.com文件夹
注：&amp;ndash;config参数应以实际情况下配置文件的名称及路径为准
3.启动容器
在docker-compose.yaml文件中加入新节点信息
version: &amp;#39;2&amp;#39;volumes:orderer.example.com:peer0.org1.example.com:peer1.org1.example.com: //加这里networks:test: peer1.org1.example.com: container_name: peer1.org1.example.com image: hyperledger/fabric-peer:latest environment: #Generic peer variables - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock # the following setting starts chaincode containers on the same # bridge network as the peers # https://docs.</description>
    </item>
    
    <item>
      <title>Fabric1.4多通道实验</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-15-fabric1.4%E5%A4%9A%E9%80%9A%E9%81%93%E5%AE%9E%E9%AA%8C/</link>
      <pubDate>Thu, 15 Apr 2021 16:11:57 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/2021-04-15-fabric1.4%E5%A4%9A%E9%80%9A%E9%81%93%E5%AE%9E%E9%AA%8C/</guid>
      <description>Hyperledger Fabric支持在一组相同的机构之间的多通道部署， 每个通道都相当于一个单独的区块链。Fabric的多通道特性 不仅可以满足机构之间不同的数据共享需求，同时也可以提高 整个Fabric网络的吞吐量。本文将演示如何使用Hyperledger Fabric 1.4.3搭建一个多通道的区块链网络、部署并访问链码。
1、Hyperledger Fabric多通道网络实验环境概述 # 我们将构造一个包含3个机构的Hyperledger Fabric网络：Org1、Org2和Org3， 每个机构中包含一个节点Peer0。网络包含两个通道：由Org1、 Org2和Org3组成的ChannelAll，以及由Org1和Org2组成的Channel12，因此 这个Fabric网络是多通道的配置。在这两个Fabric通道上我们将部署同样的链码， 即Fabrc-Samples中提供的Simple Asset链码：
2、Hyperledger Fabric多通道网络实验环境搭建 # Step 1：在Hyperledger官方提供的fabric-samples目录下克隆本教程提供的示例代码：
cd fabric-samplesgit clone https://github.com/kctam/3org2ch_143.gitcd 3org2ch_143 Step 2：为参与Fabric通道的机构生成所需的密码学资料
../bin/cryptogen generate --config=./crypto-config.yaml Step 3：生成Fabric通道素材
mkdir channel-artifacts &amp;amp;&amp;amp; export FABRIC_CFG_PATH=$PWD../bin/configtxgen -profile OrdererGenesis \-outputBlock ./channel-artifacts/genesis.blockexport CHANNEL_ONE_NAME=channelallexport CHANNEL_ONE_PROFILE=ChannelAllexport CHANNEL_TWO_NAME=channel12export CHANNEL_TWO_PROFILE=Channel12../bin/configtxgen -profile ${CHANNEL_ONE_PROFILE} \-outputCreateChannelTx ./channel-artifacts/${CHANNEL_ONE_NAME}.tx \-channelID $CHANNEL_ONE_NAME../bin/configtxgen -profile ${CHANNEL_TWO_PROFILE} \-outputCreateChannelTx ./channel-artifacts/${CHANNEL_TWO_NAME}.tx \-channelID $CHANNEL_TWO_NAME.</description>
    </item>
    
    <item>
      <title>椭圆曲线加密</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2021-04-12-%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86/</link>
      <pubDate>Mon, 12 Apr 2021 12:17:45 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2021-04-12-%E6%A4%AD%E5%9C%86%E6%9B%B2%E7%BA%BF%E5%8A%A0%E5%AF%86/</guid>
      <description>通过椭圆曲线加密实现数字签名 # 私钥公钥如何产生？ # 随机生成一个256位的二进制数
11011100111110101100101010000100111100101000011&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;
dcfaca84f325f65a&amp;hellip;,&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip;&amp;hellip; 16进制
一、为什么叫椭圆曲线 # 圆锥曲线可以用二次方程表示。椭圆曲线是用三次方程表示，如下： 其中，a 和 b 的取值不同，椭圆曲线的形状会有所改变，经典的形状如下图所示：
椭圆曲线有以下两个特点：
画一条直线跟椭圆曲线相交，它们最多有三个交点； 关于 X 轴对称。 A（x,y) k* A
二、椭圆曲线运算法则 # 1. 椭圆曲线加法 # 根据上面介绍的椭圆曲线的特性“画一条直线跟椭圆曲线相交，它们最多有三个交点”，可以进行以下定义：
假设椭圆曲线上有 P、Q 两个点，经过这两个点做一条直线和椭圆曲线相交于第三点 R，然后做关于 x 轴的对称点 -R，-R 即是 R 的逆元，根据阿贝尔群的定义，-R 也一定在椭圆曲线上。定义 P+Q = -R，也就是说椭圆曲线上任意两点的和也在椭圆曲线上，同样可以引申出椭圆曲线上任意三点的和为 0 即 P+Q+R = 0。如图：
假如 P=Q，则作椭圆曲线在 P 点的切线，与曲线相交于 R，则 R = P+P = 2P 2. 椭圆曲线乘法 # 根据上面椭圆曲线的加法可以得出下列等式：P+P = 2P（过点 P 切线作一条直线）P+2P = 3P（过点 P 和 2P 作一条直线）P+3P = 4P（过点 P 和 3P 作一条直线）假设 P 是椭圆曲线上的一个点，正整数 K 乘以 P 可以总结成公式为：(k-1) * P + P = k * P如果把 k 看作是两个数相乘即 k = m * n，则可以得出满足以下性质（在椭圆曲线密钥交换中会用到）：(m * n) * P = m * (n * P) = (n * m)p = n * (m*P)</description>
    </item>
    
    <item>
      <title>config.yaml文件详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-30-config-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 30 Mar 2021 15:47:11 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-30-config-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid>
      <description>基于fabric 2.3 # 客户端使用sdk与fabric网络交互，需要告诉sdk两类信息：
我是谁：即当前客户端的信息，包含所属组织、密钥和证书文件的路径等， 这是每个客户端专用的信息。 对方是谁：即fabric网络结构的信息，channel、org、orderer和peer等 的怎么组合起当前fabric网络的，这些结构信息应当与configytx.yaml中是一致的。这是通用配置，每个客户端都可以拿来使用。另外，这部分信息并不需要是完整fabric网络信息，如果当前客户端只和部分节点交互，那配置文件中只需要包含所使用到的网络信息。 原文件 # 我们复制官方的config_e2e_multiorg_bootstrap.yaml文件
文件位置：https://github.com/hyperledger/fabric-sdk-go/blob/main/test/fixtures/config/config_e2e_multiorg_bootstrap.yaml
######################## 声明部分 ############################### # Copyright SecureKey Technologies Inc. All Rights Reserved. # 版权所有 SecureKey Technologies Inc. 保留所有权利。 # SPDX-License-Identifier: Apache-2.0 # # The network connection profile provides client applications the information about the target blockchain network that are necessary #for the applications to interact with it. These are all knowledge #that must be acquired from out-of-band sources. This file provides #such a source.</description>
    </item>
    
    <item>
      <title>docker-compose.yaml文件详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-30-docker-compose-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Tue, 30 Mar 2021 15:46:41 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-30-docker-compose-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid>
      <description>基于fabric2.3
原文件 # # Copyright IBM Corp. All Rights Reserved.## SPDX-License-Identifier: Apache-2.0#version: &amp;#39;2&amp;#39;volumes: # 数据卷映射, 本地 -&amp;gt; docker镜像orderer.example.com:peer0.org1.example.com:peer1.org1.example.com:networks: # 指定容器运行的网络, 同一网络中的容器才能相互通信test:services:orderer.example.com: # 定义的第1个服务名container_name: orderer.example.com # 容器名称, 可以自定义image: hyperledger/fabric-orderer:latestenvironment: # 环境变量设置- FABRIC_LOGGING_SPEC=DEBUG #日志级别- ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 #orderer节点监听的地址- ORDERER_GENERAL_LISTENPORT=7050 #orderer默认监听7050，端口可修改- ORDERER_GENERAL_GENESISMETHOD=file #创世块的来源，file表示来源于文件#指定创世块文件路径- ORDERER_GENERAL_GENESISFILE=/var/hyperledger/orderer/orderer.genesis.block- ORDERER_GENERAL_LOCALMSPID=OrdererMSP #这个ID不一样会出问题- ORDERER_GENERAL_LOCALMSPDIR=/var/hyperledger/orderer/msp #msp账号路径# enabled TLS- ORDERER_GENERAL_TLS_ENABLED=true #通信时是否使用TLS加密#私钥文件- ORDERER_GENERAL_TLS_PRIVATEKEY=/var/hyperledger/orderer/tls/server.</description>
    </item>
    
    <item>
      <title>configtx.yaml文件详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-29-configtx-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Mon, 29 Mar 2021 19:38:28 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-29-configtx-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid>
      <description>基于fabric2.3
原文件 # ---Organizations: #部分指定OrdereOrg与PeerOrg的组织信息- &amp;amp;OrdererOrg #相当于定义了一个变量，其他地方可以引用Name: OrdererOrg #组织名称#将MSP定义加载为IDID: OrdererMSP #MSP的IDMSPDir: crypto-config/ordererOrganizations/example.com/msp #MSP配置文件的路径Policies: #组织策略， 其中`Rule`定义了规则，`OR`为或，`AND`为并Readers: Type: SignatureRule: &amp;#34;OR(&amp;#39;OrdererMSP.member&amp;#39;)&amp;#34;Writers:Type: SignatureRule: &amp;#34;OR(&amp;#39;OrdererMSP.member&amp;#39;)&amp;#34;Admins:Type: SignatureRule: &amp;#34;OR(&amp;#39;OrdererMSP.admin&amp;#39;)&amp;#34; #Admins策略只能由管理员角色的身份提交的事务来满足#OrdererEndpoints是所有orderers这个组织运行，其客户名单和同级可以分别连接以推送事务和接收块。OrdererEndpoints:- orderer.example.com:7050- &amp;amp;Org1Name: Org1MSPID: Org1MSPMSPDir: crypto-config/peerOrganizations/org1.example.com/mspPolicies:Readers:Type: SignatureRule: &amp;#34;OR(&amp;#39;Org1MSP.admin&amp;#39;, &amp;#39;Org1MSP.peer&amp;#39;, &amp;#39;Org1MSP.client&amp;#39;)&amp;#34;Writers:Type: SignatureRule: &amp;#34;OR(&amp;#39;Org1MSP.admin&amp;#39;, &amp;#39;Org1MSP.client&amp;#39;)&amp;#34;Admins:Type: SignatureRule: &amp;#34;OR(&amp;#39;Org1MSP.admin&amp;#39;)&amp;#34;Endorsement: #有具有对等角色的身份才能满足该Endorsement策略Type: SignatureRule: &amp;#34;OR(&amp;#39;Org1MSP.</description>
    </item>
    
    <item>
      <title>crypto-config.yaml文件详解</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-29-crypto-config-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</link>
      <pubDate>Mon, 29 Mar 2021 18:48:08 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6/2021-03-29-crypto-config-yaml%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3/</guid>
      <description>基于fabric2.3
源码 # OrdererOrgs: #排序节点组织信息- Name: Orderer #排序节点组织名Domain: example.com #排序节点组织根域名EnableNodeOUs: false #指定是否生成config.yaml文件Specs:- Hostname: orderer #hostname+domain组成orderer节点的完整域名PeerOrgs: #对等节点组织信息- Name: Org1 #第一个组织名，自己起Domain: org1.example.com #第一个组织根域名EnableNodeOUs: false #在msp下生成config.yaml文件Template: #组织中peer节点的数目Count: 1Users: #组织中普通用户的数目Count: 1- Name: Org2Domain: org2.example.comEnableNodeOUs: falseTemplate:Count: 1Users:Count: 1 使用以下命令生成证书文件。
cryptogen工具 # 子命令 # generate：生成的组织结构及身份证书信息。
showtemplate：显示默认配置模版
version：显示版本信息
参数 # &amp;ndash;config ：指定要使用的配置模版文件
&amp;ndash;output；指定生成内容的输出目录
cryptogen generate --config=crypto-config.yaml Fabric证书文件结构</description>
    </item>
    
    <item>
      <title>centos安装fabric1.2</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-18-centos%E5%AE%89%E8%A3%85fabric1.2/</link>
      <pubDate>Thu, 18 Mar 2021 16:04:53 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/fabric/%E7%8E%AF%E5%A2%83%E6%B5%8B%E8%AF%95/2021-03-18-centos%E5%AE%89%E8%A3%85fabric1.2/</guid>
      <description>一、环境安装 # 1、安装基本工具 # yum install curl 2、安装docker # 2.1确保yum包更新到最新 # yum update -y 2.2 对服务器进行清理， 如果之前安装过Docker ， 需要先执行卸载操作，具体命令 # sudo yum remove docker docker-client docker-client-latest docker-common docker-latest docker-latest-logrotate docker-logrotate docker-selinux docker-engine-selinux docker-engine 2.3 安装需要的软件包： # yum install -y yum-utils device-mapper-persistent-data lvm2 2.4添加docker yum 源 # sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 2.5安装docker # yum install docker-ce -y 2.6查看docker版本信息，是否安装成功 # docker --version 2.7 docker基本命令 # 启动docker：
systemctl start docker 停止docker：
systemctl stop docker 重启docker：</description>
    </item>
    
    <item>
      <title>密码学基础</title>
      <link>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2021-03-04-%E5%AF%86%E7%A0%81%E5%AD%A6%E5%9F%BA%E7%A1%80/</link>
      <pubDate>Thu, 04 Mar 2021 15:29:17 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E5%8C%BA%E5%9D%97%E9%93%BE/%E5%AF%86%E7%A0%81%E5%AD%A6/2021-03-04-%E5%AF%86%E7%A0%81%E5%AD%A6%E5%9F%BA%E7%A1%80/</guid>
      <description>DES 数据加密标准 # 不安全 ，分组密码，8
3DES # 安全，进行了3次des加密
加密过程：加密，解密，加密
解密过程：解密，加密，解密
CBC 密码块链模式 # 特点：密文没有规律，经常使用
最后一个明文分组需要填充
需要初始化向量-一个数组
明文分组的填充 刚好够也需要填充 填充明文分组代码实现 # package main //编写填充函数，如果最后一个分组字数不够，填充 //、、、、、字数刚好合适，添加一个新的分组 //填充的字节的值==缺少的字节数 func paddingLastGroup(plainText []byte, bloclSize int) []byte { //plainText 参数：明文 bloclSize 明文分组字节长度 []byte 返回值 //1、求出最后一个组中剩余的字节数 28%8=3..4 32%8=4.。0 padNum:=ploclSize-len(plainText)%bloclSize //填充的字数 //2、创建新的切片，长度==padNum, 每个字节值byte(padNum) char :=[]byte{byte(padNum)} //长度1， //切片创建，并初始化 newPlan := bytes.Repeat(char,padNum) //3、newPlain数组追加到原始明文的后边 newText := append(plainText,newPlain..) return newText } 删除尾部明文分组实现 # func unPaddingLastGrooup(plainText []byte) []byte { //1、拿去切片中的最后一个字节 length := len(plainText) lastChar :=plainText[length -1] //byte 类型 number :=int (lastChar) //尾部填充的字节个数 return plainText[:length -number] } 对称加密实现（go） # #加密流程： 1、创建一个底层使用des/3des/aes的密码接口 &amp;#34;crypto/des&amp;#34; func NewCipher(key []byte) (cipher.</description>
    </item>
    
    <item>
      <title>个人博客搭建Hexo</title>
      <link>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BAhexo/</link>
      <pubDate>Mon, 11 Jan 2021 16:15:02 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B0/%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BAhexo/</guid>
      <description>基于Mac # 所需环境 # 一 安装git # 二 安装node.js # # 首先检查时候安装了git和node.js，终端输入一下命令，node -v #是否出现安装版本信息，出现说明已经安装了git --version #同上述情况# 如果没有安装，则进行安装,都可以通过直接下载安装测序进行安装，这里不演示，提供下载网址：[git]: https://sourceforge.net/projects/git-osx-installer/[node.js]: https://nodejs.org/en/ 三 安装hexo # npm install -g hexo-cli 创建blog文件夹，并初始化建立博客框架 # 在你的家目录下创建一个blog文件夹mkdir blog# 进入目录cd blog# 初始化目录hexo init开启本地服务 # hexo s 出现 http://localhost:4000 可以在浏览器输入网址访问查看效果
现在 整个hexo 博客已经部署完成
设置主题 # 一 克隆GitHub文件到blog/themes文件夹下
git clone -b master https://github.com/jerryc127/hexo-theme-butterfly.git themes/butterfly 二 在 Hexo 网站根目录中输入
npm i hexo-theme-butterfly 三 在hexo工作文件夹的根配置文件_config.</description>
    </item>
    
    <item>
      <title>Mac连接数据库所遇到的问题</title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mac%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</link>
      <pubDate>Mon, 11 Jan 2021 16:12:38 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/mysql/mac%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%89%80%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</guid>
      <description>在Mac上安装好之后，在系统偏好设置里找到mysql，点击并选择启动mysql；
打开终端面板，输入：mysql -u root -p
问题来了，因为之后显示的是：-bash: mysql: command not found
方法如下：
1.在你的Mac终端,输入： cd ~
会进入~文件夹
2.然后输入：touch .bash_profile
回车执行后，
3.再输入：open -e .bash_profile
这时候会出现一个TextEdit，如果以前没有配置过环境变量，呈现在你眼前的就是一个空白文档，你需要在这个空白文档里输入：export PATH=$PATH:/usr/local/mysql/bin
然后关闭这个TextEdit
4.继续回到终端面板，输入：source ~/.bash_profile
以上，问题就解决啦！！！
现在你再输入：mysql -u root -p
回车后就会显示：Enter password:</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/ai/basic/ai%E7%9F%A5%E8%AF%86%E6%99%AE%E5%8F%8A/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/basic/ai%E7%9F%A5%E8%AF%86%E6%99%AE%E5%8F%8A/</guid>
      <description>title: &amp;#34;AI知识普及&amp;#34; weight: 1 # bookFlatSection: false # bookToc: true # bookHidden: false # bookCollapseSection: false # bookComments: false # bookSearchExclude: false AI的起源 # 人工神经网络的萌芽（1950年代） # 人工智能概念的提出后，发展出了符号主义、联结主义(神经网络)，相继取得了一批令人瞩目的研究成果，如机器定理证明、跳棋程序、人机对话等，掀起人工智能发展的第一个高潮。
1943年：首次出现神经网络理论。 1957年：首个人工神经网络模型“感知器”被提出。 停滞期开始（1960年代-1980年代） # 人工智能发展初期的突破性进展大大提升了人们对人工智能的期望，人们开始尝试更具挑战性的任务，然而计算力及理论等的匮乏使得不切实际目标的落空，人工智能的发展走入低谷。
1969年：提出“感知器”的局限性。 1986年：提出多层感知器理论。 人工神经网络的复苏（1990年代-2000年代） # 人工智能走入应用发展的新高潮。专家系统模拟人类专家的知识和经验解决特定领域的问题，实现了人工智能从理论研究走向实际应用、从一般推理策略探讨转向运用专门知识的重大突破。而机器学习(特别是神经网络)探索不同的学习策略和各种学习方法，在大量的实际应用中也开始慢慢复苏。
2006年：提出深度信念网络结构。 人工智能的平稳发展（20世纪90年代—2010年） # 由于互联网技术的迅速发展，加速了人工智能的创新研究，促使人工智能技术进一步走向实用化，人工智能相关的各个领域都取得长足进步。在2000年代初，由于专家系统的项目都需要编码太多的显式规则，这降低了效率并增加了成本，人工智能研究的重心从基于知识系统转向了机器学习方向。
深度学习的发展（2010年代-2020年代） # 随着大数据、云计算、互联网、物联网等信息技术的发展，泛在感知数据和图形处理器等计算平台推动以深度神经网络为代表的人工智能技术飞速发展，大幅跨越了科学与应用之间的技术鸿沟，诸如图像分类、语音识别、知识问答、人机对弈、无人驾驶等人工智能技术实现了重大的技术突破，迎来爆发式增长的新高潮。
2012年：AlexNet在ILSVRC竞赛中夺冠，标志着深度学习的突破。 2016年：阿尔法围棋（AlphaGo）赢得围棋比赛，展示了深度学习的强大能力。 生成式人工智能热潮的开始（2020年代至今） # 2022年，生成式人工智能（Generative AI）迎来了突破性进展，特别是在自然语言处理、图像生成、代码生成等领域取得显著成果。以OpenAI发布的GPT-3、GPT-4为代表的大型语言模型，能够生成与人类语言高度相似的文本内容，广泛应用于自动写作、对话系统、文本摘要等场景。同时，扩散模型（Diffusion Model）在图像生成领域崭露头角，能够生成高度逼真的图像和多模态内容，推动了艺术创作、虚拟现实、游戏设计等行业的变革。生成式人工智能的崛起标志着人工智能进入一个更加智能化、创意化、个性化的新阶段，正在深刻影响各行各业的发展。
人工智能，机器学习，深度学习等范围和概念 # 神经网络 # 神经网络(Artificial Neural Networks)：人工神经网络的简称， 是一种应用类似于大脑神经突触联接的结构或网络，进行信息处理的数学模型 。神经网络是一门重要机器学习技术，它是目前最火热的研究方向—深度学习之基础。
神经元 一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。
单层神经网络数学模型 1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP，神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。具体的神经元模型如下图所示：
一个简单神经元模型中每一个有向箭头线称为 连接 ；每一个连接上有一个值，称为权值或权重。连接是神经元中最重要的东西。每一个连接上都有一个权重。一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。
深度神经网络架构 深度神经网络又名深度学习网络，拥有多个隐藏层，包含数百万个链接在一起的人工神经元。名为权重的数字代表节点之间的连接。如果节点之间相互激励，则该权重为正值，如果节点之间相互压制，则该权重为负值。节点的权重值越高，对其他节点的影响力就越大。
代码示例 # 通过简单的神经网络来拟合二元一次方程</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/ai/computer-vision/yolov8%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B&#43;%E5%AE%9E%E8%B7%B5/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/computer-vision/yolov8%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B&#43;%E5%AE%9E%E8%B7%B5/</guid>
      <description>title: &amp;#34;YOLOv8快速上手+实践&amp;#34; weight: 2 # bookFlatSection: false # bookToc: true # bookHidden: false # bookCollapseSection: false # bookComments: false # bookSearchExclude: false YOLOv8快速上手+实践 # 前言 # 本文旨在快速上手并不涉及细致的训练超参数调优和YOLO源码层面的解析。
YOLOv8是YOLO家族中流行的实时目标检测系统，以其快速、准确和高效的特性在计算机视觉领域中广泛应用（目前YOLO的发展很快，YOLOv10就在前不久也已经正式发布）。本文将详细介绍如何在NVIDIA GPU环境下部署YOLOv8，从环境配置、库安装，到模型训练和应用的全流程操作，并在其中结合实际的火焰特征识别的实践。
环境部署（N卡） # 需要提前准备好要使用的Python环境，此步骤不再赘述
安装和配置CUDA # 前往nvidia的开发者网站，选择下载CUDA toolkit
先检查一下本地环境显卡驱动支持的最高CUDA版本，查看的CUDA toolkit 版本不能高于显卡驱动支持的最高版本
方式一：打开N卡的控制面板，在系统信息的组件里
方式二：使用命令nvidia-smi查看CUDA版本
其次建议要下载前先确认下准备使用的Pytorch版本，尽量CUDA toolkit的版本和Pytorch支持的保持一致，起码不能使用低版本
最新版的CUDA：https://developer.nvidia.com/cuda-downloads 历史版本：https://developer.nvidia.com/cuda-toolkit-archive 跟着安装程序走即可，最后检查一下安装是否成功：
nvcc --version 成功输出版本信息即为成功
【可选】下载&amp;amp;安装CUDNN库 # cuDNN 是用于深度神经网络的 GPU 加速库
继续回到之前的N卡开发者网站上，需要注册登录后才能下载
最新版本：https://developer.nvidia.com/cudnn 历史版本：https://developer.nvidia.com/rdp/cudnn-archive 下载的版本也需要和CUDA的大版本一一对应
下载下来的CUDNN库包括bin、include和lib目录，将目录下对应的所有文件复制到之前CUDA toolkit
的安装目录下即可
安装PyTorch # 官网：https://pytorch.org/get-started/locally/
选择自己环境的配置项，复制pip或者conda的命令来安装即可
pip3 install torch torchvision torchaudio --index-url https://download.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/ai/generative-ai/%E5%88%A9%E7%94%A8dspy%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90prompt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/ai/generative-ai/%E5%88%A9%E7%94%A8dspy%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90prompt/</guid>
      <description>title: &amp;#34;利用DSPy自动生成Prompt&amp;#34; weight: 1 # bookFlatSection: false # bookToc: true # bookHidden: false # bookCollapseSection: false # bookComments: false # bookSearchExclude: false 什么是DSPy # 这里直接引用文章 DSPy 入门： 再见提示，你好编程 中的介绍，详细内容请看原文
DSPy（&amp;ldquo;Declarative Self-improving Language Programs (in Python)&amp;quot;，发音为 &amp;ldquo;dee-es-pie&amp;rdquo;）是斯坦福大学 NLP 研究人员开发的 &amp;ldquo;基础模型编程 &amp;ldquo;框架。它强调编程而非提示，并将构建基于 LM 的管道从操作提示转向编程。因此，它旨在解决构建基于 LM 应用程序时的脆弱性问题。
DSPy Github
可以参考官方Github先安装一下
数据准备 # 我利用豆包生成了一部分数据
[ { &amp;#34;query&amp;#34;: &amp;#34;腾讯2021年和2022年分别盈利多少？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;腾讯2021年盈利多少？&amp;#34;, &amp;#34;腾讯2022年盈利多少？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;苹果公司和微软公司哪个市值更高？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;苹果公司的市值是多少？&amp;#34;, &amp;#34;微软公司的市值是多少？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;为什么电动汽车越来越受欢迎？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;电动汽车的市场份额如何变化？&amp;#34;, &amp;#34;电动汽车有哪些优势？&amp;#34;, &amp;#34;政策和基础设施如何支持电动汽车发展？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;什么是机器学习？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;机器学习的定义是什么？&amp;#34;, &amp;#34;机器学习有哪些主要类型？&amp;#34;, &amp;#34;机器学习的应用领域有哪些？&amp;#34;, &amp;#34;机器学习与人工智能的关系是什么？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;如何制定一个有效的健身计划？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;制定健身计划前需要评估哪些身体指标？&amp;#34;, &amp;#34;健身目标有哪些类型？&amp;#34;, &amp;#34;如何根据目标选择适合的训练方式？&amp;#34;, &amp;#34;如何安排训练频率和强度？&amp;#34;, &amp;#34;如何制定合理的饮食计划？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;华为、苹果和三星在智能手机市场的份额分别是多少？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;华为在智能手机市场的份额是多少？&amp;#34;, &amp;#34;苹果在智能手机市场的份额是多少？&amp;#34;, &amp;#34;三星在智能手机市场的份额是多少？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;2023年全球票房最高的电影是哪部，它的导演是谁，票房收入是多少？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;2023年全球票房排名如何？&amp;#34;, &amp;#34;2023年全球票房最高的电影是哪部？&amp;#34;, &amp;#34;这部电影的导演是谁？&amp;#34;, &amp;#34;这部电影的全球票房收入是多少？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;世界上最高的三座山峰分别是什么，它们的海拔高度是多少，位于哪个国家或地区？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;世界上海拔最高的山峰有哪些？&amp;#34;, &amp;#34;世界上最高的三座山峰分别是什么？&amp;#34;, &amp;#34;这三座山峰的海拔高度分别是多少？&amp;#34;, &amp;#34;这三座山峰分别位于哪个国家或地区？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;如果一个正方形的边长增加20%，那么它的面积会增加多少百分比？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;正方形的面积公式是什么？&amp;#34;, &amp;#34;边长增加20%后新的边长是多少？&amp;#34;, &amp;#34;新的面积是多少？&amp;#34;, &amp;#34;面积增加的百分比如何计算？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;请列出所有位于北半球、人口超过1000万且属于发达国家的城市。&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;如何确定城市是否位于北半球？&amp;#34;, &amp;#34;哪些城市的人口超过1000万？&amp;#34;, &amp;#34;如何定义发达国家？&amp;#34;, &amp;#34;如何筛选同时满足这三个条件的城市？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;从经济学和环境科学的角度分析，推广电动汽车对减少碳排放和促进经济发展有哪些影响？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;电动汽车如何减少碳排放？&amp;#34;, &amp;#34;推广电动汽车的经济成本和效益是什么？&amp;#34;, &amp;#34;电动汽车产业对经济发展有哪些促进作用？&amp;#34;, &amp;#34;如何平衡环保目标和经济发展需求？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;企业在决定是否推出新产品时，应该考虑哪些因素？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;市场需求和竞争情况如何？&amp;#34;, &amp;#34;新产品的研发成本和生产难度如何？&amp;#34;, &amp;#34;新产品的营销策略和渠道有哪些？&amp;#34;, &amp;#34;新产品的潜在风险和回报如何？&amp;#34;, &amp;#34;企业的资源和能力是否支持新产品的推出？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;分析某城市过去十年的人口变化情况，并预测未来五年的人口趋势。&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;如何获取该城市过去十年的人口数据？&amp;#34;, &amp;#34;人口变化的主要原因是什么？&amp;#34;, &amp;#34;如何分析人口增长或减少的趋势？&amp;#34;, &amp;#34;有哪些因素可能影响未来人口趋势？&amp;#34;, &amp;#34;如何建立人口预测模型？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;解释为什么植物需要阳光进行光合作用。&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;什么是光合作用？&amp;#34;, &amp;#34;阳光在光合作用中的作用是什么？&amp;#34;, &amp;#34;光合作用的化学过程是什么？&amp;#34;, &amp;#34;植物如何捕获和利用光能？&amp;#34;, &amp;#34;光合作用对植物生长和生存的重要性是什么？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;为什么越来越多的年轻人选择独居生活？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;独居生活的定义是什么？&amp;#34;, &amp;#34;独居生活在年轻人中的流行趋势如何？&amp;#34;, &amp;#34;经济因素如何影响年轻人的居住选择？&amp;#34;, &amp;#34;社会观念和价值观的变化如何影响独居现象？&amp;#34;, &amp;#34;独居生活对个人和社会有哪些影响？&amp;#34; ] }, { &amp;#34;query&amp;#34;: &amp;#34;如何设计一个吸引人的用户界面？&amp;#34;, &amp;#34;answer&amp;#34;: [ &amp;#34;用户界面设计的基本原则有哪些？&amp;#34;, &amp;#34;如何了解目标用户的需求和偏好？&amp;#34;, &amp;#34;如何选择合适的颜色、字体和布局？&amp;#34;, &amp;#34;如何设计直观的导航和交互元素？&amp;#34;, &amp;#34;如何测试和优化用户界面设计？&amp;#34; ] } ] 数据读取 # import dspy with open(&amp;#34;test.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E5%AE%89%E5%85%A8%E6%8C%87%E5%8D%97/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/go%E5%AE%89%E5%85%A8%E6%8C%87%E5%8D%97/</guid>
      <description>title: &amp;#34;go安全指南&amp;#34; weight: 1 # bookFlatSection: false # bookToc: true # bookHidden: false # bookCollapseSection: false # bookComments: false # bookSearchExclude: false go安全指南 # 腾讯发布了Go语言代码安全指南 - 知乎 (zhihu.com)
GORM 食谱 — 研发期刊（第二期） 1.0.0 文档 (forensix.cn)
CGO的避坑和流畅使用 — 研发期刊（第一期） 1.0.0 文档 (forensix.cn)
Go线程安全双向链表 — 研发期刊（第一期） 1.0.0 文档 (forensix.cn)
包概念、包特点、包名约束、main 包、包的声明、包的引用、包初始化 # https://blog.csdn.net/wohu1104/article/details/104387100
MySql主从配置 # 从数据库读取日志文件，进行相应操作，复制主数据库内容，保证内容一致性。
docker 命令 # sudo netstat -antup | grep docker 显示当前系统上与 Docker 相关的 TCP 和 UDP 连接，并显示与这些连接关联的进程标识符（PID）和程序名称。 netstat: netstat 是一个用于显示网络连接信息的命令。它可以显示当前系统上的网络连接、监听端口、路由表等信息。 -antup: 这是 netstat 命令的选项参数。具体含义如下： -a: 显示所有的连接，包括正在进行的连接和监听状态的连接。 -n: 使用数字形式显示 IP 地址和端口号，而不是使用主机名和服务名。 -t: 仅显示 TCP 连接。 -u: 仅显示 UDP 连接。 -p: 显示与连接关联的进程标识符（PID）和程序名称。 |: 管道符号，用于将一个命令的输出作为另一个命令的输入。在这个命令中，它将 netstat 命令的输出传递给下一个命令。 grep docker: grep 是一个用于在文本中搜索模式的命令。在这个命令中，它用于过滤包含 &amp;ldquo;docker&amp;rdquo; 字符串的行，以便只显示与 Docker 相关的网络连接信息。 Ceph # MinIO # https://blog.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/json%E5%BA%8F%E5%88%97%E5%8C%96/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/json%E5%BA%8F%E5%88%97%E5%8C%96/</guid>
      <description>一、忽略字段 # 我们知道，通过tag,可以有条件地实现定制Go JSON序列化的方式，比如json:&amp;quot;abc,omitempty&amp;quot;, 当字段的值为空的时候，我们可以在序列化后的数据中不包含这个值，而json:&amp;quot;-&amp;quot;可以直接不被JSON序列化,如果想被序列化key-，可以设置tag为json:&amp;quot;-,&amp;quot;,加个逗号
二、改变一个字段显示 # 有下面这个结构体
type MyUser struct { ID int64 `json:&amp;#34;id&amp;#34;` Name string `json:&amp;#34;name&amp;#34;` LastSeen time.Time `json:&amp;#34;lastSeen&amp;#34;` } 如果临时想改变LastSeen字段显示为时间戳（或者密码我们不想打印到日志中，用***代替）
方案一 # 最简单的方式是引入另外一个辅助struct,在MarshalJSON中使用它进行正确的格式化：
func (u *MyUser) MarshalJSON() ([]byte, error) { return json.Marshal(&amp;amp;struct { ID int64 `json:&amp;#34;id&amp;#34;` Name string `json:&amp;#34;name&amp;#34;` LastSeen int64 `json:&amp;#34;lastSeen&amp;#34;` }{ ID: u.ID, Name: u.Name, LastSeen: u.LastSeen.Unix(), }) } 方案二 # 方案一在遇到多字段的时候会很麻烦，如果我们能把原始struct嵌入到新的struct中，并让它继承所有不需要改变的字段就太好了:
func (u *MyUser) MarshalJSON() ([]byte, error) { return json.Marshal(&amp;amp;struct { LastSeen int64 `json:&amp;#34;lastSeen&amp;#34;` *MyUser }{ LastSeen: u.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E4%BD%BF%E7%94%A8ollvm%E6%B7%B7%E6%B7%86hello-world/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E4%BD%BF%E7%94%A8ollvm%E6%B7%B7%E6%B7%86hello-world/</guid>
      <description>title: &amp;#34;使用OLLVM混淆Hello World&amp;#34; weight: 1 # bookFlatSection: false # bookToc: true # bookHidden: false # bookCollapseSection: false # bookComments: false # bookSearchExclude: false 背景 # 在某些功能开发过程中，可能会出现规避杀软的需求。然而，程序常常被 360 杀毒检测出，无论是静态分析、沙箱测试，还是运行时拦截都难以避免。
考虑到程序代码可能过于简单，我们尝试通过增加代码复杂度的方式来降低被检测的风险。LLVM 是一个备受瞩目的编译器基础设施项目，曾有开发者借助其混淆功能达到相关目的，因此本项目也决定以此为切入点。
网络资料内容良莠不齐，因此本文选择通过实践深入理解。
LLVM 简介 # 以下内容整理自维基百科：
LLVM 项目最初由伊利诺伊大学厄巴纳-香槟分校的 Vikram Adve 和 Chris Lattner 于 2000 年发起，目标是为各种静态和动态语言开发统一的编译技术。LLVM 使用 BSD 协议进行开源，2005 年 Chris Lattner 被苹果公司聘用，其团队所开发的技术成为 macOS 和 iOS 开发工具的重要组成部分。
&amp;ldquo;LLVM&amp;rdquo; 最初意指 &amp;ldquo;Low Level Virtual Machine&amp;rdquo;，但由于项目发展已超出虚拟机范畴，该缩写的含义已被官方弃用。目前，LLVM 泛指包括 LLVM IR、调试工具、C++ 标准库等在内的编译工具集合。
LLVM 项目结构 # 克隆 llvm-project（版本 18.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E5%90%88%E9%9B%86/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB%E5%90%88%E9%9B%86/</guid>
      <description>title: &amp;#34;经验分享合集&amp;#34;weight: 1# bookFlatSection: false# bookToc: true# bookHidden: false# bookCollapseSection: false# bookComments: false# bookSearchExclude: false [TOC]
Go部分 # xorm # 若使用纯 go 版本的 sqlite 驱动 github.com/glebarez/go-sqlite 需要注意 xorm 默认的驱动没有 sqlite 类型，需要手动 RegisterDriver ，示例如下： package main import ( &amp;#34;fmt&amp;#34; _ &amp;#34;github.com/glebarez/go-sqlite&amp;#34; &amp;#34;github.com/go-xorm/xorm&amp;#34; &amp;#34;strings&amp;#34; &amp;#34;xorm.io/core&amp;#34; ) type sqlite3Driver struct {} func (p *sqlite3Driver) Parse(driverName, dataSourceName string) (*core.Uri, error) { if strings.Contains(dataSourceName, &amp;#34;?&amp;#34;) { dataSourceName = dataSourceName[:strings.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/sqlite%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8F%92%E5%85%A5%E4%BC%98%E5%8C%96/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/sqlite/sqlite%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8F%92%E5%85%A5%E4%BC%98%E5%8C%96/</guid>
      <description>title: SQLite数据库插入优化 date: 2022-05-24 15:47:27 SQLite数据库插入优化 # 需求 # 视频取证业务中，由于部分监控录像品牌视频碎块太多，扫描视频过程中每三秒会产生百万级数据需要插入数据库，原有数据插入接口太慢，无法达到要求，导致我们产品的扫描速度很慢，极大影响客户体验。
视频取证产品使用的是SQLite数据库，SQLite 是一种轻量级、基于文件的关系数据库管理系统 (RDBMS)，以其简单性、可移植性和独立性而闻名。它适用于需要独立数据库解决方案的嵌入式系统、移动应用程序和小型项目。SQLite的设计初衷是用于单用户或嵌入式应用，专注于轻量级、低资源消耗。因此，当插入数据量较大时，SQLite的性能会受到影响，比不上MySQL等其他数据库。若替换其他数据库完全没有必要。
性能优化过程 # 现有性能 # 以465.76GB镜像文件为例，取最后十次提交日志：
本次提交数量：5897 本次提交耗时：0.3652427 本次提交速率：16145.428779274713 总提交数量：126400613 总提交耗时：4654.896980299996 平均速率：27154.330919661686 本次提交数量：358796 本次提交耗时：12.5037099 本次提交速率：28695.16350503301 总提交数量：126759409 总提交耗时：4667.4006901999965 平均速率：27158.45872546426 本次提交数量：50902 本次提交耗时：3.0472488 本次提交速率：16704.24810734194 总提交数量：126810311 总提交耗时：4670.447938999996 平均速率：27151.637842076394 本次提交数量：5102 本次提交耗时：0.997792 本次提交速率：5113.2901446393635 总提交数量：126815413 总提交耗时：4671.445730999996 平均速率：27146.93058691558 本次提交数量：375604 本次提交耗时：14.2310447 本次提交速率：26393.28369195552 总提交数量：127191017 总提交耗时：4685.676775699996 平均速率：27144.64165766083 本次提交数量：490755 本次提交耗时：17.804466 本次提交速率：27563.589944230844 总提交数量：127681772 总提交耗时：4703.481241699996 平均速率：27146.227536319784 本次提交数量：254451 本次提交耗时：9.9592031 本次提交速率：25549.333359814704 总提交数量：127936223 总提交耗时：4713.440444799996 平均速率：27142.853399398085 本次提交数量：543249 本次提交耗时：19.6061585 本次提交速率：27708.07958121934 总提交数量：128479472 总提交耗时：4733.046603299996 平均速率：27145.194790691683 本次提交数量：424834 本次提交耗时：14.9654391 本次提交速率：28387.673569831975 总提交数量：128904306 总提交耗时：4748.</description>
    </item>
    
    <item>
      <title></title>
      <link>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/%E6%95%B0%E6%8D%AE%E5%BA%93/%E5%9F%BA%E7%A1%80/%E5%B8%B8%E8%A7%81%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/</guid>
      <description>title: 几种常见数据库的备份与恢复 date: 2022-05-24 15:47:27 MySQL的数据备份与恢复 # 1. 数据的备份类型 # 数据的备份类型根据其自身的特性主要分为以下几组：
1.完全备份　完全备份指的是备份整个数据集( 即整个数据库 )
2.部分备份 部分备份指的是备份部分数据集(例如: 只备份一个表) 而部分备份又分为：
增量备份 增量备份指的是备份自上一次备份以来(增量或完全)以来变化的数据。特点: 节约空间、还原麻烦 差异备份 差异备份指的是备份自上一次完全备份以来变化的数据。特点: 浪费空间、还原比增量备份简单 2. MySQL备份数据的方式 # 在MySQl中备份数据一般有三种方式：
热备份 热备份指的是当数据库进行备份时, 数据库的读写操作均不是受影响
温备份 温备份指的是当数据库进行备份时, 数据库的读操作可以执行, 但是不能执行写操作
冷备份 冷备份指的是当数据库进行备份时, 数据库不能进行读写操作, 即数据库要下线
MySQL中进行不同方式的备份还要考虑存储引擎是否支持： 1）MyISAM 热备 × 温备 √ 冷备 √ 2）InnoDB 热备 √ 温备 √ 冷备 √
我们考虑完数据备份, 数据库的运行状态之后还需要考虑对于MySQL数据库中数据的备份方式：
（1）物理备份 物理备份一般就是通过tar,cp等命令直接打包复制数据库的数据文件达到备份的效果 （2）逻辑备份 逻辑备份一般就是通过特定工具从数据库中导出数据并另存备份(逻辑备份会丢失数据精度) 3.备份工具 # 常用的备份工具有：
mysqldump： 逻辑备份工具, 适用于所有的存储引擎, 支持温备、完全备份、部分备份、对于InnoDB存储引擎支持热备 cp, tar 等归档复制工具： 物理备份工具, 适用于所有的存储引擎, 冷备、完全备份、部分备份 lvm2 snapshot： 几乎热备, 借助文件系统管理工具进行备份 mysqlhotcopy： 名不副实的的一个工具, 几乎冷备, 仅支持MyISAM存储引擎 xtrabackup： 一款非常强大的InnoDB/XtraDB热备工具, 支持完全备份、增量备份, 由percona提供 下面介绍一下mysqldump的使用</description>
    </item>
    
    <item>
      <title>Goland常用技巧</title>
      <link>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/goland%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chain-code.github.io/docs/golang/%E5%9F%BA%E7%A1%80/goland%E5%B8%B8%E7%94%A8%E6%8A%80%E5%B7%A7/</guid>
      <description>注释 # IDEA注释 # // 这是一个单行注释/*这是一个多行注释可以用于注释多行代码*/ 函数注释 # // add 函数将两个整数相加并返回结果 // 参数: a - 第一个整数, b - 第二个整数 // 返回值: 两个整数的和 func add(a, b int) int { return a + b } TODO：英语翻译为待办事项，备忘录。如果代码中有该标识，说明在标识处有功能代码待编写，待实现的功能在说明中会简略说明。
FIXME：可以拆成短语，fix me ，意为修理我。如果代码中有该标识，说明标识处代码需要修正，甚至代码是错误的，不能工作，需要修复，如何修正会在说明中简略说明。
// FIXME: 这里有一个需要修复的问题 可小写// TODO: 添加错误处理代码 添加新的注释格式 # 标签 说明 TODO: 以后要添加的功能 FIXME: 已知的BUG,以后需要修正 HACK: 代码不太好，需要优化 XXX: 包含所有tag的tag,不好明确到底用哪个tag REVIEW: 虽然好用，最好还是评审一下 OPTIMIZE: 性能不好，需要优化 NOTE: 一些说明 WARNING: 请注意 代码报红处理方法 # 设置import规范 # 1、标准库</description>
    </item>
    
  </channel>
</rss>
